{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb7593a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:36:20.140320Z",
     "iopub.status.busy": "2025-07-10T07:36:20.140046Z",
     "iopub.status.idle": "2025-07-10T07:36:20.177035Z",
     "shell.execute_reply": "2025-07-10T07:36:20.176255Z"
    },
    "papermill": {
     "duration": 0.048416,
     "end_time": "2025-07-10T07:36:20.178196",
     "exception": false,
     "start_time": "2025-07-10T07:36:20.129780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing single_file_rl_training_final.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile single_file_rl_training_final.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enhanced Single-file implementation of Reinforcement Learning for Hierarchical Employee Training Optimization\n",
    "Based on the research paper by Soumedhik Bharati, Rupsha Sadhukhan, Debanjali Saha\n",
    "\n",
    "Improvements implemented:\n",
    "- Fixed learning rate scheduling with StepLR for consistent learning\n",
    "- Reduced entropy coefficient for more decisive policies\n",
    "- Enhanced budget management through increased cost penalties\n",
    "- Amplified efficiency bonuses to encourage budget awareness\n",
    "- ADDED: Deep analysis metrics - ROI, Pareto Fronts, and Policy Trajectory Heatmaps\n",
    "\n",
    "Usage:\n",
    "    python single_file_rl_training.py --mode train --episodes 3000 --cost-penalty 0.015\n",
    "    python single_file_rl_training.py --mode compare --episodes 1000\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Tuple, Optional, Literal\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Gymnasium for RL environment\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration class for training parameters with budget-aware defaults\"\"\"\n",
    "    # Training parameters\n",
    "    num_episodes: int = 3000  # Increased for budget management learning\n",
    "    gamma: float = 0.99\n",
    "    learning_rate: float = 3e-4\n",
    "    hidden_dim: int = 128\n",
    "    use_baseline: bool = True\n",
    "    entropy_coefficient: float = 0.001  # FIXED: Reduced from 0.01 for more decisive policy\n",
    "    \n",
    "    # Learning rate scheduling parameters\n",
    "    lr_step_size: int = 750  # NEW: Steps before LR decay\n",
    "    lr_gamma: float = 0.9    # NEW: LR decay factor\n",
    "    \n",
    "    # Environment parameters\n",
    "    D: int = 8  # Number of skills\n",
    "    K: int = 4  # Number of training modules\n",
    "    alpha: List[float] = None  # Learning rates for modules\n",
    "    beta: float = 0.01  # Reduced forgetting rate\n",
    "    kappa: float = 1.2  # Reduced diminishing returns\n",
    "    C_max: float = 120.0  # Increased budget\n",
    "    \n",
    "    # Reward strategy\n",
    "    reward_strategy: Literal['basic', 'terminal', 'efficiency', 'hybrid'] = 'hybrid'\n",
    "    cost_penalty: float = 0.01  # FIXED: Increased from 0.002 for budget awareness (5x increase)\n",
    "    skill_amplifier: float = 1.0  # Amplify skill improvements\n",
    "    terminal_bonus_multiplier: float = 1.5  # Terminal reward multiplier\n",
    "    \n",
    "    # Budget penalty parameters - FIXED: Increased for harsher budget enforcement\n",
    "    base_budget_penalty: float = 4.0  # FIXED: Increased from 2.0\n",
    "    max_budget_penalty: float = 8.0   # FIXED: Increased from 4.0\n",
    "    \n",
    "    # Logging and saving\n",
    "    log_interval: int = 25  # More frequent logging for better monitoring\n",
    "    save_interval: int = 500  # Less frequent saves for faster training\n",
    "    model_save_path: str = 'models/employee_training_model.pth'\n",
    "    plot_save_path: str = 'plots/'\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    eval_episodes: int = 100\n",
    "    eval_render: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.alpha is None:\n",
    "            self.alpha = [0.3, 0.25, 0.2, 0.35]\n",
    "        \n",
    "        # Validate configuration\n",
    "        if self.entropy_coefficient > 0.1:\n",
    "            print(f\"WARNING: High entropy coefficient ({self.entropy_coefficient}) may prevent policy convergence\")\n",
    "        \n",
    "        if self.lr_step_size > self.num_episodes // 2:\n",
    "            print(f\"WARNING: LR step size ({self.lr_step_size}) is large relative to training episodes ({self.num_episodes})\")\n",
    "        \n",
    "        # Budget management validation\n",
    "        if self.cost_penalty < 0.005:\n",
    "            print(f\"INFO: Low cost penalty ({self.cost_penalty}) may lead to budget overruns\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EMPLOYEE TRAINING ENVIRONMENT\n",
    "# =============================================================================\n",
    "\n",
    "class EmployeeTrainingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Enhanced Custom Gymnasium environment with budget-aware reward structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.D = config.D\n",
    "        self.K = config.K\n",
    "        self.beta = config.beta\n",
    "        self.kappa = config.kappa\n",
    "        self.C_max = config.C_max\n",
    "        self.gamma = config.gamma\n",
    "        \n",
    "        # Learning rates for each training module\n",
    "        self.alpha = config.alpha\n",
    "        \n",
    "        # Training module costs\n",
    "        self.costs = [10.0, 15.0, 20.0, 12.0]\n",
    "        \n",
    "        # Define which sub-attributes each training module targets\n",
    "        self.module_targets = {\n",
    "            0: [0, 1],      # Technical Skills: Coding, Debugging\n",
    "            1: [2, 3],      # Technical Skills: Testing, Architecture\n",
    "            2: [4, 5],      # Soft Skills: Communication, Leadership\n",
    "            3: [6, 7]       # Soft Skills: Teamwork, Problem-solving\n",
    "        }\n",
    "        \n",
    "        # Cross-attribute synergy matrix\n",
    "        self.synergy_matrix = self._initialize_synergy_matrix()\n",
    "        \n",
    "        # Gymnasium spaces\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(self.D,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(self.K)\n",
    "        \n",
    "        # Episode state\n",
    "        self.current_skills = None\n",
    "        self.current_cost = 0.0\n",
    "        self.episode_length = 0\n",
    "        self.max_episode_length = 50\n",
    "        self.initial_skills = None  # Track initial state for terminal rewards\n",
    "        \n",
    "    def _initialize_synergy_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Initialize the cross-attribute synergy matrix.\"\"\"\n",
    "        synergy = np.zeros((self.D, self.D))\n",
    "        \n",
    "        # Within technical skills\n",
    "        synergy[0, 1] = synergy[1, 0] = 0.3  # Coding <-> Debugging\n",
    "        synergy[0, 2] = synergy[2, 0] = 0.2  # Coding <-> Testing\n",
    "        synergy[1, 2] = synergy[2, 1] = 0.4  # Debugging <-> Testing\n",
    "        synergy[2, 3] = synergy[3, 2] = 0.3  # Testing <-> Architecture\n",
    "        \n",
    "        # Within soft skills\n",
    "        synergy[4, 5] = synergy[5, 4] = 0.4  # Communication <-> Leadership\n",
    "        synergy[4, 6] = synergy[6, 4] = 0.3  # Communication <-> Teamwork\n",
    "        synergy[5, 6] = synergy[6, 5] = 0.2  # Leadership <-> Teamwork\n",
    "        synergy[6, 7] = synergy[7, 6] = 0.3  # Teamwork <-> Problem-solving\n",
    "        \n",
    "        # Cross-domain synergies\n",
    "        synergy[1, 7] = synergy[7, 1] = 0.15  # Debugging <-> Problem-solving\n",
    "        synergy[3, 5] = synergy[5, 3] = 0.1   # Architecture <-> Leadership\n",
    "        \n",
    "        return synergy\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Initialize skills randomly between 0.1 and 0.6\n",
    "        self.current_skills = self.np_random.uniform(0.1, 0.6, size=self.D).astype(np.float32)\n",
    "        self.initial_skills = self.current_skills.copy()  # Store initial state\n",
    "        self.current_cost = 0.0\n",
    "        self.episode_length = 0\n",
    "        \n",
    "        return self.current_skills.copy(), {}\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"Execute one step with enhanced budget penalty handling.\"\"\"\n",
    "        if action < 0 or action >= self.K:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "        \n",
    "        # Calculate cost and check budget constraint\n",
    "        action_cost = self.costs[action]\n",
    "        \n",
    "        # FIXED: Enhanced budget penalty - harsher penalties for better budget discipline\n",
    "        if self.current_cost + action_cost > self.C_max:\n",
    "            # Calculate proportional penalty based on budget overrun\n",
    "            overrun_amount = (self.current_cost + action_cost - self.C_max)\n",
    "            overrun_ratio = overrun_amount / self.C_max\n",
    "            \n",
    "            # Penalty scales from base_penalty to max_penalty based on overrun\n",
    "            penalty_scale = min(1.0, overrun_ratio)\n",
    "            budget_penalty = -(self.config.base_budget_penalty + \n",
    "                             penalty_scale * (self.config.max_budget_penalty - self.config.base_budget_penalty))\n",
    "            \n",
    "            info = {\n",
    "                \"budget_exceeded\": True,\n",
    "                \"current_cost\": self.current_cost + action_cost,\n",
    "                \"overrun_amount\": overrun_amount,\n",
    "                \"budget_penalty\": budget_penalty,\n",
    "                \"skill_improvement\": 0.0,\n",
    "                \"total_skill_improvement\": np.sum(self.current_skills - self.initial_skills),\n",
    "                \"budget_utilization\": (self.current_cost + action_cost) / self.C_max,\n",
    "                \"terminated\": True,\n",
    "            }\n",
    "            \n",
    "            return self.current_skills.copy(), budget_penalty, True, False, info\n",
    "        \n",
    "        # Store previous skills for reward calculation\n",
    "        prev_skills = self.current_skills.copy()\n",
    "        \n",
    "        # Apply training module\n",
    "        self.current_skills = self._apply_training(self.current_skills, action)\n",
    "        \n",
    "        # Update cost and episode length\n",
    "        self.current_cost += action_cost\n",
    "        self.episode_length += 1\n",
    "        \n",
    "        # Check termination conditions\n",
    "        terminated = (self.episode_length >= self.max_episode_length or \n",
    "                     self.current_cost >= self.C_max)\n",
    "        \n",
    "        # Calculate reward based on strategy\n",
    "        reward = self._calculate_reward(prev_skills, self.current_skills, action_cost, terminated)\n",
    "        \n",
    "        info = {\n",
    "            \"current_cost\": self.current_cost,\n",
    "            \"episode_length\": self.episode_length,\n",
    "            \"skill_improvement\": np.sum(self.current_skills - prev_skills),\n",
    "            \"total_skill_improvement\": np.sum(self.current_skills - self.initial_skills),\n",
    "            \"budget_utilization\": self.current_cost / self.C_max,\n",
    "            \"terminated\": terminated,\n",
    "            \"budget_exceeded\": False\n",
    "        }\n",
    "        \n",
    "        return self.current_skills.copy(), reward, terminated, False, info\n",
    "    \n",
    "    def _apply_training(self, skills: np.ndarray, action: int) -> np.ndarray:\n",
    "        \"\"\"Apply training module to current skills.\"\"\"\n",
    "        new_skills = skills.copy()\n",
    "        alpha_a = self.alpha[action]\n",
    "        target_attributes = self.module_targets[action]\n",
    "        \n",
    "        # Calculate potential gains for each attribute\n",
    "        for j in range(self.D):\n",
    "            # Direct training effect\n",
    "            if j in target_attributes:\n",
    "                delta_j = (1 - skills[j]) ** self.kappa\n",
    "            else:\n",
    "                # Cross-attribute synergy effect\n",
    "                delta_j = 0.0\n",
    "                for k in target_attributes:\n",
    "                    delta_j += self.synergy_matrix[j, k] * (1 - skills[j]) ** self.kappa\n",
    "            \n",
    "            # Apply training gain and forgetting\n",
    "            new_skills[j] = skills[j] + alpha_a * delta_j - self.beta * skills[j]\n",
    "        \n",
    "        # Clip skills to valid range [0, 1]\n",
    "        new_skills = np.clip(new_skills, 0.0, 1.0)\n",
    "        \n",
    "        return new_skills\n",
    "    \n",
    "    def _calculate_reward(self, prev_skills: np.ndarray, new_skills: np.ndarray, \n",
    "                         cost: float, terminated: bool) -> float:\n",
    "        \"\"\"Calculate reward with enhanced budget awareness.\"\"\"\n",
    "        skill_improvement = np.sum(new_skills - prev_skills)\n",
    "        \n",
    "        if self.config.reward_strategy == 'basic':\n",
    "            # Basic: Amplified skill gain - increased cost penalty\n",
    "            return (self.config.skill_amplifier * skill_improvement - \n",
    "                   self.config.cost_penalty * cost)\n",
    "        \n",
    "        elif self.config.reward_strategy == 'terminal':\n",
    "            # Terminal: Small step rewards + large terminal bonus\n",
    "            step_reward = skill_improvement - self.config.cost_penalty * cost\n",
    "            \n",
    "            if terminated and self.current_cost <= self.C_max:\n",
    "                # Terminal bonus based on total skill level and budget efficiency\n",
    "                total_skill_level = np.sum(self.current_skills)\n",
    "                budget_efficiency = (self.C_max - self.current_cost) / self.C_max\n",
    "                terminal_bonus = (total_skill_level * self.config.terminal_bonus_multiplier + \n",
    "                                budget_efficiency * 2.0)\n",
    "                return step_reward + terminal_bonus\n",
    "            \n",
    "            return step_reward\n",
    "        \n",
    "        elif self.config.reward_strategy == 'efficiency':\n",
    "            # Efficiency: Reward skill gains, bonus for finishing under budget\n",
    "            step_reward = skill_improvement  # No cost penalty during episode\n",
    "            \n",
    "            if terminated:\n",
    "                # End-of-episode rewards\n",
    "                total_skills = np.sum(self.current_skills)\n",
    "                if self.current_cost <= self.C_max:\n",
    "                    budget_bonus = (self.C_max - self.current_cost) * 0.1\n",
    "                    skill_bonus = total_skills * 1.5\n",
    "                    return step_reward + budget_bonus + skill_bonus\n",
    "                else:\n",
    "                    return step_reward - 8.0  # Increased penalty for exceeding budget\n",
    "            \n",
    "            return step_reward\n",
    "        \n",
    "        elif self.config.reward_strategy == 'hybrid':\n",
    "            # FIXED: Enhanced hybrid strategy with budget-aware terminal bonus\n",
    "            base_reward = self.config.skill_amplifier * skill_improvement\n",
    "            cost_penalty = self.config.cost_penalty * cost\n",
    "            \n",
    "            if terminated and self.current_cost <= self.C_max:\n",
    "                # FIXED: Enhanced terminal bonus heavily favoring efficiency\n",
    "                total_improvement = np.sum(self.current_skills - self.initial_skills)\n",
    "                \n",
    "                # Amplify efficiency bonus by 2x and keep improvement bonus the same\n",
    "                efficiency_bonus = ((self.C_max - self.current_cost) / self.C_max) * 2.0  # 2x amplification\n",
    "                improvement_bonus = total_improvement * 0.5  # Keep same\n",
    "                \n",
    "                terminal_bonus = improvement_bonus + efficiency_bonus\n",
    "                return base_reward - cost_penalty + terminal_bonus\n",
    "            \n",
    "            return base_reward - cost_penalty\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reward strategy: {self.config.reward_strategy}\")\n",
    "    \n",
    "    def get_hierarchical_skills(self, skills: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calculate hierarchical skill aggregations.\"\"\"\n",
    "        return {\n",
    "            \"technical_skills\": np.mean(skills[0:4]),\n",
    "            \"soft_skills\": np.mean(skills[4:8]),\n",
    "            \"coding_debugging\": np.mean(skills[0:2]),\n",
    "            \"testing_architecture\": np.mean(skills[2:4]),\n",
    "            \"communication_leadership\": np.mean(skills[4:6]),\n",
    "            \"teamwork_problem_solving\": np.mean(skills[6:8])\n",
    "        }\n",
    "    \n",
    "    def render(self, mode: str = \"human\") -> None:\n",
    "        \"\"\"Render the current state.\"\"\"\n",
    "        if mode == \"human\":\n",
    "            hierarchical = self.get_hierarchical_skills(self.current_skills)\n",
    "            print(f\"Episode Length: {self.episode_length}, Cost: {self.current_cost:.2f}/{self.C_max}\")\n",
    "            print(\"Hierarchical Skills:\")\n",
    "            for skill_name, value in hierarchical.items():\n",
    "                print(f\"  {skill_name}: {value:.3f}\")\n",
    "            print(f\"Individual Skills: {self.current_skills}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED NEURAL NETWORKS AND AGENT\n",
    "# =============================================================================\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Enhanced policy network with better initialization and regularization.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Better initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Enhanced value network with better initialization.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Better initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class EnhancedREINFORCEAgent:\n",
    "    \"\"\"\n",
    "    Enhanced REINFORCE agent with fixed learning rate scheduling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.state_dim = config.D\n",
    "        self.action_dim = config.K\n",
    "        self.use_baseline = config.use_baseline\n",
    "        self.entropy_coefficient = config.entropy_coefficient\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, config.hidden_dim, self.action_dim)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # FIXED: Use StepLR instead of ExponentialLR for stable learning\n",
    "        self.policy_scheduler = StepLR(\n",
    "            self.policy_optimizer, \n",
    "            step_size=config.lr_step_size, \n",
    "            gamma=config.lr_gamma\n",
    "        )\n",
    "        \n",
    "        # Value network (optional baseline)\n",
    "        if self.use_baseline:\n",
    "            self.value_net = ValueNetwork(self.state_dim, config.hidden_dim)\n",
    "            self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=config.learning_rate)\n",
    "            self.value_scheduler = StepLR(\n",
    "                self.value_optimizer, \n",
    "                step_size=config.lr_step_size, \n",
    "                gamma=config.lr_gamma\n",
    "            )\n",
    "        \n",
    "        # Episode memory\n",
    "        self.log_probs: List[torch.Tensor] = []\n",
    "        self.rewards: List[float] = []\n",
    "        self.states: List[torch.Tensor] = []\n",
    "        self.entropies: List[torch.Tensor] = []  # For entropy regularization\n",
    "        \n",
    "        # Training statistics\n",
    "        self.training_stats = {\n",
    "            'policy_losses': [],\n",
    "            'value_losses': [],\n",
    "            'entropy_losses': [],\n",
    "            'learning_rates': [],\n",
    "            'lr_decay_steps': []  # Track when LR decays occur\n",
    "        }\n",
    "        \n",
    "        print(f\"Initialized budget-aware agent with LR={config.learning_rate}, \"\n",
    "              f\"cost_penalty={config.cost_penalty}, step_size={config.lr_step_size}\")\n",
    "        \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action using current policy and track entropy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_probs = self.policy_net(state_tensor)\n",
    "        \n",
    "        # Sample action from probability distribution\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Store log probability, entropy, and state\n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        self.entropies.append(dist.entropy())\n",
    "        self.states.append(state_tensor)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward: float) -> None:\n",
    "        \"\"\"Store reward for current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update_policy(self, gamma: float = 0.99) -> Dict[str, float]:\n",
    "        \"\"\"Enhanced policy update with improved learning rate tracking.\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return {\"policy_loss\": 0.0, \"value_loss\": 0.0, \"entropy_loss\": 0.0, \"learning_rate\": 0.0}\n",
    "        \n",
    "        # Store current learning rate before any updates\n",
    "        current_lr = self.policy_optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize returns for stability\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss components\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        entropy_losses = []\n",
    "        \n",
    "        for i, (log_prob, entropy, R) in enumerate(zip(self.log_probs, self.entropies, returns)):\n",
    "            if self.use_baseline:\n",
    "                # Use value function as baseline\n",
    "                state = self.states[i]\n",
    "                baseline = self.value_net(state).squeeze()\n",
    "                advantage = R - baseline\n",
    "                policy_losses.append(-log_prob * advantage.detach())\n",
    "                value_losses.append(F.mse_loss(baseline, R))\n",
    "            else:\n",
    "                policy_losses.append(-log_prob * R)\n",
    "            \n",
    "            entropy_losses.append(entropy)\n",
    "        \n",
    "        # Combine losses\n",
    "        policy_loss = torch.stack(policy_losses).mean()\n",
    "        entropy_loss = torch.stack(entropy_losses).mean()\n",
    "        \n",
    "        # Total policy loss with entropy regularization\n",
    "        total_policy_loss = policy_loss - self.entropy_coefficient * entropy_loss\n",
    "        \n",
    "        # Update policy network\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        total_policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Track LR decay steps\n",
    "        prev_lr = current_lr\n",
    "        self.policy_scheduler.step()\n",
    "        new_lr = self.policy_optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if new_lr != prev_lr:\n",
    "            self.training_stats['lr_decay_steps'].append(len(self.training_stats['learning_rates']))\n",
    "        \n",
    "        # Update value network if using baseline\n",
    "        value_loss = torch.tensor(0.0)\n",
    "        if self.use_baseline and value_losses:\n",
    "            value_loss = torch.stack(value_losses).mean()\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 1.0)\n",
    "            self.value_optimizer.step()\n",
    "            self.value_scheduler.step()\n",
    "        \n",
    "        # Store training statistics\n",
    "        self.training_stats['policy_losses'].append(policy_loss.item())\n",
    "        self.training_stats['value_losses'].append(value_loss.item())\n",
    "        self.training_stats['entropy_losses'].append(entropy_loss.item())\n",
    "        self.training_stats['learning_rates'].append(new_lr)\n",
    "        \n",
    "        # Clear episode memory\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.states.clear()\n",
    "        self.entropies.clear()\n",
    "        \n",
    "        return {\n",
    "            \"policy_loss\": policy_loss.item(),\n",
    "            \"value_loss\": value_loss.item(),\n",
    "            \"entropy_loss\": entropy_loss.item(),\n",
    "            \"learning_rate\": new_lr\n",
    "        }\n",
    "    \n",
    "    def get_state_value(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Get state value estimate (if using baseline).\"\"\"\n",
    "        if not self.use_baseline:\n",
    "            return 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            return self.value_net(state_tensor).item()\n",
    "    \n",
    "    def save_model(self, filepath: str) -> None:\n",
    "        \"\"\"Save trained model with training statistics.\"\"\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'policy_optimizer': self.policy_optimizer.state_dict(),\n",
    "            'policy_scheduler': self.policy_scheduler.state_dict(),\n",
    "            'config': self.config,\n",
    "            'training_stats': self.training_stats\n",
    "        }\n",
    "        \n",
    "        if self.use_baseline:\n",
    "            checkpoint['value_net'] = self.value_net.state_dict()\n",
    "            checkpoint['value_optimizer'] = self.value_optimizer.state_dict()\n",
    "            checkpoint['value_scheduler'] = self.value_scheduler.state_dict()\n",
    "        \n",
    "        torch.save(checkpoint, filepath)\n",
    "    \n",
    "    def load_model(self, filepath: str) -> None:\n",
    "        \"\"\"Load trained model.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: Model file not found at {filepath}. Using random agent.\")\n",
    "            return\n",
    "        \n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "        \n",
    "        if 'policy_scheduler' in checkpoint:\n",
    "            self.policy_scheduler.load_state_dict(checkpoint['policy_scheduler'])\n",
    "        \n",
    "        if self.use_baseline and 'value_net' in checkpoint:\n",
    "            self.value_net.load_state_dict(checkpoint['value_net'])\n",
    "            self.value_optimizer.load_state_dict(checkpoint['value_optimizer'])\n",
    "            if 'value_scheduler' in checkpoint:\n",
    "                self.value_scheduler.load_state_dict(checkpoint['value_scheduler'])\n",
    "        \n",
    "        if 'training_stats' in checkpoint:\n",
    "            self.training_stats = checkpoint['training_stats']\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedTrainingLoop:\n",
    "    \"\"\"Enhanced training loop with budget management focus.\"\"\"\n",
    "    \n",
    "    def __init__(self, env: EmployeeTrainingEnv, agent: EnhancedREINFORCEAgent, config: TrainingConfig):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.config = config\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_costs = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.skill_improvements = []\n",
    "        self.budget_utilizations = []\n",
    "        self.success_episodes = []\n",
    "        self.budget_exceeded_episodes = []  # Track budget violations\n",
    "        self.episode_rois = [] # NEW: Return on Investment metric\n",
    "        \n",
    "        # Recent performance tracking\n",
    "        self.recent_rewards = deque(maxlen=100)\n",
    "        self.recent_lengths = deque(maxlen=100)\n",
    "        self.recent_success = deque(maxlen=100)\n",
    "        self.recent_entropy = deque(maxlen=100)  # Track entropy trend\n",
    "        self.recent_budget_exceeded = deque(maxlen=100)  # Track budget discipline\n",
    "        \n",
    "        # Performance milestones\n",
    "        self.first_positive_reward_episode = None\n",
    "        self.first_80_percent_success = None\n",
    "        self.first_budget_discipline = None  # NEW: When budget exceeded rate drops below 20%\n",
    "        \n",
    "    def run_episode(self) -> Dict[str, float]:\n",
    "        \"\"\"Run a single training episode with enhanced budget metrics.\"\"\"\n",
    "        state, _ = self.env.reset()\n",
    "        initial_skills = state.copy()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        budget_exceeded = False\n",
    "        info = {}\n",
    "        \n",
    "        while True:\n",
    "            # Select action\n",
    "            action = self.agent.select_action(state)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            \n",
    "            # Store reward\n",
    "            self.agent.store_reward(reward)\n",
    "            \n",
    "            # Update metrics\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            # Track budget exceeded\n",
    "            if info.get('budget_exceeded', False):\n",
    "                budget_exceeded = True\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Check termination\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Update policy at end of episode\n",
    "        losses = self.agent.update_policy(self.config.gamma)\n",
    "        \n",
    "        # Determine success (positive reward AND within budget)\n",
    "        success = (episode_reward > 0 and not budget_exceeded)\n",
    "        \n",
    "        # Track milestones\n",
    "        episode_num = len(self.episode_rewards) + 1\n",
    "        if episode_reward > 0 and self.first_positive_reward_episode is None:\n",
    "            self.first_positive_reward_episode = episode_num\n",
    "            print(f\"🎉 First positive reward achieved at episode {episode_num}!\")\n",
    "            \n",
    "        # NEW: Calculate ROI (Return on Investment)\n",
    "        total_skill_improvement = info.get('total_skill_improvement', 0)\n",
    "        final_cost = info.get('current_cost', 0)\n",
    "        roi = (total_skill_improvement / final_cost) if final_cost > 0 else 0.0\n",
    "\n",
    "        return {\n",
    "            'episode_reward': episode_reward,\n",
    "            'episode_length': episode_length,\n",
    "            'episode_cost': final_cost,\n",
    "            'skill_improvement': total_skill_improvement,\n",
    "            'budget_utilization': info.get('budget_utilization', 0),\n",
    "            'budget_exceeded': budget_exceeded,\n",
    "            'success': success,\n",
    "            'roi': roi, # NEW\n",
    "            'policy_loss': losses['policy_loss'],\n",
    "            'value_loss': losses['value_loss'],\n",
    "            'entropy_loss': losses['entropy_loss'],\n",
    "            'learning_rate': losses['learning_rate']\n",
    "        }\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        \"\"\"Enhanced training loop with budget discipline tracking.\"\"\"\n",
    "        print(\"Starting Budget-Aware Training with Enhanced Cost Penalties...\")\n",
    "        print(f\"Episodes: {self.config.num_episodes}\")\n",
    "        print(f\"Environment: {self.env.D} skills, {self.env.K} training modules\")\n",
    "        print(f\"Agent: {'Actor-Critic' if self.agent.use_baseline else 'REINFORCE'} with entropy regularization\")\n",
    "        print(f\"Reward Strategy: {self.config.reward_strategy}\")\n",
    "        print(f\"Learning Rate: {self.config.learning_rate} (StepLR: step={self.config.lr_step_size}, gamma={self.config.lr_gamma})\")\n",
    "        print(f\"Cost Penalty: {self.config.cost_penalty} (5x increased for budget awareness)\")\n",
    "        print(f\"Budget Penalties: base={self.config.base_budget_penalty}, max={self.config.max_budget_penalty}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        best_avg_reward = float('-inf')\n",
    "        episodes_since_improvement = 0\n",
    "        \n",
    "        for episode in range(self.config.num_episodes):\n",
    "            # Run episode\n",
    "            episode_metrics = self.run_episode()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.episode_rewards.append(episode_metrics['episode_reward'])\n",
    "            self.episode_lengths.append(episode_metrics['episode_length'])\n",
    "            self.episode_costs.append(episode_metrics['episode_cost'])\n",
    "            self.skill_improvements.append(episode_metrics['skill_improvement'])\n",
    "            self.budget_utilizations.append(episode_metrics['budget_utilization'])\n",
    "            self.success_episodes.append(episode_metrics['success'])\n",
    "            self.budget_exceeded_episodes.append(episode_metrics['budget_exceeded'])\n",
    "            self.episode_rois.append(episode_metrics['roi']) # NEW\n",
    "            self.policy_losses.append(episode_metrics['policy_loss'])\n",
    "            self.value_losses.append(episode_metrics['value_loss'])\n",
    "            self.entropy_losses.append(episode_metrics['entropy_loss'])\n",
    "            self.learning_rates.append(episode_metrics['learning_rate'])\n",
    "            \n",
    "            # Update recent performance\n",
    "            self.recent_rewards.append(episode_metrics['episode_reward'])\n",
    "            self.recent_lengths.append(episode_metrics['episode_length'])\n",
    "            self.recent_success.append(episode_metrics['success'])\n",
    "            self.recent_entropy.append(episode_metrics['entropy_loss'])\n",
    "            self.recent_budget_exceeded.append(episode_metrics['budget_exceeded'])\n",
    "            \n",
    "            # Check for improvement and milestones\n",
    "            if len(self.recent_rewards) >= 100:\n",
    "                current_avg = np.mean(self.recent_rewards)\n",
    "                current_success_rate = np.mean(self.recent_success)\n",
    "                current_budget_exceeded_rate = np.mean(self.recent_budget_exceeded)\n",
    "                \n",
    "                if current_avg > best_avg_reward:\n",
    "                    best_avg_reward = current_avg\n",
    "                    episodes_since_improvement = 0\n",
    "                else:\n",
    "                    episodes_since_improvement += 1\n",
    "                \n",
    "                # Check for 80% success rate milestone\n",
    "                if current_success_rate >= 0.8 and self.first_80_percent_success is None:\n",
    "                    self.first_80_percent_success = episode + 1\n",
    "                    print(f\"🎯 80% success rate achieved at episode {episode + 1}!\")\n",
    "                \n",
    "                # NEW: Check for budget discipline milestone\n",
    "                if current_budget_exceeded_rate <= 0.2 and self.first_budget_discipline is None:\n",
    "                    self.first_budget_discipline = episode + 1\n",
    "                    print(f\"💰 Budget discipline achieved (≤20% exceeded) at episode {episode + 1}!\")\n",
    "            \n",
    "            # Logging with enhanced budget information\n",
    "            if (episode + 1) % self.config.log_interval == 0:\n",
    "                self._log_progress(episode + 1, episode_metrics)\n",
    "            \n",
    "            # Save model\n",
    "            if (episode + 1) % self.config.save_interval == 0:\n",
    "                self.agent.save_model(self.config.model_save_path)\n",
    "                print(f\"💾 Model saved at episode {episode + 1}\")\n",
    "        \n",
    "        # Final save\n",
    "        self.agent.save_model(self.config.model_save_path)\n",
    "        \n",
    "        # Training summary with budget focus\n",
    "        training_time = time.time() - start_time\n",
    "        final_success_rate = np.mean(self.recent_success) if len(self.recent_success) >= 10 else 0\n",
    "        final_entropy = np.mean(self.recent_entropy) if len(self.recent_entropy) >= 10 else 0\n",
    "        final_budget_exceeded_rate = np.mean(self.recent_budget_exceeded) if len(self.recent_budget_exceeded) >= 10 else 0\n",
    "        \n",
    "        print(f\"\\n{'='*90}\")\n",
    "        print(\"BUDGET-AWARE TRAINING COMPLETED\")\n",
    "        print(f\"{'='*90}\")\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "        print(f\"Best average reward (last 100): {best_avg_reward:.3f}\")\n",
    "        print(f\"Final average reward (last 100): {np.mean(self.recent_rewards):.3f}\")\n",
    "        print(f\"Final success rate (last 100): {final_success_rate:.2%}\")\n",
    "        print(f\"Final budget exceeded rate (last 100): {final_budget_exceeded_rate:.2%} 🎯\")\n",
    "        print(f\"Final entropy: {final_entropy:.3f} (max possible: {np.log(self.env.K):.3f})\")\n",
    "        print(f\"Final learning rate: {self.learning_rates[-1]:.2e}\")\n",
    "        \n",
    "        if self.first_positive_reward_episode:\n",
    "            print(f\"First positive reward: Episode {self.first_positive_reward_episode}\")\n",
    "        if self.first_80_percent_success:\n",
    "            print(f\"80% success milestone: Episode {self.first_80_percent_success}\")\n",
    "        if self.first_budget_discipline:\n",
    "            print(f\"Budget discipline milestone: Episode {self.first_budget_discipline}\")\n",
    "        \n",
    "        # LR decay analysis\n",
    "        lr_decays = len(self.agent.training_stats['lr_decay_steps'])\n",
    "        print(f\"Learning rate decayed {lr_decays} times during training\")\n",
    "        \n",
    "    def _log_progress(self, episode: int, metrics: Dict[str, float]) -> None:\n",
    "        \"\"\"Enhanced progress logging with budget management focus.\"\"\"\n",
    "        avg_reward = np.mean(self.recent_rewards)\n",
    "        avg_length = np.mean(self.recent_lengths)\n",
    "        success_rate = np.mean(self.recent_success)\n",
    "        avg_entropy = np.mean(self.recent_entropy)\n",
    "        budget_exceeded_rate = np.mean(self.recent_budget_exceeded)\n",
    "        \n",
    "        # Trend indicators\n",
    "        reward_trend = \"📈\" if len(self.recent_rewards) >= 50 and avg_reward > np.mean(list(self.recent_rewards)[:25]) else \"📉\"\n",
    "        entropy_trend = \"📉\" if len(self.recent_entropy) >= 50 and avg_entropy < np.mean(list(self.recent_entropy)[:25]) else \"📈\"\n",
    "        budget_trend = \"📉\" if len(self.recent_budget_exceeded) >= 50 and budget_exceeded_rate < np.mean(list(self.recent_budget_exceeded)[:25]) else \"📈\"\n",
    "        \n",
    "        print(f\"Episode {episode:4d} | \"\n",
    "              f\"Reward: {metrics['episode_reward']:6.2f} | \"\n",
    "              f\"Avg: {avg_reward:6.2f} {reward_trend} | \"\n",
    "              f\"Success: {success_rate:5.2%} | \"\n",
    "              f\"Budget Exceeded: {budget_exceeded_rate:5.2%} {budget_trend} | \"\n",
    "              f\"Entropy: {avg_entropy:5.3f} {entropy_trend} | \"\n",
    "              f\"LR: {metrics['learning_rate']:.2e}\")\n",
    "    \n",
    "    def evaluate(self, num_episodes: int = 100, render: bool = False, visualize_trajectory: bool = False) -> Dict[str, any]:\n",
    "        \"\"\"Enhanced evaluation with detailed budget analysis and optional trajectory visualization.\"\"\"\n",
    "        print(f\"\\nEvaluating budget-aware policy over {num_episodes} episodes...\")\n",
    "        \n",
    "        eval_rewards = []\n",
    "        eval_lengths = []\n",
    "        eval_costs = []\n",
    "        eval_skill_improvements = []\n",
    "        eval_budget_utilizations = []\n",
    "        eval_successes = []\n",
    "        eval_budget_exceeded = []\n",
    "        action_trajectories = [] # NEW: For trajectory heatmap\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            initial_skills = state.copy()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            budget_exceeded = False\n",
    "            current_trajectory = [] # NEW\n",
    "\n",
    "            while True:\n",
    "                # Select action (deterministic for evaluation)\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                    action_probs = self.agent.policy_net(state_tensor)\n",
    "                    action = torch.argmax(action_probs).item()\n",
    "                \n",
    "                current_trajectory.append(action) # NEW\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                if info.get('budget_exceeded', False):\n",
    "                    budget_exceeded = True\n",
    "                \n",
    "                if render and episode == 0:\n",
    "                    self.env.render()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            action_trajectories.append(current_trajectory) # NEW\n",
    "            eval_rewards.append(episode_reward)\n",
    "            eval_lengths.append(episode_length)\n",
    "            eval_costs.append(info.get('current_cost', 0))\n",
    "            eval_skill_improvements.append(info.get('total_skill_improvement', 0))\n",
    "            eval_budget_utilizations.append(info.get('budget_utilization', 0))\n",
    "            eval_successes.append(episode_reward > 0 and not budget_exceeded)\n",
    "            eval_budget_exceeded.append(budget_exceeded)\n",
    "        \n",
    "        # NEW: Generate trajectory heatmap if requested\n",
    "        if visualize_trajectory:\n",
    "            self._plot_policy_trajectory(\n",
    "                action_trajectories,\n",
    "                save_path=os.path.join(self.config.plot_save_path, 'policy_trajectory_heatmap.png')\n",
    "            )\n",
    "\n",
    "        results = {\n",
    "            'mean_reward': np.mean(eval_rewards),\n",
    "            'std_reward': np.std(eval_rewards),\n",
    "            'mean_length': np.mean(eval_lengths),\n",
    "            'mean_cost': np.mean(eval_costs),\n",
    "            'mean_skill_improvement': np.mean(eval_skill_improvements),\n",
    "            'mean_budget_utilization': np.mean(eval_budget_utilizations),\n",
    "            'success_rate': np.mean(eval_successes),\n",
    "            'budget_exceeded_rate': np.mean(eval_budget_exceeded),\n",
    "            'reward_improvement': np.mean(eval_rewards) - np.mean(self.episode_rewards[:100]) if len(self.episode_rewards) >= 100 else 0,\n",
    "            # NEW: Return raw data for Pareto plot\n",
    "            'raw_costs': eval_costs,\n",
    "            'raw_skill_improvements': eval_skill_improvements,\n",
    "        }\n",
    "        \n",
    "        print(\"Budget-Aware Evaluation Results:\")\n",
    "        print(f\"  Mean Reward: {results['mean_reward']:.3f} ± {results['std_reward']:.3f}\")\n",
    "        print(f\"  Mean Length: {results['mean_length']:.2f}\")\n",
    "        print(f\"  Mean Cost: {results['mean_cost']:.2f}\")\n",
    "        print(f\"  Mean Skill Improvement: {results['mean_skill_improvement']:.3f}\")\n",
    "        print(f\"  Mean Budget Utilization: {results['mean_budget_utilization']:.2%}\")\n",
    "        print(f\"  Success Rate: {results['success_rate']:.2%} 🎯\")\n",
    "        print(f\"  Budget Exceeded Rate: {results['budget_exceeded_rate']:.2%} 💰\")\n",
    "        print(f\"  Reward Improvement from Start: {results['reward_improvement']:+.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_policy_trajectory(self, action_trajectories: List[List[int]], save_path: str = None):\n",
    "        \"\"\"NEW: Visualize agent's action sequences as a heatmap.\"\"\"\n",
    "        print(\"\\nGenerating policy trajectory heatmap...\")\n",
    "\n",
    "        if not action_trajectories:\n",
    "            print(\"No trajectories to plot.\")\n",
    "            return\n",
    "\n",
    "        # Pad trajectories to the same length for the heatmap\n",
    "        max_len = max(len(t) for t in action_trajectories) if action_trajectories else 0\n",
    "        padded_trajectories = np.full((len(action_trajectories), max_len), -1.0)\n",
    "        for i, t in enumerate(action_trajectories):\n",
    "            padded_trajectories[i, :len(t)] = t\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Create a discrete colormap\n",
    "        cmap = cm.get_cmap('viridis', self.env.K)\n",
    "        \n",
    "        ax = sns.heatmap(\n",
    "            padded_trajectories,\n",
    "            cmap=cmap,\n",
    "            cbar_kws={'ticks': np.arange(self.env.K), 'label': 'Training Module Chosen'},\n",
    "            vmin=-0.5, vmax=self.env.K - 0.5,\n",
    "            linewidths=.5,\n",
    "            annot=False\n",
    "        )\n",
    "        \n",
    "        # Set colorbar labels\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_ticklabels([f'Module {i}' for i in range(self.env.K)])\n",
    "\n",
    "        plt.title(f'Policy Action Trajectories over {len(action_trajectories)} Evaluation Episodes')\n",
    "        plt.xlabel('Step within Episode')\n",
    "        plt.ylabel('Evaluation Episode')\n",
    "        \n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"-> Policy trajectory heatmap saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def plot_enhanced_training_curves(self, save_path: str = None) -> None:\n",
    "        \"\"\"Enhanced training curves with budget management and ROI analysis.\"\"\"\n",
    "        fig, axes = plt.subplots(4, 3, figsize=(20, 20)) # Increased grid size for ROI plot\n",
    "        fig.suptitle('Enhanced Training Analysis', fontsize=16, y=1.02)\n",
    "\n",
    "        # Episode rewards with milestone markers\n",
    "        axes[0, 0].plot(self.episode_rewards, alpha=0.7, label='Raw')\n",
    "        axes[0, 0].plot(self._smooth_curve(self.episode_rewards, 50), 'r-', linewidth=2, label='Smoothed')\n",
    "        \n",
    "        # Mark milestones\n",
    "        if self.first_positive_reward_episode:\n",
    "            axes[0, 0].axvline(self.first_positive_reward_episode, color='green', linestyle='--', \n",
    "                             label=f'First Positive ({self.first_positive_reward_episode})')\n",
    "        if self.first_80_percent_success:\n",
    "            axes[0, 0].axvline(self.first_80_percent_success, color='blue', linestyle='--',\n",
    "                             label=f'80% Success ({self.first_80_percent_success})')\n",
    "        if self.first_budget_discipline:\n",
    "            axes[0, 0].axvline(self.first_budget_discipline, color='purple', linestyle='--',\n",
    "                             label=f'Budget Discipline ({self.first_budget_discipline})')\n",
    "        \n",
    "        axes[0, 0].set_title('Episode Rewards with Budget Milestones')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Reward')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Success rate with budget analysis\n",
    "        success_rate = self._smooth_curve([int(s) for s in self.success_episodes], 50)\n",
    "        budget_exceeded_rate = self._smooth_curve([int(b) for b in self.budget_exceeded_episodes], 50)\n",
    "        axes[0, 1].plot(success_rate, 'g-', linewidth=2, label='Success Rate')\n",
    "        axes[0, 1].plot(budget_exceeded_rate, 'r-', linewidth=2, label='Budget Exceeded Rate')\n",
    "        axes[0, 1].axhline(0.2, color='orange', linestyle=':', label='Budget Discipline Target (20%)')\n",
    "        axes[0, 1].set_title('Success vs Budget Management')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Rate')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Learning rate with decay markers\n",
    "        axes[0, 2].plot(self.learning_rates, 'b-', linewidth=2)\n",
    "        # Mark LR decay steps\n",
    "        for decay_step in self.agent.training_stats.get('lr_decay_steps', []):\n",
    "            if decay_step < len(self.learning_rates):\n",
    "                axes[0, 2].axvline(decay_step, color='red', linestyle=':', alpha=0.7)\n",
    "        axes[0, 2].set_title('Learning Rate Schedule')\n",
    "        axes[0, 2].set_xlabel('Episode')\n",
    "        axes[0, 2].set_ylabel('Learning Rate')\n",
    "        axes[0, 2].set_yscale('log')\n",
    "        axes[0, 2].grid(True)\n",
    "        \n",
    "        # Entropy evolution\n",
    "        axes[1, 0].plot(self.entropy_losses, alpha=0.7, label='Raw')\n",
    "        axes[1, 0].plot(self._smooth_curve(self.entropy_losses, 50), 'r-', linewidth=2, label='Smoothed')\n",
    "        axes[1, 0].axhline(np.log(self.env.K), color='black', linestyle='--', \n",
    "                          label=f'Max Entropy ({np.log(self.env.K):.3f})')\n",
    "        axes[1, 0].set_title('Policy Entropy Evolution')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Entropy')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Budget utilization with improved emphasis\n",
    "        axes[1, 1].plot(self.budget_utilizations, alpha=0.7, label='Raw')\n",
    "        axes[1, 1].plot(self._smooth_curve(self.budget_utilizations, 50), 'r-', linewidth=2, label='Smoothed')\n",
    "        axes[1, 1].axhline(1.0, color='red', linestyle='--', linewidth=2, label='Budget Limit (100%)')\n",
    "        axes[1, 1].axhline(0.8, color='orange', linestyle=':', label='Efficient Target (80%)')\n",
    "        axes[1, 1].set_title('Budget Utilization (Key Metric)')\n",
    "        axes[1, 1].set_xlabel('Episode')\n",
    "        axes[1, 1].set_ylabel('Budget Used %')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        # Skill improvements\n",
    "        axes[1, 2].plot(self.skill_improvements, alpha=0.7)\n",
    "        axes[1, 2].plot(self._smooth_curve(self.skill_improvements, 50), 'r-', linewidth=2)\n",
    "        axes[1, 2].set_title('Skill Improvements')\n",
    "        axes[1, 2].set_xlabel('Episode')\n",
    "        axes[1, 2].set_ylabel('Total Skill Δ')\n",
    "        axes[1, 2].grid(True)\n",
    "        \n",
    "        # Policy and value losses\n",
    "        axes[2, 0].plot(self.policy_losses, alpha=0.7)\n",
    "        axes[2, 0].plot(self._smooth_curve(self.policy_losses, 50), 'r-', linewidth=2)\n",
    "        axes[2, 0].set_title('Policy Losses')\n",
    "        axes[2, 0].set_xlabel('Episode')\n",
    "        axes[2, 0].set_ylabel('Loss')\n",
    "        axes[2, 0].grid(True)\n",
    "        \n",
    "        if self.agent.use_baseline:\n",
    "            axes[2, 1].plot(self.value_losses, alpha=0.7)\n",
    "            axes[2, 1].plot(self._smooth_curve(self.value_losses, 50), 'r-', linewidth=2)\n",
    "            axes[2, 1].set_title('Value Losses')\n",
    "            axes[2, 1].set_xlabel('Episode')\n",
    "            axes[2, 1].set_ylabel('Loss')\n",
    "            axes[2, 1].grid(True)\n",
    "        else:\n",
    "            axes[2, 1].text(0.5, 0.5, 'No Baseline Used', ha='center', va='center', \n",
    "                           transform=axes[2, 1].transAxes, fontsize=14)\n",
    "            axes[2, 1].set_title('Value Losses')\n",
    "        \n",
    "        # Budget management effectiveness\n",
    "        budget_effectiveness = []\n",
    "        for i in range(len(self.episode_rewards)):\n",
    "            if i >= 99:  # Calculate over last 100 episodes\n",
    "                recent_success = np.mean(self.success_episodes[i-99:i+1])\n",
    "                recent_budget_ok = 1 - np.mean(self.budget_exceeded_episodes[i-99:i+1])\n",
    "                effectiveness = (recent_success + recent_budget_ok) / 2  # Combined metric\n",
    "                budget_effectiveness.append(effectiveness)\n",
    "        \n",
    "        if budget_effectiveness:\n",
    "            axes[2, 2].plot(range(99, len(self.episode_rewards)), budget_effectiveness, 'g-', linewidth=2)\n",
    "            axes[2, 2].axhline(0.8, color='blue', linestyle='--', label='Target (80%)')\n",
    "            axes[2, 2].set_title('Budget Management Effectiveness')\n",
    "            axes[2, 2].set_xlabel('Episode')\n",
    "            axes[2, 2].set_ylabel('Effectiveness Score')\n",
    "            axes[2, 2].legend()\n",
    "            axes[2, 2].grid(True)\n",
    "\n",
    "        # NEW: Return on Investment (ROI) Plot\n",
    "        axes[3, 0].plot(self.episode_rois, alpha=0.7, label='Raw')\n",
    "        axes[3, 0].plot(self._smooth_curve(self.episode_rois, 50), 'g-', linewidth=2, label='Smoothed')\n",
    "        axes[3, 0].set_title('Return on Investment (ROI)')\n",
    "        axes[3, 0].set_xlabel('Episode')\n",
    "        axes[3, 0].set_ylabel('Skill Improve / Cost')\n",
    "        axes[3, 0].legend()\n",
    "        axes[3, 0].grid(True)\n",
    "        \n",
    "        # Turn off unused subplots\n",
    "        axes[3, 1].axis('off')\n",
    "        axes[3, 2].axis('off')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "        \n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Budget-aware training curves saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def _smooth_curve(self, values: List[float], window: int = 50) -> List[float]:\n",
    "        \"\"\"Apply moving average smoothing to a curve.\"\"\"\n",
    "        if len(values) < window:\n",
    "            return values\n",
    "        \n",
    "        smoothed = []\n",
    "        for i in range(len(values)):\n",
    "            start = max(0, i - window + 1)\n",
    "            end = i + 1\n",
    "            smoothed.append(np.mean(values[start:end]))\n",
    "        \n",
    "        return smoothed\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE COMPARISON FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "class PerformanceComparison:\n",
    "    \"\"\"Framework for comparing budget-aware configurations.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_config: TrainingConfig):\n",
    "        self.base_config = base_config\n",
    "        self.results = {}\n",
    "        \n",
    "    def run_comparison(self, configurations: Dict[str, TrainingConfig], episodes: int = 1500) -> Dict:\n",
    "        \"\"\"Run training with different budget-aware configurations.\"\"\"\n",
    "        print(\"=== Budget-Aware Performance Comparison Framework ===\")\n",
    "        print(f\"Testing {len(configurations)} configurations with {episodes} episodes each\")\n",
    "        print(\"Focus: Budget management and cost-effectiveness\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for config_name, config in configurations.items():\n",
    "            print(f\"\\nTesting Configuration: {config_name}\")\n",
    "            print(f\"  Reward Strategy: {config.reward_strategy}\")\n",
    "            print(f\"  Cost Penalty: {config.cost_penalty}\")\n",
    "            print(f\"  Budget Penalties: base={config.base_budget_penalty}, max={config.max_budget_penalty}\")\n",
    "            print(f\"  Entropy Coefficient: {config.entropy_coefficient}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Update episodes for comparison\n",
    "            config.num_episodes = episodes\n",
    "            config.log_interval = max(episodes // 20, 25)\n",
    "            \n",
    "            # Create environment and agent\n",
    "            env = EmployeeTrainingEnv(config)\n",
    "            agent = EnhancedREINFORCEAgent(config)\n",
    "            training_loop = EnhancedTrainingLoop(env, agent, config)\n",
    "            \n",
    "            # Train\n",
    "            start_time = time.time()\n",
    "            training_loop.train()\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_results = training_loop.evaluate(num_episodes=100, render=False, visualize_trajectory=False)\n",
    "            \n",
    "            # Store results with budget-focused metrics\n",
    "            results[config_name] = {\n",
    "                'config': config,\n",
    "                'training_time': training_time,\n",
    "                'final_rewards': training_loop.episode_rewards[-100:],\n",
    "                'final_success_rate': np.mean(training_loop.success_episodes[-100:]),\n",
    "                'final_budget_exceeded_rate': np.mean(training_loop.budget_exceeded_episodes[-100:]),\n",
    "                'final_entropy': np.mean(training_loop.entropy_losses[-100:]),\n",
    "                'budget_discipline_episode': training_loop.first_budget_discipline,\n",
    "                'eval_results': eval_results,\n",
    "                'training_loop': training_loop\n",
    "            }\n",
    "            \n",
    "            print(f\"  Training Time: {training_time:.2f}s\")\n",
    "            print(f\"  Final Success Rate: {eval_results['success_rate']:.2%}\")\n",
    "            print(f\"  Final Budget Exceeded Rate: {eval_results['budget_exceeded_rate']:.2%}\")\n",
    "            print(f\"  Final Mean Reward: {eval_results['mean_reward']:.3f}\")\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"Print budget-focused comparison summary.\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to summarize. Run comparison first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"BUDGET-AWARE PERFORMANCE COMPARISON SUMMARY\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Sort by success rate (primary) and low budget exceeded rate (secondary)\n",
    "        sorted_results = sorted(self.results.items(), \n",
    "                              key=lambda x: (x[1]['eval_results']['success_rate'], \n",
    "                                           -x[1]['eval_results']['budget_exceeded_rate']), \n",
    "                              reverse=True)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"{'Rank':<4} {'Configuration':<20} {'Success Rate':<12} {'Budget Exceed':<12} \"\n",
    "              f\"{'Mean Reward':<12} {'Budget Discipline':<16}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Results\n",
    "        for i, (config_name, result) in enumerate(sorted_results, 1):\n",
    "            eval_results = result['eval_results']\n",
    "            budget_discipline = result['budget_discipline_episode'] or \"Not achieved\"\n",
    "            \n",
    "            print(f\"{i:<4} {config_name:<20} {eval_results['success_rate']:<12.2%} \"\n",
    "                  f\"{eval_results['budget_exceeded_rate']:<12.2%} {eval_results['mean_reward']:<12.3f} \"\n",
    "                  f\"{str(budget_discipline):<16}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"WINNER (Best Budget Management):\", sorted_results[0][0])\n",
    "        winner_config = sorted_results[0][1]['config']\n",
    "        winner_result = sorted_results[0][1]\n",
    "        \n",
    "        print(f\"  Reward Strategy: {winner_config.reward_strategy}\")\n",
    "        print(f\"  Cost Penalty: {winner_config.cost_penalty}\")\n",
    "        print(f\"  Budget Penalties: base={winner_config.base_budget_penalty}, max={winner_config.max_budget_penalty}\")\n",
    "        print(f\"  Success Rate: {winner_result['eval_results']['success_rate']:.2%}\")\n",
    "        print(f\"  Budget Exceeded Rate: {winner_result['eval_results']['budget_exceeded_rate']:.2%}\")\n",
    "        \n",
    "        if winner_result['budget_discipline_episode']:\n",
    "            print(f\"  Budget Discipline Achieved: Episode {winner_result['budget_discipline_episode']}\")\n",
    "\n",
    "    def plot_comparison_pareto(self, save_path: str = None):\n",
    "        \"\"\"NEW: Plot a Pareto front comparing Cost vs. Skill Improvement for all configs.\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No comparison results to plot.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nGenerating Pareto front comparison plot...\")\n",
    "        plt.figure(figsize=(14, 9))\n",
    "        \n",
    "        colors = cm.get_cmap('tab10', len(self.results))\n",
    "        \n",
    "        for i, (config_name, result) in enumerate(self.results.items()):\n",
    "            eval_res = result.get('eval_results', {})\n",
    "            costs = eval_res.get('raw_costs')\n",
    "            improvements = eval_res.get('raw_skill_improvements')\n",
    "            \n",
    "            if costs and improvements:\n",
    "                plt.scatter(costs, improvements, alpha=0.6, s=50, color=colors(i), label=config_name)\n",
    "            else:\n",
    "                print(f\"Warning: No evaluation data for Pareto plot for '{config_name}'.\")\n",
    "\n",
    "        plt.title('Pareto Front: Cost vs. Skill Improvement across Configurations', fontsize=16)\n",
    "        plt.xlabel('Total Episode Cost', fontsize=12)\n",
    "        plt.ylabel('Total Skill Improvement', fontsize=12)\n",
    "        plt.legend(title='Configurations', fontsize=10)\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        \n",
    "        # Highlight the efficient frontier (top-left)\n",
    "        ax = plt.gca()\n",
    "        ax.text(0.05, 0.95, 'More Efficient Policies →', transform=ax.transAxes, fontsize=14,\n",
    "                verticalalignment='top', horizontalalignment='left',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"-> Pareto front plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION AND UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"Create necessary directories for saving models and plots.\"\"\"\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "\n",
    "def get_budget_aware_configurations() -> Dict[str, TrainingConfig]:\n",
    "    \"\"\"\n",
    "    Get refined budget-aware configurations to find the optimal trade-off \n",
    "    between high reward and strict budget management.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Base parameters that work well ---\n",
    "    base_params = {\n",
    "        'learning_rate': 3e-4,\n",
    "        'entropy_coefficient': 0.001,\n",
    "        'lr_step_size': 1500, # Give more time to learn before LR decay\n",
    "        'lr_gamma': 0.9,\n",
    "        'reward_strategy': 'hybrid' # This strategy has proven effective\n",
    "    }\n",
    "\n",
    "    # --- Create a modified config for the fast learner ---\n",
    "    fast_learner_params = base_params.copy()\n",
    "    fast_learner_params['learning_rate'] = 5e-4 # Override the learning rate here\n",
    "\n",
    "    return {\n",
    "        # Hypothesis: A balanced approach. A modest increase in cost penalty \n",
    "        # might be enough to improve budget discipline without hurting rewards too much.\n",
    "        'balanced_approach': TrainingConfig(\n",
    "            **base_params,\n",
    "            cost_penalty=0.01, # 5x the original\n",
    "            base_budget_penalty=4.0,\n",
    "            max_budget_penalty=8.0\n",
    "        ),\n",
    "\n",
    "        # Hypothesis: Slightly more aggressive on cost. This is the configuration\n",
    "        # recommended from the last analysis. It should further reduce budget overruns.\n",
    "        'strong_incentive': TrainingConfig(\n",
    "            **base_params,\n",
    "            cost_penalty=0.015, # 7.5x the original\n",
    "            base_budget_penalty=5.0,\n",
    "            max_budget_penalty=10.0\n",
    "        ),\n",
    "\n",
    "        # Hypothesis: Very conservative. This agent should almost never exceed the budget.\n",
    "        # The key question is: will it be too scared to take actions, resulting in low skill gain?\n",
    "        'ultra_conservative': TrainingConfig(\n",
    "            **base_params,\n",
    "            cost_penalty=0.025, # Over 10x the original penalty\n",
    "            base_budget_penalty=6.0,\n",
    "            max_budget_penalty=12.0\n",
    "        ),\n",
    "\n",
    "        # Hypothesis: Focus on the terminal reward. Maybe a huge bonus for finishing\n",
    "        # with budget to spare is a better signal than a punishing per-step cost.\n",
    "        'terminal_focus_strong': TrainingConfig(\n",
    "            reward_strategy='terminal',\n",
    "            cost_penalty=0.008, # Lower step penalty, as the focus is on the end\n",
    "            entropy_coefficient=0.001,\n",
    "            learning_rate=3e-4,\n",
    "            base_budget_penalty=5.0,\n",
    "            max_budget_penalty=10.0,\n",
    "            terminal_bonus_multiplier=3.5 # Greatly increased terminal bonus\n",
    "        ),\n",
    "        \n",
    "        # FIXED: Unpack the modified dictionary to avoid the TypeError\n",
    "        'fast_learner_frugal': TrainingConfig(\n",
    "            **fast_learner_params, # Use the modified params\n",
    "            cost_penalty=0.02,  # High cost penalty\n",
    "            base_budget_penalty=5.0,\n",
    "            max_budget_penalty=10.0\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Budget-aware main entry point with enhanced cost management.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Budget-Aware Employee Training Optimization')\n",
    "    \n",
    "    # --- Existing Arguments ---\n",
    "    parser.add_argument('--mode', choices=['train', 'evaluate', 'visualize', 'compare'], \n",
    "                       default='train', help='Run mode')\n",
    "    parser.add_argument('--model', type=str, help='Path to saved model for evaluation')\n",
    "    parser.add_argument('--episodes', type=int, default=3000, help='Number of training episodes')\n",
    "    parser.add_argument('--reward-strategy', choices=['basic', 'terminal', 'efficiency', 'hybrid'], \n",
    "                       default='hybrid', help='Reward strategy to use')\n",
    "    parser.add_argument('--cost-penalty', type=float, default=0.01, help='Cost penalty coefficient')\n",
    "    parser.add_argument('--entropy-coef', type=float, default=0.001, help='Entropy regularization coefficient')\n",
    "    parser.add_argument('--learning-rate', type=float, default=3e-4, help='Learning rate')\n",
    "    parser.add_argument('--lr-step-size', type=int, default=750, help='Learning rate decay step size')\n",
    "    parser.add_argument('--lr-gamma', type=float, default=0.9, help='Learning rate decay factor')\n",
    "    parser.add_argument('--no-baseline', action='store_true', help='Disable baseline (Actor-Critic)')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "    \n",
    "    # --- FIXED: Add the missing arguments ---\n",
    "    parser.add_argument('--base-budget-penalty', type=float, default=4.0, help='Base penalty for exceeding budget')\n",
    "    parser.add_argument('--max-budget-penalty', type=float, default=8.0, help='Maximum penalty for exceeding budget')\n",
    "    parser.add_argument('--terminal-bonus-multiplier', type=float, default=1.5, help='Multiplier for terminal bonus in reward function')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    # Create directories\n",
    "    create_directories()\n",
    "    \n",
    "    print(\"Budget-Aware Employee Training Optimization System\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Mode: {args.mode}\")\n",
    "    print(f\"Seed: {args.seed}\")\n",
    "    print(f\"User: Soumedhik\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if args.mode == 'compare':\n",
    "        # Performance comparison mode for budget-aware configurations\n",
    "        print(\"Running performance comparison with budget-aware configurations...\")\n",
    "        \n",
    "        base_config = TrainingConfig()\n",
    "        comparison = PerformanceComparison(base_config)\n",
    "        configurations = get_budget_aware_configurations()\n",
    "        \n",
    "        # Run the comparison\n",
    "        results = comparison.run_comparison(configurations, episodes=args.episodes)\n",
    "        \n",
    "        # Summarize and present the results\n",
    "        comparison.print_summary()\n",
    "        # NEW: Plot the pareto front\n",
    "        comparison.plot_comparison_pareto(\n",
    "            save_path=os.path.join(base_config.plot_save_path, 'comparison_pareto_front.png')\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Create a single configuration for train, evaluate, or visualize modes\n",
    "        # FIXED: Pass all the newly added arguments to the config\n",
    "        config = TrainingConfig(\n",
    "            num_episodes=args.episodes,\n",
    "            reward_strategy=args.reward_strategy,\n",
    "            cost_penalty=args.cost_penalty,\n",
    "            entropy_coefficient=args.entropy_coef,\n",
    "            learning_rate=args.learning_rate,\n",
    "            lr_step_size=args.lr_step_size,\n",
    "            lr_gamma=args.lr_gamma,\n",
    "            use_baseline=not args.no_baseline,\n",
    "            base_budget_penalty=args.base_budget_penalty,\n",
    "            max_budget_penalty=args.max_budget_penalty,\n",
    "            terminal_bonus_multiplier=args.terminal_bonus_multiplier\n",
    "        )\n",
    "        \n",
    "        # Instantiate environment and agent\n",
    "        env = EmployeeTrainingEnv(config)\n",
    "        agent = EnhancedREINFORCEAgent(config)\n",
    "        training_loop = EnhancedTrainingLoop(env, agent, config)\n",
    "        \n",
    "        print(f\"Environment: {env.D} skills, {env.K} training modules\")\n",
    "        print(f\"Agent: {'Enhanced Actor-Critic' if config.use_baseline else 'Enhanced REINFORCE'}\")\n",
    "        print(f\"Reward Strategy: {config.reward_strategy}\")\n",
    "        print(f\"Cost Penalty: {config.cost_penalty}\")\n",
    "        print(f\"Learning Rate: {config.learning_rate} (StepLR: {config.lr_step_size}/{config.lr_gamma})\")\n",
    "        \n",
    "        if args.mode == 'train':\n",
    "            # Training mode\n",
    "            print(\"\\nStarting budget-aware training...\")\n",
    "            training_loop.train()\n",
    "            \n",
    "            # Plot the training curves\n",
    "            training_loop.plot_enhanced_training_curves(\n",
    "                save_path=os.path.join(config.plot_save_path, 'budget_aware_training_curves.png')\n",
    "            )\n",
    "            \n",
    "            # Evaluate the newly trained model\n",
    "            print(\"\\nEvaluating trained model...\")\n",
    "            eval_results = training_loop.evaluate(\n",
    "                num_episodes=config.eval_episodes,\n",
    "                render=config.eval_render,\n",
    "                visualize_trajectory=True # NEW: Visualize final policy\n",
    "            )\n",
    "            \n",
    "        elif args.mode == 'evaluate':\n",
    "            # Evaluation mode\n",
    "            if args.model:\n",
    "                agent.load_model(args.model)\n",
    "                print(f\"Loaded model from {args.model}\")\n",
    "            else:\n",
    "                print(\"Warning: No model specified for evaluation. Using a randomly initialized policy.\")\n",
    "            \n",
    "            eval_results = training_loop.evaluate(\n",
    "                num_episodes=config.eval_episodes,\n",
    "                render=True,\n",
    "                visualize_trajectory=True # NEW: Also visualize on standalone eval\n",
    "            )\n",
    "            \n",
    "        elif args.mode == 'visualize':\n",
    "            # Visualization mode to inspect a trained agent's behavior\n",
    "            if args.model:\n",
    "                agent.load_model(args.model)\n",
    "                print(f\"Loaded model from {args.model}\")\n",
    "            else:\n",
    "                print(\"Warning: No model specified for visualization. Using a randomly initialized policy.\")\n",
    "\n",
    "            print(\"\\nGenerating visualizations for the agent's policy...\")\n",
    "\n",
    "            # --- 1. Visualize Skill Progression on Sample Episodes ---\n",
    "            num_viz_episodes = 5\n",
    "            skill_histories = []\n",
    "            plt.figure(figsize=(12, 7))\n",
    "\n",
    "            for i in range(num_viz_episodes):\n",
    "                state, _ = env.reset()\n",
    "                episode_skills = [state.copy()]\n",
    "                \n",
    "                while True:\n",
    "                    with torch.no_grad():\n",
    "                        action_probs = agent.policy_net(torch.FloatTensor(state).unsqueeze(0))\n",
    "                        action = torch.argmax(action_probs).item()\n",
    "                    \n",
    "                    state, _, terminated, truncated, _ = env.step(action)\n",
    "                    episode_skills.append(state.copy())\n",
    "\n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                \n",
    "                skill_histories.append(np.array(episode_skills))\n",
    "\n",
    "            # Plot average skill level over time for each sample episode\n",
    "            for i, history in enumerate(skill_histories):\n",
    "                avg_skill_per_step = np.mean(history, axis=1)\n",
    "                plt.plot(avg_skill_per_step, alpha=0.8, label=f'Sample Run {i+1}')\n",
    "            \n",
    "            plt.title('Agent Policy: Average Skill Progression')\n",
    "            plt.xlabel('Training Step in Episode')\n",
    "            plt.ylabel('Average Skill Level')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            viz_path = os.path.join(config.plot_save_path, 'visualized_skill_progression.png')\n",
    "            plt.savefig(viz_path, bbox_inches='tight')\n",
    "            print(f\"-> Skill progression plot saved to {viz_path}\")\n",
    "            plt.show()\n",
    "\n",
    "            # --- 2. Visualize the Synergy Matrix ---\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            skill_names = [\n",
    "                'Coding', 'Debugging', 'Testing', 'Arch.',\n",
    "                'Comm.', 'Leader.', 'Teamwork', 'Prob. Solv.'\n",
    "            ]\n",
    "            sns.heatmap(env.synergy_matrix, xticklabels=skill_names, yticklabels=skill_names,\n",
    "                       annot=True, fmt='.2f', cmap='viridis', cbar_kws={'label': 'Synergy Coefficient'})\n",
    "            plt.title('Environment: Cross-Attribute Synergy Matrix')\n",
    "            synergy_path = os.path.join(config.plot_save_path, 'visualized_synergy_matrix.png')\n",
    "            plt.savefig(synergy_path, bbox_inches='tight')\n",
    "            print(f\"-> Synergy matrix plot saved to {synergy_path}\")\n",
    "            plt.show()\n",
    "\n",
    "            print(\"\\nVisualizations generated and saved to 'plots/' directory.\")\n",
    "\n",
    "    print(\"\\nEnhanced system execution completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555516d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T07:36:20.185130Z",
     "iopub.status.busy": "2025-07-10T07:36:20.184901Z",
     "iopub.status.idle": "2025-07-10T10:38:18.902171Z",
     "shell.execute_reply": "2025-07-10T10:38:18.901433Z"
    },
    "papermill": {
     "duration": 10918.722158,
     "end_time": "2025-07-10T10:38:18.903796",
     "exception": false,
     "start_time": "2025-07-10T07:36:20.181638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget-Aware Employee Training Optimization System\r\n",
      "================================================================================\r\n",
      "Mode: compare\r\n",
      "Seed: 42\r\n",
      "User: Soumedhik\r\n",
      "================================================================================\r\n",
      "Running performance comparison with budget-aware configurations...\r\n",
      "=== Budget-Aware Performance Comparison Framework ===\r\n",
      "Testing 5 configurations with 100000 episodes each\r\n",
      "Focus: Budget management and cost-effectiveness\r\n",
      "======================================================================\r\n",
      "\r\n",
      "Testing Configuration: balanced_approach\r\n",
      "  Reward Strategy: hybrid\r\n",
      "  Cost Penalty: 0.01\r\n",
      "  Budget Penalties: base=4.0, max=8.0\r\n",
      "  Entropy Coefficient: 0.001\r\n",
      "--------------------------------------------------\r\n",
      "Initialized budget-aware agent with LR=0.0003, cost_penalty=0.01, step_size=1500\r\n",
      "Starting Budget-Aware Training with Enhanced Cost Penalties...\r\n",
      "Episodes: 100000\r\n",
      "Environment: 8 skills, 4 training modules\r\n",
      "Agent: Actor-Critic with entropy regularization\r\n",
      "Reward Strategy: hybrid\r\n",
      "Learning Rate: 0.0003 (StepLR: step=1500, gamma=0.9)\r\n",
      "Cost Penalty: 0.01 (5x increased for budget awareness)\r\n",
      "Budget Penalties: base=4.0, max=8.0\r\n",
      "------------------------------------------------------------------------------------------\r\n",
      "🎉 First positive reward achieved at episode 9!\r\n",
      "💾 Model saved at episode 500\r\n",
      "💾 Model saved at episode 1000\r\n",
      "💾 Model saved at episode 1500\r\n",
      "💾 Model saved at episode 2000\r\n",
      "💾 Model saved at episode 2500\r\n",
      "💾 Model saved at episode 3000\r\n",
      "💾 Model saved at episode 3500\r\n",
      "💾 Model saved at episode 4000\r\n",
      "💾 Model saved at episode 4500\r\n",
      "Episode 5000 | Reward:   3.45 | Avg:  -0.27 📉 | Success: 44.00% | Budget Exceeded: 56.00% 📈 | Entropy: 0.605 📈 | LR: 2.19e-04\r\n",
      "💾 Model saved at episode 5000\r\n",
      "💾 Model saved at episode 5500\r\n",
      "💾 Model saved at episode 6000\r\n",
      "💾 Model saved at episode 6500\r\n",
      "💾 Model saved at episode 7000\r\n",
      "💾 Model saved at episode 7500\r\n",
      "💾 Model saved at episode 8000\r\n",
      "💾 Model saved at episode 8500\r\n",
      "💾 Model saved at episode 9000\r\n",
      "💾 Model saved at episode 9500\r\n",
      "Episode 10000 | Reward:  -2.67 | Avg:  -0.26 📉 | Success: 45.00% | Budget Exceeded: 55.00% 📈 | Entropy: 0.509 📉 | LR: 1.59e-04\r\n",
      "💾 Model saved at episode 10000\r\n",
      "💾 Model saved at episode 10500\r\n",
      "💾 Model saved at episode 11000\r\n",
      "💾 Model saved at episode 11500\r\n",
      "💾 Model saved at episode 12000\r\n",
      "💾 Model saved at episode 12500\r\n",
      "💾 Model saved at episode 13000\r\n",
      "💾 Model saved at episode 13500\r\n",
      "💾 Model saved at episode 14000\r\n",
      "💾 Model saved at episode 14500\r\n",
      "Episode 15000 | Reward:  -3.44 | Avg:  -0.06 📉 | Success: 48.00% | Budget Exceeded: 52.00% 📈 | Entropy: 0.450 📉 | LR: 1.05e-04\r\n",
      "💾 Model saved at episode 15000\r\n",
      "💾 Model saved at episode 15500\r\n",
      "💾 Model saved at episode 16000\r\n",
      "💾 Model saved at episode 16500\r\n",
      "💾 Model saved at episode 17000\r\n",
      "💾 Model saved at episode 17500\r\n",
      "💾 Model saved at episode 18000\r\n",
      "💾 Model saved at episode 18500\r\n",
      "💾 Model saved at episode 19000\r\n",
      "💾 Model saved at episode 19500\r\n",
      "Episode 20000 | Reward:  -2.54 | Avg:  -0.01 📈 | Success: 49.00% | Budget Exceeded: 51.00% 📉 | Entropy: 0.434 📈 | LR: 7.63e-05\r\n",
      "💾 Model saved at episode 20000\r\n",
      "💾 Model saved at episode 20500\r\n",
      "💾 Model saved at episode 21000\r\n",
      "💾 Model saved at episode 21500\r\n",
      "💾 Model saved at episode 22000\r\n",
      "💾 Model saved at episode 22500\r\n",
      "💾 Model saved at episode 23000\r\n",
      "💾 Model saved at episode 23500\r\n",
      "💾 Model saved at episode 24000\r\n",
      "💾 Model saved at episode 24500\r\n",
      "Episode 25000 | Reward:  -3.10 | Avg:   0.61 📉 | Success: 60.00% | Budget Exceeded: 40.00% 📈 | Entropy: 0.422 📉 | LR: 5.56e-05\r\n",
      "💾 Model saved at episode 25000\r\n",
      "💾 Model saved at episode 25500\r\n",
      "💾 Model saved at episode 26000\r\n",
      "💾 Model saved at episode 26500\r\n",
      "💾 Model saved at episode 27000\r\n",
      "💾 Model saved at episode 27500\r\n",
      "💾 Model saved at episode 28000\r\n",
      "💾 Model saved at episode 28500\r\n",
      "💾 Model saved at episode 29000\r\n",
      "💾 Model saved at episode 29500\r\n",
      "Episode 30000 | Reward:   3.10 | Avg:   0.38 📉 | Success: 57.00% | Budget Exceeded: 43.00% 📈 | Entropy: 0.403 📈 | LR: 3.65e-05\r\n",
      "💾 Model saved at episode 30000\r\n",
      "💾 Model saved at episode 30500\r\n",
      "💾 Model saved at episode 31000\r\n",
      "💾 Model saved at episode 31500\r\n",
      "💾 Model saved at episode 32000\r\n",
      "💾 Model saved at episode 32500\r\n",
      "💾 Model saved at episode 33000\r\n",
      "💾 Model saved at episode 33500\r\n",
      "💾 Model saved at episode 34000\r\n",
      "💾 Model saved at episode 34500\r\n",
      "Episode 35000 | Reward:  -2.23 | Avg:   1.13 📉 | Success: 70.00% | Budget Exceeded: 30.00% 📈 | Entropy: 0.392 📈 | LR: 2.66e-05\r\n",
      "💾 Model saved at episode 35000\r\n",
      "💾 Model saved at episode 35500\r\n",
      "💾 Model saved at episode 36000\r\n",
      "💾 Model saved at episode 36500\r\n",
      "💾 Model saved at episode 37000\r\n",
      "💾 Model saved at episode 37500\r\n",
      "💾 Model saved at episode 38000\r\n",
      "💾 Model saved at episode 38500\r\n",
      "💾 Model saved at episode 39000\r\n",
      "💾 Model saved at episode 39500\r\n",
      "Episode 40000 | Reward:   2.87 | Avg:   0.43 📉 | Success: 58.00% | Budget Exceeded: 42.00% 📈 | Entropy: 0.386 📈 | LR: 1.94e-05\r\n",
      "💾 Model saved at episode 40000\r\n",
      "💾 Model saved at episode 40500\r\n",
      "💾 Model saved at episode 41000\r\n",
      "💾 Model saved at episode 41500\r\n",
      "💾 Model saved at episode 42000\r\n",
      "💾 Model saved at episode 42500\r\n",
      "💾 Model saved at episode 43000\r\n",
      "💾 Model saved at episode 43500\r\n",
      "💾 Model saved at episode 44000\r\n",
      "💾 Model saved at episode 44500\r\n",
      "🎯 80% success rate achieved at episode 44616!\r\n",
      "💰 Budget discipline achieved (≤20% exceeded) at episode 44616!\r\n",
      "Episode 45000 | Reward:  -2.58 | Avg:   0.56 📉 | Success: 60.00% | Budget Exceeded: 40.00% 📈 | Entropy: 0.382 📉 | LR: 1.27e-05\r\n",
      "💾 Model saved at episode 45000\r\n",
      "💾 Model saved at episode 45500\r\n",
      "💾 Model saved at episode 46000\r\n",
      "💾 Model saved at episode 46500\r\n",
      "💾 Model saved at episode 47000\r\n",
      "💾 Model saved at episode 47500\r\n",
      "💾 Model saved at episode 48000\r\n",
      "💾 Model saved at episode 48500\r\n",
      "💾 Model saved at episode 49000\r\n",
      "💾 Model saved at episode 49500\r\n",
      "Episode 50000 | Reward:   3.04 | Avg:   1.14 📈 | Success: 70.00% | Budget Exceeded: 30.00% 📉 | Entropy: 0.375 📉 | LR: 9.27e-06\r\n",
      "💾 Model saved at episode 50000\r\n",
      "💾 Model saved at episode 50500\r\n",
      "💾 Model saved at episode 51000\r\n",
      "💾 Model saved at episode 51500\r\n",
      "💾 Model saved at episode 52000\r\n",
      "💾 Model saved at episode 52500\r\n",
      "💾 Model saved at episode 53000\r\n",
      "💾 Model saved at episode 53500\r\n",
      "💾 Model saved at episode 54000\r\n",
      "💾 Model saved at episode 54500\r\n",
      "Episode 55000 | Reward:   3.36 | Avg:   1.12 📉 | Success: 70.00% | Budget Exceeded: 30.00% 📈 | Entropy: 0.378 📈 | LR: 6.76e-06\r\n",
      "💾 Model saved at episode 55000\r\n",
      "💾 Model saved at episode 55500\r\n",
      "💾 Model saved at episode 56000\r\n",
      "💾 Model saved at episode 56500\r\n",
      "💾 Model saved at episode 57000\r\n",
      "💾 Model saved at episode 57500\r\n",
      "💾 Model saved at episode 58000\r\n",
      "💾 Model saved at episode 58500\r\n",
      "💾 Model saved at episode 59000\r\n",
      "💾 Model saved at episode 59500\r\n",
      "Episode 60000 | Reward:   3.41 | Avg:   1.02 📉 | Success: 68.00% | Budget Exceeded: 32.00% 📈 | Entropy: 0.378 📈 | LR: 4.43e-06\r\n",
      "💾 Model saved at episode 60000\r\n",
      "💾 Model saved at episode 60500\r\n",
      "💾 Model saved at episode 61000\r\n",
      "💾 Model saved at episode 61500\r\n",
      "💾 Model saved at episode 62000\r\n",
      "💾 Model saved at episode 62500\r\n",
      "💾 Model saved at episode 63000\r\n",
      "💾 Model saved at episode 63500\r\n",
      "💾 Model saved at episode 64000\r\n",
      "💾 Model saved at episode 64500\r\n",
      "Episode 65000 | Reward:   2.79 | Avg:   1.14 📉 | Success: 70.00% | Budget Exceeded: 30.00% 📈 | Entropy: 0.374 📈 | LR: 3.23e-06\r\n",
      "💾 Model saved at episode 65000\r\n",
      "💾 Model saved at episode 65500\r\n",
      "💾 Model saved at episode 66000\r\n",
      "💾 Model saved at episode 66500\r\n",
      "💾 Model saved at episode 67000\r\n",
      "💾 Model saved at episode 67500\r\n",
      "💾 Model saved at episode 68000\r\n",
      "💾 Model saved at episode 68500\r\n",
      "💾 Model saved at episode 69000\r\n",
      "💾 Model saved at episode 69500\r\n",
      "Episode 70000 | Reward:  -2.57 | Avg:   0.68 📈 | Success: 62.00% | Budget Exceeded: 38.00% 📉 | Entropy: 0.370 📉 | LR: 2.36e-06\r\n",
      "💾 Model saved at episode 70000\r\n",
      "💾 Model saved at episode 70500\r\n",
      "💾 Model saved at episode 71000\r\n",
      "💾 Model saved at episode 71500\r\n",
      "💾 Model saved at episode 72000\r\n",
      "💾 Model saved at episode 72500\r\n",
      "💾 Model saved at episode 73000\r\n",
      "💾 Model saved at episode 73500\r\n",
      "💾 Model saved at episode 74000\r\n",
      "💾 Model saved at episode 74500\r\n",
      "Episode 75000 | Reward:   3.83 | Avg:   1.02 📈 | Success: 67.00% | Budget Exceeded: 33.00% 📉 | Entropy: 0.373 📉 | LR: 1.55e-06\r\n",
      "💾 Model saved at episode 75000\r\n",
      "💾 Model saved at episode 75500\r\n",
      "💾 Model saved at episode 76000\r\n",
      "💾 Model saved at episode 76500\r\n",
      "💾 Model saved at episode 77000\r\n",
      "💾 Model saved at episode 77500\r\n",
      "💾 Model saved at episode 78000\r\n",
      "💾 Model saved at episode 78500\r\n",
      "💾 Model saved at episode 79000\r\n",
      "💾 Model saved at episode 79500\r\n",
      "Episode 80000 | Reward:   2.97 | Avg:   0.99 📈 | Success: 67.00% | Budget Exceeded: 33.00% 📉 | Entropy: 0.366 📉 | LR: 1.13e-06\r\n",
      "💾 Model saved at episode 80000\r\n",
      "💾 Model saved at episode 80500\r\n",
      "💾 Model saved at episode 81000\r\n",
      "💾 Model saved at episode 81500\r\n",
      "💾 Model saved at episode 82000\r\n",
      "💾 Model saved at episode 82500\r\n",
      "💾 Model saved at episode 83000\r\n",
      "💾 Model saved at episode 83500\r\n",
      "💾 Model saved at episode 84000\r\n",
      "💾 Model saved at episode 84500\r\n",
      "Episode 85000 | Reward:  -3.05 | Avg:   1.06 📉 | Success: 69.00% | Budget Exceeded: 31.00% 📈 | Entropy: 0.365 📈 | LR: 8.22e-07\r\n",
      "💾 Model saved at episode 85000\r\n",
      "💾 Model saved at episode 85500\r\n",
      "💾 Model saved at episode 86000\r\n",
      "💾 Model saved at episode 86500\r\n",
      "💾 Model saved at episode 87000\r\n",
      "💾 Model saved at episode 87500\r\n",
      "💾 Model saved at episode 88000\r\n",
      "💾 Model saved at episode 88500\r\n",
      "💾 Model saved at episode 89000\r\n",
      "💾 Model saved at episode 89500\r\n",
      "Episode 90000 | Reward:  -2.91 | Avg:   0.73 📈 | Success: 63.00% | Budget Exceeded: 37.00% 📉 | Entropy: 0.377 📈 | LR: 5.39e-07\r\n",
      "💾 Model saved at episode 90000\r\n",
      "💾 Model saved at episode 90500\r\n",
      "💾 Model saved at episode 91000\r\n",
      "💾 Model saved at episode 91500\r\n",
      "💾 Model saved at episode 92000\r\n",
      "💾 Model saved at episode 92500\r\n",
      "💾 Model saved at episode 93000\r\n",
      "💾 Model saved at episode 93500\r\n",
      "💾 Model saved at episode 94000\r\n",
      "💾 Model saved at episode 94500\r\n",
      "Episode 95000 | Reward:   2.77 | Avg:   0.55 📉 | Success: 60.00% | Budget Exceeded: 40.00% 📈 | Entropy: 0.368 📈 | LR: 3.93e-07\r\n",
      "💾 Model saved at episode 95000\r\n",
      "💾 Model saved at episode 95500\r\n",
      "💾 Model saved at episode 96000\r\n",
      "💾 Model saved at episode 96500\r\n",
      "💾 Model saved at episode 97000\r\n",
      "💾 Model saved at episode 97500\r\n",
      "💾 Model saved at episode 98000\r\n",
      "💾 Model saved at episode 98500\r\n",
      "💾 Model saved at episode 99000\r\n",
      "💾 Model saved at episode 99500\r\n",
      "Episode 100000 | Reward:   2.55 | Avg:   0.68 📉 | Success: 63.00% | Budget Exceeded: 37.00% 📈 | Entropy: 0.375 📉 | LR: 2.87e-07\r\n",
      "💾 Model saved at episode 100000\r\n",
      "\r\n",
      "==========================================================================================\r\n",
      "BUDGET-AWARE TRAINING COMPLETED\r\n",
      "==========================================================================================\r\n",
      "Training time: 2219.27 seconds\r\n",
      "Best average reward (last 100): 1.865\r\n",
      "Final average reward (last 100): 0.683\r\n",
      "Final success rate (last 100): 63.00%\r\n",
      "Final budget exceeded rate (last 100): 37.00% 🎯\r\n",
      "Final entropy: 0.375 (max possible: 1.386)\r\n",
      "Final learning rate: 2.87e-07\r\n",
      "First positive reward: Episode 9\r\n",
      "80% success milestone: Episode 44616\r\n",
      "Budget discipline milestone: Episode 44616\r\n",
      "Learning rate decayed 66 times during training\r\n",
      "\r\n",
      "Evaluating budget-aware policy over 100 episodes...\r\n",
      "Budget-Aware Evaluation Results:\r\n",
      "  Mean Reward: 0.724 ± 2.819\r\n",
      "  Mean Length: 11.27\r\n",
      "  Mean Cost: 122.84\r\n",
      "  Mean Skill Improvement: 2.673\r\n",
      "  Mean Budget Utilization: 102.37%\r\n",
      "  Success Rate: 62.00% 🎯\r\n",
      "  Budget Exceeded Rate: 38.00% 💰\r\n",
      "  Reward Improvement from Start: +3.420\r\n",
      "  Training Time: 2219.27s\r\n",
      "  Final Success Rate: 62.00%\r\n",
      "  Final Budget Exceeded Rate: 38.00%\r\n",
      "  Final Mean Reward: 0.724\r\n",
      "\r\n",
      "Testing Configuration: strong_incentive\r\n",
      "  Reward Strategy: hybrid\r\n",
      "  Cost Penalty: 0.015\r\n",
      "  Budget Penalties: base=5.0, max=10.0\r\n",
      "  Entropy Coefficient: 0.001\r\n",
      "--------------------------------------------------\r\n",
      "Initialized budget-aware agent with LR=0.0003, cost_penalty=0.015, step_size=1500\r\n",
      "Starting Budget-Aware Training with Enhanced Cost Penalties...\r\n",
      "Episodes: 100000\r\n",
      "Environment: 8 skills, 4 training modules\r\n",
      "Agent: Actor-Critic with entropy regularization\r\n",
      "Reward Strategy: hybrid\r\n",
      "Learning Rate: 0.0003 (StepLR: step=1500, gamma=0.9)\r\n",
      "Cost Penalty: 0.015 (5x increased for budget awareness)\r\n",
      "Budget Penalties: base=5.0, max=10.0\r\n",
      "------------------------------------------------------------------------------------------\r\n",
      "🎉 First positive reward achieved at episode 71!\r\n",
      "💾 Model saved at episode 500\r\n",
      "💾 Model saved at episode 1000\r\n",
      "💾 Model saved at episode 1500\r\n",
      "💾 Model saved at episode 2000\r\n",
      "💾 Model saved at episode 2500\r\n",
      "💾 Model saved at episode 3000\r\n",
      "💾 Model saved at episode 3500\r\n",
      "💾 Model saved at episode 4000\r\n",
      "💾 Model saved at episode 4500\r\n",
      "Episode 5000 | Reward:  -3.96 | Avg:  -1.33 📉 | Success: 45.00% | Budget Exceeded: 55.00% 📈 | Entropy: 0.533 📈 | LR: 2.19e-04\r\n",
      "💾 Model saved at episode 5000\r\n",
      "💾 Model saved at episode 5500\r\n",
      "💾 Model saved at episode 6000\r\n",
      "💾 Model saved at episode 6500\r\n",
      "💾 Model saved at episode 7000\r\n",
      "💾 Model saved at episode 7500\r\n",
      "💾 Model saved at episode 8000\r\n",
      "💾 Model saved at episode 8500\r\n",
      "💾 Model saved at episode 9000\r\n",
      "💾 Model saved at episode 9500\r\n",
      "Episode 10000 | Reward:   2.62 | Avg:  -0.48 📈 | Success: 60.00% | Budget Exceeded: 40.00% 📉 | Entropy: 0.439 📈 | LR: 1.59e-04\r\n",
      "💾 Model saved at episode 10000\r\n",
      "💾 Model saved at episode 10500\r\n",
      "💾 Model saved at episode 11000\r\n",
      "💾 Model saved at episode 11500\r\n",
      "💾 Model saved at episode 12000\r\n",
      "💾 Model saved at episode 12500\r\n",
      "💾 Model saved at episode 13000\r\n",
      "💾 Model saved at episode 13500\r\n",
      "💾 Model saved at episode 14000\r\n",
      "💾 Model saved at episode 14500\r\n",
      "Episode 15000 | Reward:  -5.02 | Avg:  -0.60 📉 | Success: 58.00% | Budget Exceeded: 42.00% 📈 | Entropy: 0.378 📉 | LR: 1.05e-04\r\n",
      "💾 Model saved at episode 15000\r\n",
      "💾 Model saved at episode 15500\r\n",
      "💾 Model saved at episode 16000\r\n",
      "💾 Model saved at episode 16500\r\n",
      "💾 Model saved at episode 17000\r\n",
      "💾 Model saved at episode 17500\r\n",
      "💾 Model saved at episode 18000\r\n",
      "💾 Model saved at episode 18500\r\n",
      "💾 Model saved at episode 19000\r\n",
      "💾 Model saved at episode 19500\r\n",
      "Episode 20000 | Reward:  -4.12 | Avg:  -0.14 📉 | Success: 64.00% | Budget Exceeded: 36.00% 📈 | Entropy: 0.336 📈 | LR: 7.63e-05\r\n",
      "💾 Model saved at episode 20000\r\n",
      "💾 Model saved at episode 20500\r\n",
      "💾 Model saved at episode 21000\r\n",
      "💾 Model saved at episode 21500\r\n",
      "💾 Model saved at episode 22000\r\n",
      "💾 Model saved at episode 22500\r\n",
      "🎯 80% success rate achieved at episode 22983!\r\n",
      "💰 Budget discipline achieved (≤20% exceeded) at episode 22983!\r\n",
      "💾 Model saved at episode 23000\r\n",
      "💾 Model saved at episode 23500\r\n",
      "💾 Model saved at episode 24000\r\n",
      "💾 Model saved at episode 24500\r\n",
      "Episode 25000 | Reward:   2.56 | Avg:   0.45 📉 | Success: 72.00% | Budget Exceeded: 28.00% 📈 | Entropy: 0.315 📈 | LR: 5.56e-05\r\n",
      "💾 Model saved at episode 25000\r\n",
      "💾 Model saved at episode 25500\r\n",
      "💾 Model saved at episode 26000\r\n",
      "💾 Model saved at episode 26500\r\n",
      "💾 Model saved at episode 27000\r\n",
      "💾 Model saved at episode 27500\r\n",
      "💾 Model saved at episode 28000\r\n",
      "💾 Model saved at episode 28500\r\n",
      "💾 Model saved at episode 29000\r\n",
      "💾 Model saved at episode 29500\r\n",
      "Episode 30000 | Reward:   2.10 | Avg:   0.37 📉 | Success: 72.00% | Budget Exceeded: 28.00% 📈 | Entropy: 0.290 📉 | LR: 3.65e-05\r\n",
      "💾 Model saved at episode 30000\r\n",
      "💾 Model saved at episode 30500\r\n",
      "💾 Model saved at episode 31000\r\n",
      "💾 Model saved at episode 31500\r\n",
      "💾 Model saved at episode 32000\r\n",
      "💾 Model saved at episode 32500\r\n",
      "💾 Model saved at episode 33000\r\n",
      "💾 Model saved at episode 33500\r\n",
      "💾 Model saved at episode 34000\r\n",
      "💾 Model saved at episode 34500\r\n",
      "Episode 35000 | Reward:   2.58 | Avg:  -0.30 📈 | Success: 62.00% | Budget Exceeded: 38.00% 📉 | Entropy: 0.292 📉 | LR: 2.66e-05\r\n",
      "💾 Model saved at episode 35000\r\n",
      "💾 Model saved at episode 35500\r\n",
      "💾 Model saved at episode 36000\r\n",
      "💾 Model saved at episode 36500\r\n",
      "💾 Model saved at episode 37000\r\n",
      "💾 Model saved at episode 37500\r\n",
      "💾 Model saved at episode 38000\r\n",
      "💾 Model saved at episode 38500\r\n",
      "💾 Model saved at episode 39000\r\n",
      "💾 Model saved at episode 39500\r\n",
      "Episode 40000 | Reward:   2.82 | Avg:  -0.34 📈 | Success: 61.00% | Budget Exceeded: 39.00% 📉 | Entropy: 0.283 📈 | LR: 1.94e-05\r\n",
      "💾 Model saved at episode 40000\r\n",
      "💾 Model saved at episode 40500\r\n",
      "💾 Model saved at episode 41000\r\n",
      "💾 Model saved at episode 41500\r\n",
      "💾 Model saved at episode 42000\r\n",
      "💾 Model saved at episode 42500\r\n",
      "💾 Model saved at episode 43000\r\n",
      "💾 Model saved at episode 43500\r\n",
      "💾 Model saved at episode 44000\r\n",
      "💾 Model saved at episode 44500\r\n",
      "Episode 45000 | Reward:   2.52 | Avg:   0.44 📈 | Success: 73.00% | Budget Exceeded: 27.00% 📉 | Entropy: 0.277 📈 | LR: 1.27e-05\r\n",
      "💾 Model saved at episode 45000\r\n",
      "💾 Model saved at episode 45500\r\n",
      "💾 Model saved at episode 46000\r\n",
      "💾 Model saved at episode 46500\r\n",
      "💾 Model saved at episode 47000\r\n",
      "💾 Model saved at episode 47500\r\n",
      "💾 Model saved at episode 48000\r\n",
      "💾 Model saved at episode 48500\r\n",
      "💾 Model saved at episode 49000\r\n",
      "💾 Model saved at episode 49500\r\n",
      "Episode 50000 | Reward:   1.57 | Avg:   0.05 📈 | Success: 67.00% | Budget Exceeded: 33.00% 📉 | Entropy: 0.270 📈 | LR: 9.27e-06\r\n",
      "💾 Model saved at episode 50000\r\n",
      "💾 Model saved at episode 50500\r\n",
      "💾 Model saved at episode 51000\r\n",
      "💾 Model saved at episode 51500\r\n",
      "💾 Model saved at episode 52000\r\n",
      "💾 Model saved at episode 52500\r\n",
      "💾 Model saved at episode 53000\r\n",
      "💾 Model saved at episode 53500\r\n",
      "💾 Model saved at episode 54000\r\n",
      "💾 Model saved at episode 54500\r\n",
      "Episode 55000 | Reward:  -4.00 | Avg:   0.23 📈 | Success: 70.00% | Budget Exceeded: 30.00% 📉 | Entropy: 0.275 📉 | LR: 6.76e-06\r\n",
      "💾 Model saved at episode 55000\r\n",
      "💾 Model saved at episode 55500\r\n",
      "💾 Model saved at episode 56000\r\n",
      "💾 Model saved at episode 56500\r\n",
      "💾 Model saved at episode 57000\r\n",
      "💾 Model saved at episode 57500\r\n",
      "💾 Model saved at episode 58000\r\n",
      "💾 Model saved at episode 58500\r\n",
      "💾 Model saved at episode 59000\r\n",
      "💾 Model saved at episode 59500\r\n",
      "Episode 60000 | Reward:   2.42 | Avg:   0.31 📈 | Success: 71.00% | Budget Exceeded: 29.00% 📉 | Entropy: 0.279 📉 | LR: 4.43e-06\r\n",
      "💾 Model saved at episode 60000\r\n",
      "💾 Model saved at episode 60500\r\n",
      "💾 Model saved at episode 61000\r\n",
      "💾 Model saved at episode 61500\r\n",
      "💾 Model saved at episode 62000\r\n",
      "💾 Model saved at episode 62500\r\n",
      "💾 Model saved at episode 63000\r\n",
      "💾 Model saved at episode 63500\r\n",
      "💾 Model saved at episode 64000\r\n",
      "💾 Model saved at episode 64500\r\n",
      "Episode 65000 | Reward:   2.58 | Avg:   0.19 📈 | Success: 70.00% | Budget Exceeded: 30.00% 📉 | Entropy: 0.274 📉 | LR: 3.23e-06\r\n",
      "💾 Model saved at episode 65000\r\n",
      "💾 Model saved at episode 65500\r\n",
      "💾 Model saved at episode 66000\r\n",
      "💾 Model saved at episode 66500\r\n",
      "💾 Model saved at episode 67000\r\n",
      "💾 Model saved at episode 67500\r\n",
      "💾 Model saved at episode 68000\r\n",
      "💾 Model saved at episode 68500\r\n",
      "💾 Model saved at episode 69000\r\n",
      "💾 Model saved at episode 69500\r\n",
      "Episode 70000 | Reward:   2.96 | Avg:   0.59 📈 | Success: 75.00% | Budget Exceeded: 25.00% 📉 | Entropy: 0.274 📈 | LR: 2.36e-06\r\n",
      "💾 Model saved at episode 70000\r\n",
      "💾 Model saved at episode 70500\r\n",
      "💾 Model saved at episode 71000\r\n",
      "💾 Model saved at episode 71500\r\n",
      "💾 Model saved at episode 72000\r\n",
      "💾 Model saved at episode 72500\r\n",
      "💾 Model saved at episode 73000\r\n",
      "💾 Model saved at episode 73500\r\n",
      "💾 Model saved at episode 74000\r\n",
      "💾 Model saved at episode 74500\r\n",
      "Episode 75000 | Reward:   2.58 | Avg:   0.61 📉 | Success: 76.00% | Budget Exceeded: 24.00% 📈 | Entropy: 0.267 📈 | LR: 1.55e-06\r\n",
      "💾 Model saved at episode 75000\r\n",
      "💾 Model saved at episode 75500\r\n",
      "💾 Model saved at episode 76000\r\n",
      "💾 Model saved at episode 76500\r\n",
      "💾 Model saved at episode 77000\r\n",
      "💾 Model saved at episode 77500\r\n",
      "💾 Model saved at episode 78000\r\n",
      "💾 Model saved at episode 78500\r\n",
      "💾 Model saved at episode 79000\r\n",
      "💾 Model saved at episode 79500\r\n",
      "Episode 80000 | Reward:   2.89 | Avg:   0.36 📈 | Success: 72.00% | Budget Exceeded: 28.00% 📉 | Entropy: 0.267 📉 | LR: 1.13e-06\r\n",
      "💾 Model saved at episode 80000\r\n",
      "💾 Model saved at episode 80500\r\n",
      "💾 Model saved at episode 81000\r\n",
      "💾 Model saved at episode 81500\r\n",
      "💾 Model saved at episode 82000\r\n",
      "💾 Model saved at episode 82500\r\n",
      "💾 Model saved at episode 83000\r\n",
      "💾 Model saved at episode 83500\r\n",
      "💾 Model saved at episode 84000\r\n",
      "💾 Model saved at episode 84500\r\n",
      "Episode 85000 | Reward:   2.30 | Avg:   0.59 📉 | Success: 75.00% | Budget Exceeded: 25.00% 📈 | Entropy: 0.269 📉 | LR: 8.22e-07\r\n",
      "💾 Model saved at episode 85000\r\n",
      "💾 Model saved at episode 85500\r\n",
      "💾 Model saved at episode 86000\r\n",
      "💾 Model saved at episode 86500\r\n",
      "💾 Model saved at episode 87000\r\n",
      "💾 Model saved at episode 87500\r\n",
      "💾 Model saved at episode 88000\r\n",
      "💾 Model saved at episode 88500\r\n",
      "💾 Model saved at episode 89000\r\n",
      "💾 Model saved at episode 89500\r\n",
      "Episode 90000 | Reward:   2.35 | Avg:   0.32 📈 | Success: 70.00% | Budget Exceeded: 30.00% 📉 | Entropy: 0.268 📈 | LR: 5.39e-07\r\n",
      "💾 Model saved at episode 90000\r\n",
      "💾 Model saved at episode 90500\r\n",
      "💾 Model saved at episode 91000\r\n",
      "💾 Model saved at episode 91500\r\n",
      "💾 Model saved at episode 92000\r\n",
      "💾 Model saved at episode 92500\r\n",
      "💾 Model saved at episode 93000\r\n",
      "💾 Model saved at episode 93500\r\n",
      "💾 Model saved at episode 94000\r\n",
      "💾 Model saved at episode 94500\r\n",
      "Episode 95000 | Reward:   1.88 | Avg:  -0.17 📈 | Success: 64.00% | Budget Exceeded: 36.00% 📉 | Entropy: 0.272 📈 | LR: 3.93e-07\r\n",
      "💾 Model saved at episode 95000\r\n",
      "💾 Model saved at episode 95500\r\n",
      "💾 Model saved at episode 96000\r\n",
      "💾 Model saved at episode 96500\r\n",
      "💾 Model saved at episode 97000\r\n",
      "💾 Model saved at episode 97500\r\n",
      "💾 Model saved at episode 98000\r\n",
      "💾 Model saved at episode 98500\r\n",
      "💾 Model saved at episode 99000\r\n",
      "💾 Model saved at episode 99500\r\n",
      "Episode 100000 | Reward:   1.72 | Avg:  -0.24 📈 | Success: 63.00% | Budget Exceeded: 37.00% 📈 | Entropy: 0.269 📈 | LR: 2.87e-07\r\n",
      "💾 Model saved at episode 100000\r\n",
      "\r\n",
      "==========================================================================================\r\n",
      "BUDGET-AWARE TRAINING COMPLETED\r\n",
      "==========================================================================================\r\n",
      "Training time: 2168.36 seconds\r\n",
      "Best average reward (last 100): 1.452\r\n",
      "Final average reward (last 100): -0.241\r\n",
      "Final success rate (last 100): 63.00%\r\n",
      "Final budget exceeded rate (last 100): 37.00% 🎯\r\n",
      "Final entropy: 0.269 (max possible: 1.386)\r\n",
      "Final learning rate: 2.87e-07\r\n",
      "First positive reward: Episode 71\r\n",
      "80% success milestone: Episode 22983\r\n",
      "Budget discipline milestone: Episode 22983\r\n",
      "Learning rate decayed 66 times during training\r\n",
      "\r\n",
      "Evaluating budget-aware policy over 100 episodes...\r\n",
      "Budget-Aware Evaluation Results:\r\n",
      "  Mean Reward: 0.403 ± 2.991\r\n",
      "  Mean Length: 11.14\r\n",
      "  Mean Cost: 121.64\r\n",
      "  Mean Skill Improvement: 2.624\r\n",
      "  Mean Budget Utilization: 101.37%\r\n",
      "  Success Rate: 73.00% 🎯\r\n",
      "  Budget Exceeded Rate: 27.00% 💰\r\n",
      "  Reward Improvement from Start: +4.758\r\n",
      "  Training Time: 2168.36s\r\n",
      "  Final Success Rate: 73.00%\r\n",
      "  Final Budget Exceeded Rate: 27.00%\r\n",
      "  Final Mean Reward: 0.403\r\n",
      "\r\n",
      "Testing Configuration: ultra_conservative\r\n",
      "  Reward Strategy: hybrid\r\n",
      "  Cost Penalty: 0.025\r\n",
      "  Budget Penalties: base=6.0, max=12.0\r\n",
      "  Entropy Coefficient: 0.001\r\n",
      "--------------------------------------------------\r\n",
      "Initialized budget-aware agent with LR=0.0003, cost_penalty=0.025, step_size=1500\r\n",
      "Starting Budget-Aware Training with Enhanced Cost Penalties...\r\n",
      "Episodes: 100000\r\n",
      "Environment: 8 skills, 4 training modules\r\n",
      "Agent: Actor-Critic with entropy regularization\r\n",
      "Reward Strategy: hybrid\r\n",
      "Learning Rate: 0.0003 (StepLR: step=1500, gamma=0.9)\r\n",
      "Cost Penalty: 0.025 (5x increased for budget awareness)\r\n",
      "Budget Penalties: base=6.0, max=12.0\r\n",
      "------------------------------------------------------------------------------------------\r\n",
      "🎉 First positive reward achieved at episode 10!\r\n",
      "💾 Model saved at episode 500\r\n",
      "💾 Model saved at episode 1000\r\n",
      "💾 Model saved at episode 1500\r\n",
      "💾 Model saved at episode 2000\r\n",
      "💾 Model saved at episode 2500\r\n",
      "💾 Model saved at episode 3000\r\n",
      "💾 Model saved at episode 3500\r\n",
      "💾 Model saved at episode 4000\r\n",
      "💾 Model saved at episode 4500\r\n",
      "Episode 5000 | Reward:  -6.97 | Avg:  -2.67 📈 | Success: 51.00% | Budget Exceeded: 49.00% 📉 | Entropy: 0.596 📈 | LR: 2.19e-04\r\n",
      "💾 Model saved at episode 5000\r\n",
      "💾 Model saved at episode 5500\r\n",
      "💾 Model saved at episode 6000\r\n",
      "💾 Model saved at episode 6500\r\n",
      "💾 Model saved at episode 7000\r\n",
      "💾 Model saved at episode 7500\r\n",
      "💾 Model saved at episode 8000\r\n",
      "💾 Model saved at episode 8500\r\n",
      "💾 Model saved at episode 9000\r\n",
      "💾 Model saved at episode 9500\r\n",
      "Episode 10000 | Reward:  -6.99 | Avg:  -2.30 📈 | Success: 55.00% | Budget Exceeded: 44.00% 📉 | Entropy: 0.515 📉 | LR: 1.59e-04\r\n",
      "💾 Model saved at episode 10000\r\n",
      "💾 Model saved at episode 10500\r\n",
      "💾 Model saved at episode 11000\r\n",
      "💾 Model saved at episode 11500\r\n",
      "💾 Model saved at episode 12000\r\n",
      "💾 Model saved at episode 12500\r\n",
      "💾 Model saved at episode 13000\r\n",
      "💾 Model saved at episode 13500\r\n",
      "💾 Model saved at episode 14000\r\n",
      "💾 Model saved at episode 14500\r\n",
      "Episode 15000 | Reward:  -6.60 | Avg:  -1.87 📈 | Success: 60.00% | Budget Exceeded: 38.00% 📉 | Entropy: 0.441 📈 | LR: 1.05e-04\r\n",
      "💾 Model saved at episode 15000\r\n",
      "💾 Model saved at episode 15500\r\n",
      "💾 Model saved at episode 16000\r\n",
      "💾 Model saved at episode 16500\r\n",
      "💾 Model saved at episode 17000\r\n",
      "💾 Model saved at episode 17500\r\n",
      "💾 Model saved at episode 18000\r\n",
      "💾 Model saved at episode 18500\r\n",
      "💾 Model saved at episode 19000\r\n",
      "💾 Model saved at episode 19500\r\n",
      "Episode 20000 | Reward:  -6.80 | Avg:  -2.51 📉 | Success: 52.00% | Budget Exceeded: 47.00% 📈 | Entropy: 0.435 📉 | LR: 7.63e-05\r\n",
      "💾 Model saved at episode 20000\r\n",
      "💾 Model saved at episode 20500\r\n",
      "💾 Model saved at episode 21000\r\n",
      "💾 Model saved at episode 21500\r\n",
      "💾 Model saved at episode 22000\r\n",
      "💾 Model saved at episode 22500\r\n",
      "💾 Model saved at episode 23000\r\n",
      "💾 Model saved at episode 23500\r\n",
      "💾 Model saved at episode 24000\r\n",
      "💾 Model saved at episode 24500\r\n",
      "Episode 25000 | Reward:   0.89 | Avg:  -2.02 📉 | Success: 59.00% | Budget Exceeded: 40.00% 📈 | Entropy: 0.424 📈 | LR: 5.56e-05\r\n",
      "💾 Model saved at episode 25000\r\n",
      "💾 Model saved at episode 25500\r\n",
      "💾 Model saved at episode 26000\r\n",
      "💾 Model saved at episode 26500\r\n",
      "💾 Model saved at episode 27000\r\n",
      "💾 Model saved at episode 27500\r\n",
      "💾 Model saved at episode 28000\r\n",
      "💾 Model saved at episode 28500\r\n",
      "💾 Model saved at episode 29000\r\n",
      "💾 Model saved at episode 29500\r\n",
      "Episode 30000 | Reward:  -6.07 | Avg:  -2.78 📈 | Success: 50.00% | Budget Exceeded: 50.00% 📉 | Entropy: 0.443 📉 | LR: 3.65e-05\r\n",
      "💾 Model saved at episode 30000\r\n",
      "💾 Model saved at episode 30500\r\n",
      "💾 Model saved at episode 31000\r\n",
      "💾 Model saved at episode 31500\r\n",
      "💾 Model saved at episode 32000\r\n",
      "💾 Model saved at episode 32500\r\n",
      "💾 Model saved at episode 33000\r\n",
      "💾 Model saved at episode 33500\r\n",
      "💾 Model saved at episode 34000\r\n",
      "💾 Model saved at episode 34500\r\n",
      "Episode 35000 | Reward:  -5.91 | Avg:  -3.10 📉 | Success: 45.00% | Budget Exceeded: 55.00% 📈 | Entropy: 0.449 📉 | LR: 2.66e-05\r\n",
      "💾 Model saved at episode 35000\r\n",
      "💾 Model saved at episode 35500\r\n",
      "💾 Model saved at episode 36000\r\n",
      "💾 Model saved at episode 36500\r\n",
      "💾 Model saved at episode 37000\r\n",
      "💾 Model saved at episode 37500\r\n",
      "💾 Model saved at episode 38000\r\n",
      "💾 Model saved at episode 38500\r\n",
      "💾 Model saved at episode 39000\r\n",
      "💾 Model saved at episode 39500\r\n",
      "Episode 40000 | Reward:   0.73 | Avg:  -2.20 📈 | Success: 54.00% | Budget Exceeded: 43.00% 📉 | Entropy: 0.435 📈 | LR: 1.94e-05\r\n",
      "💾 Model saved at episode 40000\r\n",
      "💾 Model saved at episode 40500\r\n",
      "💾 Model saved at episode 41000\r\n",
      "💾 Model saved at episode 41500\r\n",
      "💾 Model saved at episode 42000\r\n",
      "💾 Model saved at episode 42500\r\n",
      "💾 Model saved at episode 43000\r\n",
      "💾 Model saved at episode 43500\r\n",
      "💾 Model saved at episode 44000\r\n",
      "💾 Model saved at episode 44500\r\n",
      "Episode 45000 | Reward:  -7.19 | Avg:  -2.71 📉 | Success: 50.00% | Budget Exceeded: 49.00% 📈 | Entropy: 0.448 📈 | LR: 1.27e-05\r\n",
      "💾 Model saved at episode 45000\r\n",
      "💾 Model saved at episode 45500\r\n",
      "💾 Model saved at episode 46000\r\n",
      "💾 Model saved at episode 46500\r\n",
      "💾 Model saved at episode 47000\r\n",
      "💾 Model saved at episode 47500\r\n",
      "💾 Model saved at episode 48000\r\n",
      "💾 Model saved at episode 48500\r\n",
      "💾 Model saved at episode 49000\r\n",
      "💾 Model saved at episode 49500\r\n",
      "Episode 50000 | Reward:   0.26 | Avg:  -2.94 📉 | Success: 47.00% | Budget Exceeded: 53.00% 📈 | Entropy: 0.440 📈 | LR: 9.27e-06\r\n",
      "💾 Model saved at episode 50000\r\n",
      "💾 Model saved at episode 50500\r\n",
      "💾 Model saved at episode 51000\r\n",
      "💾 Model saved at episode 51500\r\n",
      "💾 Model saved at episode 52000\r\n",
      "💾 Model saved at episode 52500\r\n",
      "💾 Model saved at episode 53000\r\n",
      "💾 Model saved at episode 53500\r\n",
      "💾 Model saved at episode 54000\r\n",
      "💾 Model saved at episode 54500\r\n",
      "Episode 55000 | Reward:   1.00 | Avg:  -3.05 📉 | Success: 46.00% | Budget Exceeded: 54.00% 📈 | Entropy: 0.449 📉 | LR: 6.76e-06\r\n",
      "💾 Model saved at episode 55000\r\n",
      "💾 Model saved at episode 55500\r\n",
      "💾 Model saved at episode 56000\r\n",
      "💾 Model saved at episode 56500\r\n",
      "💾 Model saved at episode 57000\r\n",
      "💾 Model saved at episode 57500\r\n",
      "💾 Model saved at episode 58000\r\n",
      "💾 Model saved at episode 58500\r\n",
      "💾 Model saved at episode 59000\r\n",
      "💾 Model saved at episode 59500\r\n",
      "Episode 60000 | Reward:   1.82 | Avg:  -2.87 📉 | Success: 48.00% | Budget Exceeded: 52.00% 📈 | Entropy: 0.447 📉 | LR: 4.43e-06\r\n",
      "💾 Model saved at episode 60000\r\n",
      "💾 Model saved at episode 60500\r\n",
      "💾 Model saved at episode 61000\r\n",
      "💾 Model saved at episode 61500\r\n",
      "💾 Model saved at episode 62000\r\n",
      "💾 Model saved at episode 62500\r\n",
      "💾 Model saved at episode 63000\r\n",
      "💾 Model saved at episode 63500\r\n",
      "💾 Model saved at episode 64000\r\n",
      "💾 Model saved at episode 64500\r\n",
      "Episode 65000 | Reward:  -6.51 | Avg:  -3.99 📉 | Success: 32.00% | Budget Exceeded: 67.00% 📈 | Entropy: 0.453 📈 | LR: 3.23e-06\r\n",
      "💾 Model saved at episode 65000\r\n",
      "💾 Model saved at episode 65500\r\n",
      "💾 Model saved at episode 66000\r\n",
      "💾 Model saved at episode 66500\r\n",
      "💾 Model saved at episode 67000\r\n",
      "💾 Model saved at episode 67500\r\n",
      "💾 Model saved at episode 68000\r\n",
      "💾 Model saved at episode 68500\r\n",
      "💾 Model saved at episode 69000\r\n",
      "💾 Model saved at episode 69500\r\n",
      "Episode 70000 | Reward:  -6.49 | Avg:  -3.52 📉 | Success: 39.00% | Budget Exceeded: 61.00% 📈 | Entropy: 0.443 📈 | LR: 2.36e-06\r\n",
      "💾 Model saved at episode 70000\r\n",
      "💾 Model saved at episode 70500\r\n",
      "💾 Model saved at episode 71000\r\n",
      "💾 Model saved at episode 71500\r\n",
      "💾 Model saved at episode 72000\r\n",
      "💾 Model saved at episode 72500\r\n",
      "💾 Model saved at episode 73000\r\n",
      "💾 Model saved at episode 73500\r\n",
      "💾 Model saved at episode 74000\r\n",
      "💾 Model saved at episode 74500\r\n",
      "Episode 75000 | Reward:   0.47 | Avg:  -2.53 📈 | Success: 53.00% | Budget Exceeded: 47.00% 📉 | Entropy: 0.442 📈 | LR: 1.55e-06\r\n",
      "💾 Model saved at episode 75000\r\n",
      "💾 Model saved at episode 75500\r\n",
      "💾 Model saved at episode 76000\r\n",
      "💾 Model saved at episode 76500\r\n",
      "💾 Model saved at episode 77000\r\n",
      "💾 Model saved at episode 77500\r\n",
      "💾 Model saved at episode 78000\r\n",
      "💾 Model saved at episode 78500\r\n",
      "💾 Model saved at episode 79000\r\n",
      "💾 Model saved at episode 79500\r\n",
      "Episode 80000 | Reward:   0.97 | Avg:  -2.67 📈 | Success: 49.00% | Budget Exceeded: 49.00% 📉 | Entropy: 0.452 📈 | LR: 1.13e-06\r\n",
      "💾 Model saved at episode 80000\r\n",
      "💾 Model saved at episode 80500\r\n",
      "💾 Model saved at episode 81000\r\n",
      "💾 Model saved at episode 81500\r\n",
      "💾 Model saved at episode 82000\r\n",
      "💾 Model saved at episode 82500\r\n",
      "💾 Model saved at episode 83000\r\n",
      "💾 Model saved at episode 83500\r\n",
      "💾 Model saved at episode 84000\r\n",
      "💾 Model saved at episode 84500\r\n",
      "Episode 85000 | Reward:   1.27 | Avg:  -2.46 📈 | Success: 52.00% | Budget Exceeded: 46.00% 📈 | Entropy: 0.453 📈 | LR: 8.22e-07\r\n",
      "💾 Model saved at episode 85000\r\n",
      "💾 Model saved at episode 85500\r\n",
      "💾 Model saved at episode 86000\r\n",
      "💾 Model saved at episode 86500\r\n",
      "💾 Model saved at episode 87000\r\n",
      "💾 Model saved at episode 87500\r\n",
      "💾 Model saved at episode 88000\r\n",
      "💾 Model saved at episode 88500\r\n",
      "💾 Model saved at episode 89000\r\n",
      "💾 Model saved at episode 89500\r\n",
      "Episode 90000 | Reward:   0.28 | Avg:  -3.10 📈 | Success: 45.00% | Budget Exceeded: 54.00% 📉 | Entropy: 0.460 📉 | LR: 5.39e-07\r\n",
      "💾 Model saved at episode 90000\r\n",
      "💾 Model saved at episode 90500\r\n",
      "💾 Model saved at episode 91000\r\n",
      "💾 Model saved at episode 91500\r\n",
      "💾 Model saved at episode 92000\r\n",
      "💾 Model saved at episode 92500\r\n",
      "💾 Model saved at episode 93000\r\n",
      "💾 Model saved at episode 93500\r\n",
      "💾 Model saved at episode 94000\r\n",
      "💾 Model saved at episode 94500\r\n",
      "Episode 95000 | Reward:   0.82 | Avg:  -3.02 📉 | Success: 45.00% | Budget Exceeded: 54.00% 📈 | Entropy: 0.458 📉 | LR: 3.93e-07\r\n",
      "💾 Model saved at episode 95000\r\n",
      "💾 Model saved at episode 95500\r\n",
      "💾 Model saved at episode 96000\r\n",
      "💾 Model saved at episode 96500\r\n",
      "💾 Model saved at episode 97000\r\n",
      "💾 Model saved at episode 97500\r\n",
      "💾 Model saved at episode 98000\r\n",
      "💾 Model saved at episode 98500\r\n",
      "💾 Model saved at episode 99000\r\n",
      "💾 Model saved at episode 99500\r\n",
      "Episode 100000 | Reward:   0.66 | Avg:  -2.62 📈 | Success: 49.00% | Budget Exceeded: 49.00% 📉 | Entropy: 0.447 📉 | LR: 2.87e-07\r\n",
      "💾 Model saved at episode 100000\r\n",
      "\r\n",
      "==========================================================================================\r\n",
      "BUDGET-AWARE TRAINING COMPLETED\r\n",
      "==========================================================================================\r\n",
      "Training time: 2183.10 seconds\r\n",
      "Best average reward (last 100): -0.939\r\n",
      "Final average reward (last 100): -2.618\r\n",
      "Final success rate (last 100): 49.00%\r\n",
      "Final budget exceeded rate (last 100): 49.00% 🎯\r\n",
      "Final entropy: 0.447 (max possible: 1.386)\r\n",
      "Final learning rate: 2.87e-07\r\n",
      "First positive reward: Episode 10\r\n",
      "Learning rate decayed 66 times during training\r\n",
      "\r\n",
      "Evaluating budget-aware policy over 100 episodes...\r\n",
      "Budget-Aware Evaluation Results:\r\n",
      "  Mean Reward: -0.611 ± 3.200\r\n",
      "  Mean Length: 11.11\r\n",
      "  Mean Cost: 121.28\r\n",
      "  Mean Skill Improvement: 2.683\r\n",
      "  Mean Budget Utilization: 101.07%\r\n",
      "  Success Rate: 78.00% 🎯\r\n",
      "  Budget Exceeded Rate: 22.00% 💰\r\n",
      "  Reward Improvement from Start: +5.603\r\n",
      "  Training Time: 2183.10s\r\n",
      "  Final Success Rate: 78.00%\r\n",
      "  Final Budget Exceeded Rate: 22.00%\r\n",
      "  Final Mean Reward: -0.611\r\n",
      "\r\n",
      "Testing Configuration: terminal_focus_strong\r\n",
      "  Reward Strategy: terminal\r\n",
      "  Cost Penalty: 0.008\r\n",
      "  Budget Penalties: base=5.0, max=10.0\r\n",
      "  Entropy Coefficient: 0.001\r\n",
      "--------------------------------------------------\r\n",
      "Initialized budget-aware agent with LR=0.0003, cost_penalty=0.008, step_size=750\r\n",
      "Starting Budget-Aware Training with Enhanced Cost Penalties...\r\n",
      "Episodes: 100000\r\n",
      "Environment: 8 skills, 4 training modules\r\n",
      "Agent: Actor-Critic with entropy regularization\r\n",
      "Reward Strategy: terminal\r\n",
      "Learning Rate: 0.0003 (StepLR: step=750, gamma=0.9)\r\n",
      "Cost Penalty: 0.008 (5x increased for budget awareness)\r\n",
      "Budget Penalties: base=5.0, max=10.0\r\n",
      "------------------------------------------------------------------------------------------\r\n",
      "🎉 First positive reward achieved at episode 27!\r\n",
      "💾 Model saved at episode 500\r\n",
      "💾 Model saved at episode 1000\r\n",
      "💾 Model saved at episode 1500\r\n",
      "💾 Model saved at episode 2000\r\n",
      "💾 Model saved at episode 2500\r\n",
      "💾 Model saved at episode 3000\r\n",
      "💾 Model saved at episode 3500\r\n",
      "💾 Model saved at episode 4000\r\n",
      "💾 Model saved at episode 4500\r\n",
      "Episode 5000 | Reward:  20.27 | Avg:   3.83 📈 | Success: 30.00% | Budget Exceeded: 70.00% 📉 | Entropy: 0.763 📈 | LR: 1.59e-04\r\n",
      "💾 Model saved at episode 5000\r\n",
      "💾 Model saved at episode 5500\r\n",
      "💾 Model saved at episode 6000\r\n",
      "💾 Model saved at episode 6500\r\n",
      "💾 Model saved at episode 7000\r\n",
      "💾 Model saved at episode 7500\r\n",
      "💾 Model saved at episode 8000\r\n",
      "💾 Model saved at episode 8500\r\n",
      "💾 Model saved at episode 9000\r\n",
      "💾 Model saved at episode 9500\r\n",
      "Episode 10000 | Reward:  -3.74 | Avg:   4.85 📉 | Success: 34.00% | Budget Exceeded: 66.00% 📈 | Entropy: 0.671 📈 | LR: 7.63e-05\r\n",
      "💾 Model saved at episode 10000\r\n",
      "💾 Model saved at episode 10500\r\n",
      "💾 Model saved at episode 11000\r\n",
      "💾 Model saved at episode 11500\r\n",
      "💾 Model saved at episode 12000\r\n",
      "💾 Model saved at episode 12500\r\n",
      "💾 Model saved at episode 13000\r\n",
      "💾 Model saved at episode 13500\r\n",
      "💾 Model saved at episode 14000\r\n",
      "💾 Model saved at episode 14500\r\n",
      "Episode 15000 | Reward:  20.68 | Avg:   3.52 📉 | Success: 29.00% | Budget Exceeded: 71.00% 📈 | Entropy: 0.658 📈 | LR: 3.65e-05\r\n",
      "💾 Model saved at episode 15000\r\n",
      "💾 Model saved at episode 15500\r\n",
      "💾 Model saved at episode 16000\r\n",
      "💾 Model saved at episode 16500\r\n",
      "💾 Model saved at episode 17000\r\n",
      "💾 Model saved at episode 17500\r\n",
      "💾 Model saved at episode 18000\r\n",
      "💾 Model saved at episode 18500\r\n",
      "💾 Model saved at episode 19000\r\n",
      "💾 Model saved at episode 19500\r\n",
      "Episode 20000 | Reward:  21.28 | Avg:   4.55 📈 | Success: 33.00% | Budget Exceeded: 67.00% 📉 | Entropy: 0.657 📈 | LR: 1.94e-05\r\n",
      "💾 Model saved at episode 20000\r\n",
      "💾 Model saved at episode 20500\r\n",
      "💾 Model saved at episode 21000\r\n",
      "💾 Model saved at episode 21500\r\n",
      "💾 Model saved at episode 22000\r\n",
      "💾 Model saved at episode 22500\r\n",
      "💾 Model saved at episode 23000\r\n",
      "💾 Model saved at episode 23500\r\n",
      "💾 Model saved at episode 24000\r\n",
      "💾 Model saved at episode 24500\r\n",
      "Episode 25000 | Reward:  20.48 | Avg:   5.99 📈 | Success: 39.00% | Budget Exceeded: 61.00% 📉 | Entropy: 0.643 📉 | LR: 9.27e-06\r\n",
      "💾 Model saved at episode 25000\r\n",
      "💾 Model saved at episode 25500\r\n",
      "💾 Model saved at episode 26000\r\n",
      "💾 Model saved at episode 26500\r\n",
      "💾 Model saved at episode 27000\r\n",
      "💾 Model saved at episode 27500\r\n",
      "💾 Model saved at episode 28000\r\n",
      "💾 Model saved at episode 28500\r\n",
      "💾 Model saved at episode 29000\r\n",
      "💾 Model saved at episode 29500\r\n",
      "Episode 30000 | Reward:  -3.37 | Avg:   1.82 📈 | Success: 22.00% | Budget Exceeded: 78.00% 📉 | Entropy: 0.641 📈 | LR: 4.43e-06\r\n",
      "💾 Model saved at episode 30000\r\n",
      "💾 Model saved at episode 30500\r\n",
      "💾 Model saved at episode 31000\r\n",
      "💾 Model saved at episode 31500\r\n",
      "💾 Model saved at episode 32000\r\n",
      "💾 Model saved at episode 32500\r\n",
      "💾 Model saved at episode 33000\r\n",
      "💾 Model saved at episode 33500\r\n",
      "💾 Model saved at episode 34000\r\n",
      "💾 Model saved at episode 34500\r\n",
      "Episode 35000 | Reward:  -3.56 | Avg:   5.99 📈 | Success: 39.00% | Budget Exceeded: 61.00% 📉 | Entropy: 0.628 📉 | LR: 2.36e-06\r\n",
      "💾 Model saved at episode 35000\r\n",
      "💾 Model saved at episode 35500\r\n",
      "💾 Model saved at episode 36000\r\n",
      "💾 Model saved at episode 36500\r\n",
      "💾 Model saved at episode 37000\r\n",
      "💾 Model saved at episode 37500\r\n",
      "💾 Model saved at episode 38000\r\n",
      "💾 Model saved at episode 38500\r\n",
      "💾 Model saved at episode 39000\r\n",
      "💾 Model saved at episode 39500\r\n",
      "Episode 40000 | Reward:  -3.03 | Avg:   4.32 📉 | Success: 32.00% | Budget Exceeded: 68.00% 📈 | Entropy: 0.639 📈 | LR: 1.13e-06\r\n",
      "💾 Model saved at episode 40000\r\n",
      "💾 Model saved at episode 40500\r\n",
      "💾 Model saved at episode 41000\r\n",
      "💾 Model saved at episode 41500\r\n",
      "💾 Model saved at episode 42000\r\n",
      "💾 Model saved at episode 42500\r\n",
      "💾 Model saved at episode 43000\r\n",
      "💾 Model saved at episode 43500\r\n",
      "💾 Model saved at episode 44000\r\n",
      "💾 Model saved at episode 44500\r\n",
      "Episode 45000 | Reward:  -3.49 | Avg:   4.31 📈 | Success: 32.00% | Budget Exceeded: 68.00% 📉 | Entropy: 0.637 📉 | LR: 5.39e-07\r\n",
      "💾 Model saved at episode 45000\r\n",
      "💾 Model saved at episode 45500\r\n",
      "💾 Model saved at episode 46000\r\n",
      "💾 Model saved at episode 46500\r\n",
      "💾 Model saved at episode 47000\r\n",
      "💾 Model saved at episode 47500\r\n",
      "💾 Model saved at episode 48000\r\n",
      "💾 Model saved at episode 48500\r\n",
      "💾 Model saved at episode 49000\r\n",
      "💾 Model saved at episode 49500\r\n",
      "Episode 50000 | Reward:  20.50 | Avg:   4.30 📈 | Success: 32.00% | Budget Exceeded: 68.00% 📉 | Entropy: 0.625 📉 | LR: 2.87e-07\r\n",
      "💾 Model saved at episode 50000\r\n",
      "💾 Model saved at episode 50500\r\n",
      "💾 Model saved at episode 51000\r\n",
      "💾 Model saved at episode 51500\r\n",
      "💾 Model saved at episode 52000\r\n",
      "💾 Model saved at episode 52500\r\n",
      "💾 Model saved at episode 53000\r\n",
      "💾 Model saved at episode 53500\r\n",
      "💾 Model saved at episode 54000\r\n",
      "💾 Model saved at episode 54500\r\n",
      "Episode 55000 | Reward:  -3.81 | Avg:   7.01 📉 | Success: 43.00% | Budget Exceeded: 57.00% 📈 | Entropy: 0.635 📉 | LR: 1.37e-07\r\n",
      "💾 Model saved at episode 55000\r\n",
      "💾 Model saved at episode 55500\r\n",
      "💾 Model saved at episode 56000\r\n",
      "💾 Model saved at episode 56500\r\n",
      "💾 Model saved at episode 57000\r\n",
      "💾 Model saved at episode 57500\r\n",
      "💾 Model saved at episode 58000\r\n",
      "💾 Model saved at episode 58500\r\n",
      "💾 Model saved at episode 59000\r\n",
      "💾 Model saved at episode 59500\r\n",
      "Episode 60000 | Reward:  -3.56 | Avg:   6.24 📉 | Success: 40.00% | Budget Exceeded: 60.00% 📈 | Entropy: 0.635 📉 | LR: 6.55e-08\r\n",
      "💾 Model saved at episode 60000\r\n",
      "💾 Model saved at episode 60500\r\n",
      "💾 Model saved at episode 61000\r\n",
      "💾 Model saved at episode 61500\r\n",
      "💾 Model saved at episode 62000\r\n",
      "💾 Model saved at episode 62500\r\n",
      "💾 Model saved at episode 63000\r\n",
      "💾 Model saved at episode 63500\r\n",
      "💾 Model saved at episode 64000\r\n",
      "💾 Model saved at episode 64500\r\n",
      "Episode 65000 | Reward:  -3.48 | Avg:   3.79 📈 | Success: 30.00% | Budget Exceeded: 70.00% 📉 | Entropy: 0.637 📈 | LR: 3.48e-08\r\n",
      "💾 Model saved at episode 65000\r\n",
      "💾 Model saved at episode 65500\r\n",
      "💾 Model saved at episode 66000\r\n",
      "💾 Model saved at episode 66500\r\n",
      "💾 Model saved at episode 67000\r\n",
      "💾 Model saved at episode 67500\r\n",
      "💾 Model saved at episode 68000\r\n",
      "💾 Model saved at episode 68500\r\n",
      "💾 Model saved at episode 69000\r\n",
      "💾 Model saved at episode 69500\r\n",
      "Episode 70000 | Reward:  20.32 | Avg:   5.96 📉 | Success: 39.00% | Budget Exceeded: 61.00% 📈 | Entropy: 0.639 📈 | LR: 1.67e-08\r\n",
      "💾 Model saved at episode 70000\r\n",
      "💾 Model saved at episode 70500\r\n",
      "💾 Model saved at episode 71000\r\n",
      "💾 Model saved at episode 71500\r\n",
      "💾 Model saved at episode 72000\r\n",
      "💾 Model saved at episode 72500\r\n",
      "💾 Model saved at episode 73000\r\n",
      "💾 Model saved at episode 73500\r\n",
      "💾 Model saved at episode 74000\r\n",
      "💾 Model saved at episode 74500\r\n",
      "Episode 75000 | Reward:  -3.18 | Avg:   3.35 📉 | Success: 28.00% | Budget Exceeded: 72.00% 📈 | Entropy: 0.636 📈 | LR: 7.97e-09\r\n",
      "💾 Model saved at episode 75000\r\n",
      "💾 Model saved at episode 75500\r\n",
      "💾 Model saved at episode 76000\r\n",
      "💾 Model saved at episode 76500\r\n",
      "💾 Model saved at episode 77000\r\n",
      "💾 Model saved at episode 77500\r\n",
      "💾 Model saved at episode 78000\r\n",
      "💾 Model saved at episode 78500\r\n",
      "💾 Model saved at episode 79000\r\n",
      "💾 Model saved at episode 79500\r\n",
      "Episode 80000 | Reward:  -4.17 | Avg:   5.34 📉 | Success: 36.00% | Budget Exceeded: 64.00% 📈 | Entropy: 0.639 📉 | LR: 4.23e-09\r\n",
      "💾 Model saved at episode 80000\r\n",
      "💾 Model saved at episode 80500\r\n",
      "💾 Model saved at episode 81000\r\n",
      "💾 Model saved at episode 81500\r\n",
      "💾 Model saved at episode 82000\r\n",
      "💾 Model saved at episode 82500\r\n",
      "💾 Model saved at episode 83000\r\n",
      "💾 Model saved at episode 83500\r\n",
      "💾 Model saved at episode 84000\r\n",
      "💾 Model saved at episode 84500\r\n",
      "Episode 85000 | Reward:  21.19 | Avg:   5.24 📈 | Success: 36.00% | Budget Exceeded: 64.00% 📈 | Entropy: 0.632 📈 | LR: 2.03e-09\r\n",
      "💾 Model saved at episode 85000\r\n",
      "💾 Model saved at episode 85500\r\n",
      "💾 Model saved at episode 86000\r\n",
      "💾 Model saved at episode 86500\r\n",
      "💾 Model saved at episode 87000\r\n",
      "💾 Model saved at episode 87500\r\n",
      "💾 Model saved at episode 88000\r\n",
      "💾 Model saved at episode 88500\r\n",
      "💾 Model saved at episode 89000\r\n",
      "💾 Model saved at episode 89500\r\n",
      "Episode 90000 | Reward:  21.43 | Avg:   3.35 📉 | Success: 28.00% | Budget Exceeded: 72.00% 📈 | Entropy: 0.639 📈 | LR: 9.69e-10\r\n",
      "💾 Model saved at episode 90000\r\n",
      "💾 Model saved at episode 90500\r\n",
      "💾 Model saved at episode 91000\r\n",
      "💾 Model saved at episode 91500\r\n",
      "💾 Model saved at episode 92000\r\n",
      "💾 Model saved at episode 92500\r\n",
      "💾 Model saved at episode 93000\r\n",
      "💾 Model saved at episode 93500\r\n",
      "💾 Model saved at episode 94000\r\n",
      "💾 Model saved at episode 94500\r\n",
      "Episode 95000 | Reward:  -3.34 | Avg:   6.29 📈 | Success: 40.00% | Budget Exceeded: 60.00% 📉 | Entropy: 0.630 📉 | LR: 5.15e-10\r\n",
      "💾 Model saved at episode 95000\r\n",
      "💾 Model saved at episode 95500\r\n",
      "💾 Model saved at episode 96000\r\n",
      "💾 Model saved at episode 96500\r\n",
      "💾 Model saved at episode 97000\r\n",
      "💾 Model saved at episode 97500\r\n",
      "💾 Model saved at episode 98000\r\n",
      "💾 Model saved at episode 98500\r\n",
      "💾 Model saved at episode 99000\r\n",
      "💾 Model saved at episode 99500\r\n",
      "Episode 100000 | Reward:  20.69 | Avg:   6.23 📉 | Success: 40.00% | Budget Exceeded: 60.00% 📈 | Entropy: 0.638 📉 | LR: 2.46e-10\r\n",
      "💾 Model saved at episode 100000\r\n",
      "\r\n",
      "==========================================================================================\r\n",
      "BUDGET-AWARE TRAINING COMPLETED\r\n",
      "==========================================================================================\r\n",
      "Training time: 2142.59 seconds\r\n",
      "Best average reward (last 100): 9.702\r\n",
      "Final average reward (last 100): 6.227\r\n",
      "Final success rate (last 100): 40.00%\r\n",
      "Final budget exceeded rate (last 100): 60.00% 🎯\r\n",
      "Final entropy: 0.638 (max possible: 1.386)\r\n",
      "Final learning rate: 2.46e-10\r\n",
      "First positive reward: Episode 27\r\n",
      "Learning rate decayed 133 times during training\r\n",
      "\r\n",
      "Evaluating budget-aware policy over 100 episodes...\r\n",
      "Budget-Aware Evaluation Results:\r\n",
      "  Mean Reward: 13.974 ± 10.993\r\n",
      "  Mean Length: 11.20\r\n",
      "  Mean Cost: 122.00\r\n",
      "  Mean Skill Improvement: 2.673\r\n",
      "  Mean Budget Utilization: 101.67%\r\n",
      "  Success Rate: 72.00% 🎯\r\n",
      "  Budget Exceeded Rate: 28.00% 💰\r\n",
      "  Reward Improvement from Start: +16.863\r\n",
      "  Training Time: 2142.59s\r\n",
      "  Final Success Rate: 72.00%\r\n",
      "  Final Budget Exceeded Rate: 28.00%\r\n",
      "  Final Mean Reward: 13.974\r\n",
      "\r\n",
      "Testing Configuration: fast_learner_frugal\r\n",
      "  Reward Strategy: hybrid\r\n",
      "  Cost Penalty: 0.02\r\n",
      "  Budget Penalties: base=5.0, max=10.0\r\n",
      "  Entropy Coefficient: 0.001\r\n",
      "--------------------------------------------------\r\n",
      "Initialized budget-aware agent with LR=0.0005, cost_penalty=0.02, step_size=1500\r\n",
      "Starting Budget-Aware Training with Enhanced Cost Penalties...\r\n",
      "Episodes: 100000\r\n",
      "Environment: 8 skills, 4 training modules\r\n",
      "Agent: Actor-Critic with entropy regularization\r\n",
      "Reward Strategy: hybrid\r\n",
      "Learning Rate: 0.0005 (StepLR: step=1500, gamma=0.9)\r\n",
      "Cost Penalty: 0.02 (5x increased for budget awareness)\r\n",
      "Budget Penalties: base=5.0, max=10.0\r\n",
      "------------------------------------------------------------------------------------------\r\n",
      "🎉 First positive reward achieved at episode 77!\r\n",
      "💾 Model saved at episode 500\r\n",
      "💾 Model saved at episode 1000\r\n",
      "💾 Model saved at episode 1500\r\n",
      "💾 Model saved at episode 2000\r\n",
      "💾 Model saved at episode 2500\r\n",
      "💾 Model saved at episode 3000\r\n",
      "💾 Model saved at episode 3500\r\n",
      "💾 Model saved at episode 4000\r\n",
      "💾 Model saved at episode 4500\r\n",
      "Episode 5000 | Reward:  -4.55 | Avg:  -1.06 📈 | Success: 60.00% | Budget Exceeded: 40.00% 📉 | Entropy: 0.471 📈 | LR: 3.65e-04\r\n",
      "💾 Model saved at episode 5000\r\n",
      "💾 Model saved at episode 5500\r\n",
      "💾 Model saved at episode 6000\r\n",
      "💾 Model saved at episode 6500\r\n",
      "💾 Model saved at episode 7000\r\n",
      "💾 Model saved at episode 7500\r\n",
      "💾 Model saved at episode 8000\r\n",
      "💾 Model saved at episode 8500\r\n",
      "💾 Model saved at episode 9000\r\n",
      "💾 Model saved at episode 9500\r\n",
      "Episode 10000 | Reward:  -5.09 | Avg:  -1.34 📈 | Success: 54.00% | Budget Exceeded: 46.00% 📉 | Entropy: 0.330 📈 | LR: 2.66e-04\r\n",
      "💾 Model saved at episode 10000\r\n",
      "💾 Model saved at episode 10500\r\n",
      "💾 Model saved at episode 11000\r\n",
      "💾 Model saved at episode 11500\r\n",
      "🎯 80% success rate achieved at episode 11704!\r\n",
      "💰 Budget discipline achieved (≤20% exceeded) at episode 11704!\r\n",
      "💾 Model saved at episode 12000\r\n",
      "💾 Model saved at episode 12500\r\n",
      "💾 Model saved at episode 13000\r\n",
      "💾 Model saved at episode 13500\r\n",
      "💾 Model saved at episode 14000\r\n",
      "💾 Model saved at episode 14500\r\n",
      "Episode 15000 | Reward:   1.27 | Avg:  -0.59 📈 | Success: 67.00% | Budget Exceeded: 33.00% 📉 | Entropy: 0.278 📈 | LR: 1.74e-04\r\n",
      "💾 Model saved at episode 15000\r\n",
      "💾 Model saved at episode 15500\r\n",
      "💾 Model saved at episode 16000\r\n",
      "💾 Model saved at episode 16500\r\n",
      "💾 Model saved at episode 17000\r\n",
      "💾 Model saved at episode 17500\r\n",
      "💾 Model saved at episode 18000\r\n",
      "💾 Model saved at episode 18500\r\n",
      "💾 Model saved at episode 19000\r\n",
      "💾 Model saved at episode 19500\r\n",
      "Episode 20000 | Reward:   1.12 | Avg:  -0.37 📈 | Success: 70.00% | Budget Exceeded: 30.00% 📉 | Entropy: 0.247 📈 | LR: 1.27e-04\r\n",
      "💾 Model saved at episode 20000\r\n",
      "💾 Model saved at episode 20500\r\n",
      "💾 Model saved at episode 21000\r\n",
      "💾 Model saved at episode 21500\r\n",
      "💾 Model saved at episode 22000\r\n",
      "💾 Model saved at episode 22500\r\n",
      "💾 Model saved at episode 23000\r\n",
      "💾 Model saved at episode 23500\r\n",
      "💾 Model saved at episode 24000\r\n",
      "💾 Model saved at episode 24500\r\n",
      "Episode 25000 | Reward:   1.11 | Avg:   0.31 📉 | Success: 80.00% | Budget Exceeded: 20.00% 📈 | Entropy: 0.227 📉 | LR: 9.27e-05\r\n",
      "💾 Model saved at episode 25000\r\n",
      "💾 Model saved at episode 25500\r\n",
      "💾 Model saved at episode 26000\r\n",
      "💾 Model saved at episode 26500\r\n",
      "💾 Model saved at episode 27000\r\n",
      "💾 Model saved at episode 27500\r\n",
      "💾 Model saved at episode 28000\r\n",
      "💾 Model saved at episode 28500\r\n",
      "💾 Model saved at episode 29000\r\n",
      "💾 Model saved at episode 29500\r\n",
      "Episode 30000 | Reward:   1.21 | Avg:   0.25 📈 | Success: 79.00% | Budget Exceeded: 21.00% 📉 | Entropy: 0.220 📈 | LR: 6.08e-05\r\n",
      "💾 Model saved at episode 30000\r\n",
      "💾 Model saved at episode 30500\r\n",
      "💾 Model saved at episode 31000\r\n",
      "💾 Model saved at episode 31500\r\n",
      "💾 Model saved at episode 32000\r\n",
      "💾 Model saved at episode 32500\r\n",
      "💾 Model saved at episode 33000\r\n",
      "💾 Model saved at episode 33500\r\n",
      "💾 Model saved at episode 34000\r\n",
      "💾 Model saved at episode 34500\r\n",
      "Episode 35000 | Reward:   1.15 | Avg:   0.10 📈 | Success: 77.00% | Budget Exceeded: 23.00% 📉 | Entropy: 0.212 📈 | LR: 4.43e-05\r\n",
      "💾 Model saved at episode 35000\r\n",
      "💾 Model saved at episode 35500\r\n",
      "💾 Model saved at episode 36000\r\n",
      "💾 Model saved at episode 36500\r\n",
      "💾 Model saved at episode 37000\r\n",
      "💾 Model saved at episode 37500\r\n",
      "💾 Model saved at episode 38000\r\n",
      "💾 Model saved at episode 38500\r\n",
      "💾 Model saved at episode 39000\r\n",
      "💾 Model saved at episode 39500\r\n",
      "Episode 40000 | Reward:   1.94 | Avg:   0.21 📈 | Success: 78.00% | Budget Exceeded: 22.00% 📉 | Entropy: 0.209 📉 | LR: 3.23e-05\r\n",
      "💾 Model saved at episode 40000\r\n",
      "💾 Model saved at episode 40500\r\n",
      "💾 Model saved at episode 41000\r\n",
      "💾 Model saved at episode 41500\r\n",
      "💾 Model saved at episode 42000\r\n",
      "💾 Model saved at episode 42500\r\n",
      "💾 Model saved at episode 43000\r\n",
      "💾 Model saved at episode 43500\r\n",
      "💾 Model saved at episode 44000\r\n",
      "💾 Model saved at episode 44500\r\n",
      "Episode 45000 | Reward:   0.78 | Avg:   0.25 📈 | Success: 80.00% | Budget Exceeded: 20.00% 📉 | Entropy: 0.217 📈 | LR: 2.12e-05\r\n",
      "💾 Model saved at episode 45000\r\n",
      "💾 Model saved at episode 45500\r\n",
      "💾 Model saved at episode 46000\r\n",
      "💾 Model saved at episode 46500\r\n",
      "💾 Model saved at episode 47000\r\n",
      "💾 Model saved at episode 47500\r\n",
      "💾 Model saved at episode 48000\r\n",
      "💾 Model saved at episode 48500\r\n",
      "💾 Model saved at episode 49000\r\n",
      "💾 Model saved at episode 49500\r\n",
      "Episode 50000 | Reward:   1.75 | Avg:   0.10 📉 | Success: 77.00% | Budget Exceeded: 23.00% 📈 | Entropy: 0.213 📉 | LR: 1.55e-05\r\n",
      "💾 Model saved at episode 50000\r\n",
      "💾 Model saved at episode 50500\r\n",
      "💾 Model saved at episode 51000\r\n",
      "💾 Model saved at episode 51500\r\n",
      "💾 Model saved at episode 52000\r\n",
      "💾 Model saved at episode 52500\r\n",
      "💾 Model saved at episode 53000\r\n",
      "💾 Model saved at episode 53500\r\n",
      "💾 Model saved at episode 54000\r\n",
      "💾 Model saved at episode 54500\r\n",
      "Episode 55000 | Reward:   1.77 | Avg:  -0.03 📉 | Success: 75.00% | Budget Exceeded: 25.00% 📈 | Entropy: 0.208 📉 | LR: 1.13e-05\r\n",
      "💾 Model saved at episode 55000\r\n",
      "💾 Model saved at episode 55500\r\n",
      "💾 Model saved at episode 56000\r\n",
      "💾 Model saved at episode 56500\r\n",
      "💾 Model saved at episode 57000\r\n",
      "💾 Model saved at episode 57500\r\n",
      "💾 Model saved at episode 58000\r\n",
      "💾 Model saved at episode 58500\r\n",
      "💾 Model saved at episode 59000\r\n",
      "💾 Model saved at episode 59500\r\n",
      "Episode 60000 | Reward:   1.53 | Avg:   0.68 📉 | Success: 85.00% | Budget Exceeded: 15.00% 📈 | Entropy: 0.212 📈 | LR: 7.39e-06\r\n",
      "💾 Model saved at episode 60000\r\n",
      "💾 Model saved at episode 60500\r\n",
      "💾 Model saved at episode 61000\r\n",
      "💾 Model saved at episode 61500\r\n",
      "💾 Model saved at episode 62000\r\n",
      "💾 Model saved at episode 62500\r\n",
      "💾 Model saved at episode 63000\r\n",
      "💾 Model saved at episode 63500\r\n",
      "💾 Model saved at episode 64000\r\n",
      "💾 Model saved at episode 64500\r\n",
      "Episode 65000 | Reward:   0.85 | Avg:  -0.09 📈 | Success: 74.00% | Budget Exceeded: 26.00% 📉 | Entropy: 0.212 📉 | LR: 5.39e-06\r\n",
      "💾 Model saved at episode 65000\r\n",
      "💾 Model saved at episode 65500\r\n",
      "💾 Model saved at episode 66000\r\n",
      "💾 Model saved at episode 66500\r\n",
      "💾 Model saved at episode 67000\r\n",
      "💾 Model saved at episode 67500\r\n",
      "💾 Model saved at episode 68000\r\n",
      "💾 Model saved at episode 68500\r\n",
      "💾 Model saved at episode 69000\r\n",
      "💾 Model saved at episode 69500\r\n",
      "Episode 70000 | Reward:   2.03 | Avg:   0.20 📈 | Success: 79.00% | Budget Exceeded: 21.00% 📉 | Entropy: 0.211 📈 | LR: 3.93e-06\r\n",
      "💾 Model saved at episode 70000\r\n",
      "💾 Model saved at episode 70500\r\n",
      "💾 Model saved at episode 71000\r\n",
      "💾 Model saved at episode 71500\r\n",
      "💾 Model saved at episode 72000\r\n",
      "💾 Model saved at episode 72500\r\n",
      "💾 Model saved at episode 73000\r\n",
      "💾 Model saved at episode 73500\r\n",
      "💾 Model saved at episode 74000\r\n",
      "💾 Model saved at episode 74500\r\n",
      "Episode 75000 | Reward:  -5.47 | Avg:   0.33 📉 | Success: 81.00% | Budget Exceeded: 19.00% 📈 | Entropy: 0.213 📉 | LR: 2.58e-06\r\n",
      "💾 Model saved at episode 75000\r\n",
      "💾 Model saved at episode 75500\r\n",
      "💾 Model saved at episode 76000\r\n",
      "💾 Model saved at episode 76500\r\n",
      "💾 Model saved at episode 77000\r\n",
      "💾 Model saved at episode 77500\r\n",
      "💾 Model saved at episode 78000\r\n",
      "💾 Model saved at episode 78500\r\n",
      "💾 Model saved at episode 79000\r\n",
      "💾 Model saved at episode 79500\r\n",
      "Episode 80000 | Reward:  -5.12 | Avg:  -0.28 📉 | Success: 72.00% | Budget Exceeded: 28.00% 📈 | Entropy: 0.218 📉 | LR: 1.88e-06\r\n",
      "💾 Model saved at episode 80000\r\n",
      "💾 Model saved at episode 80500\r\n",
      "💾 Model saved at episode 81000\r\n",
      "💾 Model saved at episode 81500\r\n",
      "💾 Model saved at episode 82000\r\n",
      "💾 Model saved at episode 82500\r\n",
      "💾 Model saved at episode 83000\r\n",
      "💾 Model saved at episode 83500\r\n",
      "💾 Model saved at episode 84000\r\n",
      "💾 Model saved at episode 84500\r\n",
      "Episode 85000 | Reward:  -4.67 | Avg:  -0.80 📈 | Success: 63.00% | Budget Exceeded: 37.00% 📉 | Entropy: 0.209 📉 | LR: 1.37e-06\r\n",
      "💾 Model saved at episode 85000\r\n",
      "💾 Model saved at episode 85500\r\n",
      "💾 Model saved at episode 86000\r\n",
      "💾 Model saved at episode 86500\r\n",
      "💾 Model saved at episode 87000\r\n",
      "💾 Model saved at episode 87500\r\n",
      "💾 Model saved at episode 88000\r\n",
      "💾 Model saved at episode 88500\r\n",
      "💾 Model saved at episode 89000\r\n",
      "💾 Model saved at episode 89500\r\n",
      "Episode 90000 | Reward:   0.80 | Avg:   0.05 📈 | Success: 76.00% | Budget Exceeded: 24.00% 📉 | Entropy: 0.212 📉 | LR: 8.99e-07\r\n",
      "💾 Model saved at episode 90000\r\n",
      "💾 Model saved at episode 90500\r\n",
      "💾 Model saved at episode 91000\r\n",
      "💾 Model saved at episode 91500\r\n",
      "💾 Model saved at episode 92000\r\n",
      "💾 Model saved at episode 92500\r\n",
      "💾 Model saved at episode 93000\r\n",
      "💾 Model saved at episode 93500\r\n",
      "💾 Model saved at episode 94000\r\n",
      "💾 Model saved at episode 94500\r\n",
      "Episode 95000 | Reward:   1.43 | Avg:   0.63 📉 | Success: 85.00% | Budget Exceeded: 15.00% 📈 | Entropy: 0.204 📈 | LR: 6.55e-07\r\n",
      "💾 Model saved at episode 95000\r\n",
      "💾 Model saved at episode 95500\r\n",
      "💾 Model saved at episode 96000\r\n",
      "💾 Model saved at episode 96500\r\n",
      "💾 Model saved at episode 97000\r\n",
      "💾 Model saved at episode 97500\r\n",
      "💾 Model saved at episode 98000\r\n",
      "💾 Model saved at episode 98500\r\n",
      "💾 Model saved at episode 99000\r\n",
      "💾 Model saved at episode 99500\r\n",
      "Episode 100000 | Reward:  -5.48 | Avg:   0.23 📈 | Success: 79.00% | Budget Exceeded: 21.00% 📉 | Entropy: 0.209 📈 | LR: 4.78e-07\r\n",
      "💾 Model saved at episode 100000\r\n",
      "\r\n",
      "==========================================================================================\r\n",
      "BUDGET-AWARE TRAINING COMPLETED\r\n",
      "==========================================================================================\r\n",
      "Training time: 2185.86 seconds\r\n",
      "Best average reward (last 100): 1.055\r\n",
      "Final average reward (last 100): 0.233\r\n",
      "Final success rate (last 100): 79.00%\r\n",
      "Final budget exceeded rate (last 100): 21.00% 🎯\r\n",
      "Final entropy: 0.209 (max possible: 1.386)\r\n",
      "Final learning rate: 4.78e-07\r\n",
      "First positive reward: Episode 77\r\n",
      "80% success milestone: Episode 11704\r\n",
      "Budget discipline milestone: Episode 11704\r\n",
      "Learning rate decayed 66 times during training\r\n",
      "\r\n",
      "Evaluating budget-aware policy over 100 episodes...\r\n",
      "Budget-Aware Evaluation Results:\r\n",
      "  Mean Reward: 0.381 ± 2.582\r\n",
      "  Mean Length: 11.10\r\n",
      "  Mean Cost: 121.16\r\n",
      "  Mean Skill Improvement: 2.623\r\n",
      "  Mean Budget Utilization: 100.97%\r\n",
      "  Success Rate: 82.00% 🎯\r\n",
      "  Budget Exceeded Rate: 18.00% 💰\r\n",
      "  Reward Improvement from Start: +5.256\r\n",
      "  Training Time: 2185.86s\r\n",
      "  Final Success Rate: 82.00%\r\n",
      "  Final Budget Exceeded Rate: 18.00%\r\n",
      "  Final Mean Reward: 0.381\r\n",
      "\r\n",
      "====================================================================================================\r\n",
      "BUDGET-AWARE PERFORMANCE COMPARISON SUMMARY\r\n",
      "====================================================================================================\r\n",
      "Rank Configuration        Success Rate Budget Exceed Mean Reward  Budget Discipline\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "1    fast_learner_frugal  82.00%       18.00%       0.381        11704           \r\n",
      "2    ultra_conservative   78.00%       22.00%       -0.611       Not achieved    \r\n",
      "3    strong_incentive     73.00%       27.00%       0.403        22983           \r\n",
      "4    terminal_focus_strong 72.00%       28.00%       13.974       Not achieved    \r\n",
      "5    balanced_approach    62.00%       38.00%       0.724        44616           \r\n",
      "\r\n",
      "====================================================================================================\r\n",
      "WINNER (Best Budget Management): fast_learner_frugal\r\n",
      "  Reward Strategy: hybrid\r\n",
      "  Cost Penalty: 0.02\r\n",
      "  Budget Penalties: base=5.0, max=10.0\r\n",
      "  Success Rate: 82.00%\r\n",
      "  Budget Exceeded Rate: 18.00%\r\n",
      "  Budget Discipline Achieved: Episode 11704\r\n",
      "\r\n",
      "Generating Pareto front comparison plot...\r\n",
      "/kaggle/working/single_file_rl_training_final.py:1267: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\r\n",
      "  colors = cm.get_cmap('tab10', len(self.results))\r\n",
      "-> Pareto front plot saved to plots/comparison_pareto_front.png\r\n",
      "Figure(1400x900)\r\n",
      "\r\n",
      "Enhanced system execution completed.\r\n"
     ]
    }
   ],
   "source": [
    "!python single_file_rl_training_final.py --mode compare --episodes 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a28ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T10:38:18.997385Z",
     "iopub.status.busy": "2025-07-10T10:38:18.996754Z",
     "iopub.status.idle": "2025-07-10T10:38:19.000672Z",
     "shell.execute_reply": "2025-07-10T10:38:18.999965Z"
    },
    "papermill": {
     "duration": 0.051791,
     "end_time": "2025-07-10T10:38:19.001776",
     "exception": false,
     "start_time": "2025-07-10T10:38:18.949985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python single_file_rl_training_final.py --mode train \\\n",
    "#     --episodes 25000 \\\n",
    "#     --reward-strategy terminal \\\n",
    "#     --cost-penalty 0.008 \\\n",
    "#     --entropy-coef 0.001 \\\n",
    "#     --base-budget-penalty 5.0 \\\n",
    "#     --max-budget-penalty 10.0 \\\n",
    "#     --terminal-bonus-multiplier 3.5"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10923.743779,
   "end_time": "2025-07-10T10:38:19.375444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-10T07:36:15.631665",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
