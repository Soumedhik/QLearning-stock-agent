{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile single_file_rl_training.py\n#!/usr/bin/env python3\n\"\"\"\nSingle-file implementation of Reinforcement Learning for Hierarchical Employee Training Optimization\nBased on the research paper by Soumedhik Bharati, Rupsha Sadhukhan, Debanjali Saha\n\nThis file contains all components in a single executable:\n- Custom Gymnasium environment for employee training\n- REINFORCE agent with optional Actor-Critic baseline\n- Training loop with logging and evaluation\n- Visualization utilities\n- Main execution and example usage\n\nUsage:\n    python single_file_rl_training.py --mode train --episodes 1000\n    python single_file_rl_training.py --mode evaluate --model models/trained_model.pth\n    python single_file_rl_training.py --mode visualize\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport time\nimport copy\nfrom collections import defaultdict, deque\nfrom typing import Dict, List, Tuple, Optional\n\n# Core libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Gymnasium for RL environment\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n# PyTorch for neural networks\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n\n# =============================================================================\n# EMPLOYEE TRAINING ENVIRONMENT\n# =============================================================================\n\nclass EmployeeTrainingEnv(gym.Env):\n    \"\"\"\n    Custom Gymnasium environment for employee training optimization.\n    Implements hierarchical skill structure with cross-attribute synergy and forgetting.\n    \"\"\"\n    \n    metadata = {\"render_modes\": [\"human\"]}\n    \n    def __init__(self, \n                 D: int = 8,  # Number of leaf-level sub-attributes\n                 K: int = 4,  # Number of training modules\n                 alpha: List[float] = None,  # Learning rates for each module\n                 beta: float = 0.02,  # Forgetting rate\n                 kappa: float = 1.5,  # Diminishing returns exponent\n                 C_max: float = 100.0,  # Maximum cost budget\n                 gamma: float = 0.99):  # Discount factor\n        \n        super().__init__()\n        \n        self.D = D  # Number of sub-attributes\n        self.K = K  # Number of training modules\n        self.beta = beta\n        self.kappa = kappa\n        self.C_max = C_max\n        self.gamma = gamma\n        \n        # Learning rates for each training module\n        self.alpha = alpha if alpha is not None else [0.3, 0.25, 0.2, 0.35]\n        \n        # Training module costs\n        self.costs = [10.0, 15.0, 20.0, 12.0]  # Cost for each training module\n        \n        # Define which sub-attributes each training module targets\n        self.module_targets = {\n            0: [0, 1],      # Technical Skills: Coding, Debugging\n            1: [2, 3],      # Technical Skills: Testing, Architecture\n            2: [4, 5],      # Soft Skills: Communication, Leadership\n            3: [6, 7]       # Soft Skills: Teamwork, Problem-solving\n        }\n        \n        # Cross-attribute synergy matrix (ρjk)\n        self.synergy_matrix = self._initialize_synergy_matrix()\n        \n        # Gymnasium spaces\n        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(D,), dtype=np.float32)\n        self.action_space = spaces.Discrete(K)\n        \n        # Episode state\n        self.current_skills = None\n        self.current_cost = 0.0\n        self.episode_length = 0\n        self.max_episode_length = 50\n        \n    def _initialize_synergy_matrix(self) -> np.ndarray:\n        \"\"\"Initialize the cross-attribute synergy matrix.\"\"\"\n        synergy = np.zeros((self.D, self.D))\n        \n        # Within technical skills\n        synergy[0, 1] = synergy[1, 0] = 0.3  # Coding <-> Debugging\n        synergy[0, 2] = synergy[2, 0] = 0.2  # Coding <-> Testing\n        synergy[1, 2] = synergy[2, 1] = 0.4  # Debugging <-> Testing\n        synergy[2, 3] = synergy[3, 2] = 0.3  # Testing <-> Architecture\n        \n        # Within soft skills\n        synergy[4, 5] = synergy[5, 4] = 0.4  # Communication <-> Leadership\n        synergy[4, 6] = synergy[6, 4] = 0.3  # Communication <-> Teamwork\n        synergy[5, 6] = synergy[6, 5] = 0.2  # Leadership <-> Teamwork\n        synergy[6, 7] = synergy[7, 6] = 0.3  # Teamwork <-> Problem-solving\n        \n        # Cross-domain synergies (technical to soft)\n        synergy[1, 7] = synergy[7, 1] = 0.15  # Debugging <-> Problem-solving\n        synergy[3, 5] = synergy[5, 3] = 0.1   # Architecture <-> Leadership\n        \n        return synergy\n    \n    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Reset the environment to initial state.\"\"\"\n        super().reset(seed=seed)\n        \n        # Initialize skills randomly between 0.1 and 0.6\n        self.current_skills = self.np_random.uniform(0.1, 0.6, size=self.D).astype(np.float32)\n        self.current_cost = 0.0\n        self.episode_length = 0\n        \n        return self.current_skills.copy(), {}\n    \n    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n        \"\"\"Execute one step in the environment.\"\"\"\n        if action < 0 or action >= self.K:\n            raise ValueError(f\"Invalid action: {action}\")\n        \n        # Calculate cost and check budget constraint\n        action_cost = self.costs[action]\n        if self.current_cost + action_cost > self.C_max:\n            # Episode terminates if budget exceeded\n            return self.current_skills.copy(), -10.0, True, False, {\"budget_exceeded\": True}\n        \n        # Store previous skills for reward calculation\n        prev_skills = self.current_skills.copy()\n        \n        # Apply training module\n        self.current_skills = self._apply_training(self.current_skills, action)\n        \n        # Update cost and episode length\n        self.current_cost += action_cost\n        self.episode_length += 1\n        \n        # Calculate reward\n        reward = self._calculate_reward(prev_skills, self.current_skills, action_cost)\n        \n        # Check termination conditions\n        terminated = (self.episode_length >= self.max_episode_length or \n                     self.current_cost >= self.C_max)\n        \n        info = {\n            \"current_cost\": self.current_cost,\n            \"episode_length\": self.episode_length,\n            \"skill_improvement\": np.sum(self.current_skills - prev_skills)\n        }\n        \n        return self.current_skills.copy(), reward, terminated, False, info\n    \n    def _apply_training(self, skills: np.ndarray, action: int) -> np.ndarray:\n        \"\"\"Apply training module to current skills.\"\"\"\n        new_skills = skills.copy()\n        alpha_a = self.alpha[action]\n        target_attributes = self.module_targets[action]\n        \n        # Calculate potential gains for each attribute\n        for j in range(self.D):\n            # Direct training effect\n            if j in target_attributes:\n                delta_j = (1 - skills[j]) ** self.kappa\n            else:\n                # Cross-attribute synergy effect\n                delta_j = 0.0\n                for k in target_attributes:\n                    delta_j += self.synergy_matrix[j, k] * (1 - skills[j]) ** self.kappa\n            \n            # Apply training gain and forgetting\n            new_skills[j] = skills[j] + alpha_a * delta_j - self.beta * skills[j]\n        \n        # Clip skills to valid range [0, 1]\n        new_skills = np.clip(new_skills, 0.0, 1.0)\n        \n        return new_skills\n    \n    def _calculate_reward(self, prev_skills: np.ndarray, new_skills: np.ndarray, cost: float) -> float:\n        \"\"\"Calculate reward based on skill improvement and cost.\"\"\"\n        skill_improvement = np.sum(new_skills - prev_skills)\n        # FIXED: Reduced cost penalty from 0.1 to 0.01 to enable positive rewards\n        return skill_improvement - 0.01 * cost\n    \n    def get_hierarchical_skills(self, skills: np.ndarray) -> Dict[str, float]:\n        \"\"\"Calculate hierarchical skill aggregations.\"\"\"\n        return {\n            \"technical_skills\": np.mean(skills[0:4]),\n            \"soft_skills\": np.mean(skills[4:8]),\n            \"coding_debugging\": np.mean(skills[0:2]),\n            \"testing_architecture\": np.mean(skills[2:4]),\n            \"communication_leadership\": np.mean(skills[4:6]),\n            \"teamwork_problem_solving\": np.mean(skills[6:8])\n        }\n    \n    def render(self, mode: str = \"human\") -> None:\n        \"\"\"Render the current state.\"\"\"\n        if mode == \"human\":\n            hierarchical = self.get_hierarchical_skills(self.current_skills)\n            print(f\"Episode Length: {self.episode_length}, Cost: {self.current_cost:.2f}/{self.C_max}\")\n            print(\"Hierarchical Skills:\")\n            for skill_name, value in hierarchical.items():\n                print(f\"  {skill_name}: {value:.3f}\")\n            print(f\"Individual Skills: {self.current_skills}\")\n\n\n# =============================================================================\n# NEURAL NETWORKS AND AGENT\n# =============================================================================\n\nclass PolicyNetwork(nn.Module):\n    \"\"\"\n    Neural network for policy approximation in REINFORCE algorithm.\n    Takes employee skill vector as input and outputs action probabilities.\n    \"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        super(PolicyNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return F.softmax(x, dim=-1)\n\n\nclass ValueNetwork(nn.Module):\n    \"\"\"\n    Value network for Actor-Critic implementation.\n    Estimates state-value function V(s).\n    \"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dim: int):\n        super(ValueNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        return self.fc3(x)\n\n\nclass REINFORCEAgent:\n    \"\"\"\n    REINFORCE agent for employee training optimization.\n    Implements policy gradient method with optional baseline.\n    \"\"\"\n    \n    def __init__(self, \n                 state_dim: int,\n                 action_dim: int,\n                 hidden_dim: int = 128,\n                 learning_rate: float = 1e-3,\n                 use_baseline: bool = True):\n        \n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.use_baseline = use_baseline\n        \n        # Policy network\n        self.policy_net = PolicyNetwork(state_dim, hidden_dim, action_dim)\n        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n        \n        # Value network (optional baseline)\n        if use_baseline:\n            self.value_net = ValueNetwork(state_dim, hidden_dim)\n            self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=learning_rate)\n        \n        # Episode memory\n        self.log_probs: List[torch.Tensor] = []\n        self.rewards: List[float] = []\n        self.states: List[torch.Tensor] = []\n        \n    def select_action(self, state: np.ndarray) -> int:\n        \"\"\"Select action using current policy.\"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action_probs = self.policy_net(state_tensor)\n        \n        # Sample action from probability distribution\n        dist = Categorical(action_probs)\n        action = dist.sample()\n        \n        # Store log probability for policy update\n        self.log_probs.append(dist.log_prob(action))\n        self.states.append(state_tensor)\n        \n        return action.item()\n    \n    def store_reward(self, reward: float) -> None:\n        \"\"\"Store reward for current step.\"\"\"\n        self.rewards.append(reward)\n    \n    def update_policy(self, gamma: float = 0.99) -> Dict[str, float]:\n        \"\"\"Update policy using REINFORCE algorithm.\"\"\"\n        if len(self.rewards) == 0:\n            return {\"policy_loss\": 0.0, \"value_loss\": 0.0}\n        \n        # Calculate discounted returns\n        returns = []\n        R = 0\n        for r in reversed(self.rewards):\n            R = r + gamma * R\n            returns.insert(0, R)\n        \n        returns = torch.tensor(returns, dtype=torch.float32)\n        \n        # Normalize returns for stability\n        if len(returns) > 1:\n            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n        \n        policy_loss = 0\n        value_loss = 0\n        \n        # Calculate policy loss\n        for i, (log_prob, R) in enumerate(zip(self.log_probs, returns)):\n            if self.use_baseline:\n                # Use value function as baseline\n                state = self.states[i]\n                baseline = self.value_net(state).squeeze()\n                advantage = R - baseline\n                policy_loss += -log_prob * advantage.detach()\n            else:\n                policy_loss += -log_prob * R\n        \n        # Update policy network\n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n        self.policy_optimizer.step()\n        \n        # Update value network if using baseline\n        if self.use_baseline:\n            # Compute value predictions for all states\n            value_predictions = []\n            for state in self.states:\n                value_pred = self.value_net(state).squeeze()\n                # Ensure value_pred is at least 1-dimensional for concatenation\n                if value_pred.dim() == 0:\n                    value_pred = value_pred.unsqueeze(0)\n                value_predictions.append(value_pred)\n            \n            # Concatenate value predictions\n            if len(value_predictions) > 1:\n                value_preds = torch.cat(value_predictions)\n            else:\n                value_preds = value_predictions[0]\n            \n            # Ensure returns has the same shape as value predictions\n            if returns.dim() == 0:\n                returns = returns.unsqueeze(0)\n            \n            value_loss = F.mse_loss(value_preds, returns)\n            \n            self.value_optimizer.zero_grad()\n            value_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 1.0)\n            self.value_optimizer.step()\n        \n        # Clear episode memory\n        self.log_probs.clear()\n        self.rewards.clear()\n        self.states.clear()\n        \n        return {\n            \"policy_loss\": policy_loss.item(),\n            \"value_loss\": value_loss.item() if self.use_baseline else 0.0\n        }\n    \n    def get_state_value(self, state: np.ndarray) -> float:\n        \"\"\"Get state value estimate (if using baseline).\"\"\"\n        if not self.use_baseline:\n            return 0.0\n        \n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            return self.value_net(state_tensor).item()\n    \n    def save_model(self, filepath: str) -> None:\n        \"\"\"Save trained model.\"\"\"\n        checkpoint = {\n            'policy_net': self.policy_net.state_dict(),\n            'policy_optimizer': self.policy_optimizer.state_dict(),\n            'state_dim': self.state_dim,\n            'action_dim': self.action_dim,\n            'use_baseline': self.use_baseline\n        }\n        \n        if self.use_baseline:\n            checkpoint['value_net'] = self.value_net.state_dict()\n            checkpoint['value_optimizer'] = self.value_optimizer.state_dict()\n        \n        torch.save(checkpoint, filepath)\n    \n    def load_model(self, filepath: str) -> None:\n        \"\"\"Load trained model.\"\"\"\n        checkpoint = torch.load(filepath)\n        self.policy_net.load_state_dict(checkpoint['policy_net'])\n        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n        \n        if self.use_baseline and 'value_net' in checkpoint:\n            self.value_net.load_state_dict(checkpoint['value_net'])\n            self.value_optimizer.load_state_dict(checkpoint['value_optimizer'])\n\n\n# =============================================================================\n# TRAINING LOOP\n# =============================================================================\n\nclass TrainingLoop:\n    \"\"\"\n    Main training loop for the employee training optimization system.\n    Handles episode execution, logging, and progress tracking.\n    \"\"\"\n    \n    def __init__(self, env, agent, config: Dict):\n        self.env = env\n        self.agent = agent\n        self.config = config\n        \n        # Training parameters\n        self.num_episodes = config.get('num_episodes', 1000)\n        self.gamma = config.get('gamma', 0.99)\n        self.log_interval = config.get('log_interval', 50)\n        self.save_interval = config.get('save_interval', 200)\n        self.model_save_path = config.get('model_save_path', 'models/employee_training_model.pth')\n        \n        # Metrics tracking\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_costs = []\n        self.policy_losses = []\n        self.value_losses = []\n        self.skill_improvements = []\n        \n        # Recent performance tracking\n        self.recent_rewards = deque(maxlen=100)\n        self.recent_lengths = deque(maxlen=100)\n        \n    def run_episode(self) -> Dict[str, float]:\n        \"\"\"Run a single training episode.\"\"\"\n        state, _ = self.env.reset()\n        episode_reward = 0\n        episode_length = 0\n        total_skill_improvement = 0\n        \n        while True:\n            # Select action\n            action = self.agent.select_action(state)\n            \n            # Execute action\n            next_state, reward, terminated, truncated, info = self.env.step(action)\n            \n            # Store reward\n            self.agent.store_reward(reward)\n            \n            # Update metrics\n            episode_reward += reward\n            episode_length += 1\n            total_skill_improvement += info.get('skill_improvement', 0)\n            \n            # Update state\n            state = next_state\n            \n            # Check termination\n            if terminated or truncated:\n                break\n        \n        # Update policy at end of episode\n        losses = self.agent.update_policy(self.gamma)\n        \n        return {\n            'episode_reward': episode_reward,\n            'episode_length': episode_length,\n            'episode_cost': info.get('current_cost', 0),\n            'skill_improvement': total_skill_improvement,\n            'policy_loss': losses['policy_loss'],\n            'value_loss': losses['value_loss']\n        }\n    \n    def train(self) -> None:\n        \"\"\"Main training loop.\"\"\"\n        print(\"Starting training...\")\n        print(f\"Episodes: {self.num_episodes}\")\n        print(f\"Environment: {self.env.D} skills, {self.env.K} training modules\")\n        print(f\"Agent: {'with' if self.agent.use_baseline else 'without'} baseline\")\n        print(\"-\" * 50)\n        \n        start_time = time.time()\n        \n        for episode in range(self.num_episodes):\n            # Run episode\n            episode_metrics = self.run_episode()\n            \n            # Store metrics\n            self.episode_rewards.append(episode_metrics['episode_reward'])\n            self.episode_lengths.append(episode_metrics['episode_length'])\n            self.episode_costs.append(episode_metrics['episode_cost'])\n            self.skill_improvements.append(episode_metrics['skill_improvement'])\n            self.policy_losses.append(episode_metrics['policy_loss'])\n            self.value_losses.append(episode_metrics['value_loss'])\n            \n            # Update recent performance\n            self.recent_rewards.append(episode_metrics['episode_reward'])\n            self.recent_lengths.append(episode_metrics['episode_length'])\n            \n            # Logging\n            if (episode + 1) % self.log_interval == 0:\n                self._log_progress(episode + 1, episode_metrics)\n            \n            # Save model\n            if (episode + 1) % self.save_interval == 0:\n                self.agent.save_model(self.model_save_path)\n                print(f\"Model saved at episode {episode + 1}\")\n        \n        # Final save\n        self.agent.save_model(self.model_save_path)\n        \n        training_time = time.time() - start_time\n        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n        print(f\"Average reward (last 100 episodes): {np.mean(self.recent_rewards):.3f}\")\n        print(f\"Average length (last 100 episodes): {np.mean(self.recent_lengths):.2f}\")\n    \n    def _log_progress(self, episode: int, metrics: Dict[str, float]) -> None:\n        \"\"\"Log training progress.\"\"\"\n        avg_reward = np.mean(self.recent_rewards)\n        avg_length = np.mean(self.recent_lengths)\n        \n        print(f\"Episode {episode:4d} | \"\n              f\"Reward: {metrics['episode_reward']:6.2f} | \"\n              f\"Avg Reward: {avg_reward:6.2f} | \"\n              f\"Length: {metrics['episode_length']:2d} | \"\n              f\"Cost: {metrics['episode_cost']:5.1f} | \"\n              f\"Skill Δ: {metrics['skill_improvement']:6.3f} | \"\n              f\"Policy Loss: {metrics['policy_loss']:6.3f}\")\n    \n    def evaluate(self, num_episodes: int = 100, render: bool = False) -> Dict[str, float]:\n        \"\"\"Evaluate trained policy.\"\"\"\n        print(f\"\\nEvaluating policy over {num_episodes} episodes...\")\n        \n        eval_rewards = []\n        eval_lengths = []\n        eval_costs = []\n        eval_skill_improvements = []\n        \n        for episode in range(num_episodes):\n            state, _ = self.env.reset()\n            episode_reward = 0\n            episode_length = 0\n            total_skill_improvement = 0\n            \n            while True:\n                # Select action (no exploration)\n                action = self.agent.select_action(state)\n                next_state, reward, terminated, truncated, info = self.env.step(action)\n                \n                episode_reward += reward\n                episode_length += 1\n                total_skill_improvement += info.get('skill_improvement', 0)\n                \n                if render and episode == 0:  # Render first episode\n                    self.env.render()\n                \n                state = next_state\n                \n                if terminated or truncated:\n                    break\n            \n            eval_rewards.append(episode_reward)\n            eval_lengths.append(episode_length)\n            eval_costs.append(info.get('current_cost', 0))\n            eval_skill_improvements.append(total_skill_improvement)\n        \n        results = {\n            'mean_reward': np.mean(eval_rewards),\n            'std_reward': np.std(eval_rewards),\n            'mean_length': np.mean(eval_lengths),\n            'mean_cost': np.mean(eval_costs),\n            'mean_skill_improvement': np.mean(eval_skill_improvements),\n            'success_rate': np.mean([r > 0 for r in eval_rewards])\n        }\n        \n        print(\"Evaluation Results:\")\n        print(f\"  Mean Reward: {results['mean_reward']:.3f} ± {results['std_reward']:.3f}\")\n        print(f\"  Mean Length: {results['mean_length']:.2f}\")\n        print(f\"  Mean Cost: {results['mean_cost']:.2f}\")\n        print(f\"  Mean Skill Improvement: {results['mean_skill_improvement']:.3f}\")\n        print(f\"  Success Rate: {results['success_rate']:.2%}\")\n        \n        return results\n    \n    def plot_training_curves(self, save_path: str = None) -> None:\n        \"\"\"Plot training progress curves.\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        \n        # Episode rewards\n        axes[0, 0].plot(self.episode_rewards, alpha=0.7)\n        axes[0, 0].plot(self._smooth_curve(self.episode_rewards, window=50), 'r-', linewidth=2)\n        axes[0, 0].set_title('Episode Rewards')\n        axes[0, 0].set_xlabel('Episode')\n        axes[0, 0].set_ylabel('Reward')\n        axes[0, 0].grid(True)\n        \n        # Episode lengths\n        axes[0, 1].plot(self.episode_lengths, alpha=0.7)\n        axes[0, 1].plot(self._smooth_curve(self.episode_lengths, window=50), 'r-', linewidth=2)\n        axes[0, 1].set_title('Episode Lengths')\n        axes[0, 1].set_xlabel('Episode')\n        axes[0, 1].set_ylabel('Length')\n        axes[0, 1].grid(True)\n        \n        # Episode costs\n        axes[0, 2].plot(self.episode_costs, alpha=0.7)\n        axes[0, 2].plot(self._smooth_curve(self.episode_costs, window=50), 'r-', linewidth=2)\n        axes[0, 2].set_title('Episode Costs')\n        axes[0, 2].set_xlabel('Episode')\n        axes[0, 2].set_ylabel('Cost')\n        axes[0, 2].grid(True)\n        \n        # Skill improvements\n        axes[1, 0].plot(self.skill_improvements, alpha=0.7)\n        axes[1, 0].plot(self._smooth_curve(self.skill_improvements, window=50), 'r-', linewidth=2)\n        axes[1, 0].set_title('Skill Improvements')\n        axes[1, 0].set_xlabel('Episode')\n        axes[1, 0].set_ylabel('Skill Δ')\n        axes[1, 0].grid(True)\n        \n        # Policy losses\n        axes[1, 1].plot(self.policy_losses, alpha=0.7)\n        axes[1, 1].plot(self._smooth_curve(self.policy_losses, window=50), 'r-', linewidth=2)\n        axes[1, 1].set_title('Policy Losses')\n        axes[1, 1].set_xlabel('Episode')\n        axes[1, 1].set_ylabel('Loss')\n        axes[1, 1].grid(True)\n        \n        # Value losses (if using baseline)\n        if self.agent.use_baseline:\n            axes[1, 2].plot(self.value_losses, alpha=0.7)\n            axes[1, 2].plot(self._smooth_curve(self.value_losses, window=50), 'r-', linewidth=2)\n            axes[1, 2].set_title('Value Losses')\n            axes[1, 2].set_xlabel('Episode')\n            axes[1, 2].set_ylabel('Loss')\n            axes[1, 2].grid(True)\n        else:\n            axes[1, 2].text(0.5, 0.5, 'No Baseline Used', \n                           horizontalalignment='center', verticalalignment='center',\n                           transform=axes[1, 2].transAxes, fontsize=14)\n            axes[1, 2].set_title('Value Losses')\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            print(f\"Training curves saved to {save_path}\")\n        \n        plt.show()\n    \n    def _smooth_curve(self, values: List[float], window: int = 50) -> List[float]:\n        \"\"\"Apply moving average smoothing to a curve.\"\"\"\n        if len(values) < window:\n            return values\n        \n        smoothed = []\n        for i in range(len(values)):\n            start = max(0, i - window + 1)\n            end = i + 1\n            smoothed.append(np.mean(values[start:end]))\n        \n        return smoothed\n\n\n# =============================================================================\n# VISUALIZATION UTILITIES\n# =============================================================================\n\nclass VisualizationUtils:\n    \"\"\"\n    Utility class for visualizing training progress and employee skill development.\n    \"\"\"\n    \n    def __init__(self):\n        plt.style.use('default')  # Use default style for compatibility\n        self.skill_names = [\n            'Coding', 'Debugging', 'Testing', 'Architecture',\n            'Communication', 'Leadership', 'Teamwork', 'Problem-solving'\n        ]\n        \n        self.hierarchical_names = {\n            'technical_skills': 'Technical Skills',\n            'soft_skills': 'Soft Skills',\n            'coding_debugging': 'Coding & Debugging',\n            'testing_architecture': 'Testing & Architecture',\n            'communication_leadership': 'Communication & Leadership',\n            'teamwork_problem_solving': 'Teamwork & Problem-solving'\n        }\n    \n    def plot_skill_progression(self, env, agent, num_episodes: int = 5, save_path: str = None) -> None:\n        \"\"\"Plot skill progression over multiple episodes.\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        colors = plt.cm.Set3(np.linspace(0, 1, len(self.skill_names)))\n        \n        for episode in range(num_episodes):\n            state, _ = env.reset()\n            initial_skills = state.copy()\n            \n            episode_skills = [initial_skills]\n            episode_actions = []\n            \n            while True:\n                action = agent.select_action(state)\n                next_state, _, terminated, truncated, _ = env.step(action)\n                \n                episode_skills.append(next_state.copy())\n                episode_actions.append(action)\n                state = next_state\n                \n                if terminated or truncated:\n                    break\n            \n            episode_skills = np.array(episode_skills)\n            \n            # Plot individual skills\n            for i, (skill_name, color) in enumerate(zip(self.skill_names, colors)):\n                axes[0, 0].plot(episode_skills[:, i], color=color, alpha=0.7, \n                               label=skill_name if episode == 0 else \"\")\n            \n            # Plot hierarchical skills\n            hierarchical_skills = []\n            for skills in episode_skills:\n                hier_skills = env.get_hierarchical_skills(skills)\n                hierarchical_skills.append(hier_skills)\n            \n            hierarchical_skills = pd.DataFrame(hierarchical_skills)\n            \n            for i, (key, name) in enumerate(self.hierarchical_names.items()):\n                if key in hierarchical_skills.columns:\n                    axes[0, 1].plot(hierarchical_skills[key], alpha=0.7,\n                                   label=name if episode == 0 else \"\")\n            \n            # Plot action distribution\n            action_counts = np.bincount(episode_actions, minlength=env.K)\n            axes[1, 0].bar(range(env.K), action_counts, alpha=0.7, \n                          label=f'Episode {episode + 1}')\n            \n            # Plot skill improvement per step\n            skill_improvements = np.diff(np.sum(episode_skills, axis=1))\n            axes[1, 1].plot(skill_improvements, alpha=0.7, \n                           label=f'Episode {episode + 1}')\n        \n        # Configure subplots\n        axes[0, 0].set_title('Individual Skill Progression')\n        axes[0, 0].set_xlabel('Training Step')\n        axes[0, 0].set_ylabel('Skill Level')\n        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        axes[0, 1].set_title('Hierarchical Skill Progression')\n        axes[0, 1].set_xlabel('Training Step')\n        axes[0, 1].set_ylabel('Skill Level')\n        axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        axes[1, 0].set_title('Action Distribution')\n        axes[1, 0].set_xlabel('Training Module')\n        axes[1, 0].set_ylabel('Usage Count')\n        axes[1, 0].set_xticks(range(env.K))\n        axes[1, 0].set_xticklabels([f'Module {i}' for i in range(env.K)])\n        axes[1, 0].legend()\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        axes[1, 1].set_title('Skill Improvement per Step')\n        axes[1, 1].set_xlabel('Training Step')\n        axes[1, 1].set_ylabel('Skill Δ')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            print(f\"Skill progression plot saved to {save_path}\")\n        \n        plt.show()\n    \n    def plot_synergy_matrix(self, env, save_path: str = None) -> None:\n        \"\"\"Plot the cross-attribute synergy matrix.\"\"\"\n        plt.figure(figsize=(10, 8))\n        \n        sns.heatmap(env.synergy_matrix,\n                   xticklabels=self.skill_names,\n                   yticklabels=self.skill_names,\n                   annot=True,\n                   fmt='.2f',\n                   cmap='coolwarm',\n                   center=0,\n                   cbar_kws={'label': 'Synergy Coefficient'})\n        \n        plt.title('Cross-Attribute Synergy Matrix')\n        plt.xlabel('Source Skill')\n        plt.ylabel('Target Skill')\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            print(f\"Synergy matrix plot saved to {save_path}\")\n        \n        plt.show()\n\n\n# =============================================================================\n# CONFIGURATION AND UTILITIES\n# =============================================================================\n\ndef create_directories():\n    \"\"\"Create necessary directories for saving models and plots.\"\"\"\n    os.makedirs('models', exist_ok=True)\n    os.makedirs('plots', exist_ok=True)\n    os.makedirs('logs', exist_ok=True)\n\n\ndef get_default_config():\n    \"\"\"Get default configuration for training.\"\"\"\n    return {\n        'num_episodes': 1000,\n        'gamma': 0.99,\n        'learning_rate': 1e-3,\n        'hidden_dim': 128,\n        'use_baseline': True,\n        'log_interval': 50,\n        'save_interval': 200,\n        'model_save_path': 'models/employee_training_model.pth',\n        'plot_save_path': 'plots/',\n        \n        # Environment parameters\n        'D': 8,  # Number of skills\n        'K': 4,  # Number of training modules\n        'alpha': [0.3, 0.25, 0.2, 0.35],  # Learning rates\n        'beta': 0.02,  # Forgetting rate\n        'kappa': 1.5,  # Diminishing returns\n        'C_max': 100.0,  # Cost budget\n        \n        # Evaluation parameters\n        'eval_episodes': 100,\n        'eval_render': False\n    }\n\n\n# =============================================================================\n# EXAMPLE USAGE FUNCTIONS\n# =============================================================================\n\ndef example_basic_training():\n    \"\"\"Basic training example with default parameters.\"\"\"\n    print(\"=== Basic Training Example ===\")\n    \n    # Create environment\n    env = EmployeeTrainingEnv(D=8, K=4)\n    \n    # Create agent\n    agent = REINFORCEAgent(\n        state_dim=env.D,\n        action_dim=env.K,\n        use_baseline=True\n    )\n    \n    # Training configuration\n    config = {\n        'num_episodes': 200,\n        'gamma': 0.99,\n        'log_interval': 50,\n        'save_interval': 100,\n        'model_save_path': 'models/basic_example.pth'\n    }\n    \n    # Create and run training loop\n    training_loop = TrainingLoop(env, agent, config)\n    training_loop.train()\n    \n    # Evaluate\n    results = training_loop.evaluate(num_episodes=20)\n    print(f\"Training completed. Final performance: {results['mean_reward']:.3f}\")\n    \n    return env, agent, training_loop\n\n\ndef example_skill_analysis():\n    \"\"\"Example showing skill development analysis.\"\"\"\n    print(\"\\n=== Skill Development Analysis ===\")\n    \n    env = EmployeeTrainingEnv()\n    agent = REINFORCEAgent(\n        state_dim=env.D,\n        action_dim=env.K,\n        use_baseline=True\n    )\n    \n    # Quick training\n    config = {\n        'num_episodes': 100,\n        'log_interval': 25,\n        'model_save_path': 'models/analysis_example.pth'\n    }\n    \n    training_loop = TrainingLoop(env, agent, config)\n    training_loop.train()\n    \n    # Analyze skill development\n    state, _ = env.reset()\n    print(f\"Initial skills: {state}\")\n    print(f\"Initial hierarchical skills: {env.get_hierarchical_skills(state)}\")\n    \n    episode_history = []\n    for step in range(20):\n        action = agent.select_action(state)\n        next_state, reward, terminated, truncated, info = env.step(action)\n        \n        episode_history.append({\n            'step': step,\n            'action': action,\n            'reward': reward,\n            'skills': next_state.copy(),\n            'hierarchical': env.get_hierarchical_skills(next_state),\n            'skill_improvement': info.get('skill_improvement', 0)\n        })\n        \n        print(f\"Step {step}: Action {action}, Reward {reward:.3f}, \"\n              f\"Skill Δ {info.get('skill_improvement', 0):.3f}\")\n        \n        state = next_state\n        if terminated or truncated:\n            break\n    \n    print(f\"\\nFinal skills: {state}\")\n    print(f\"Final hierarchical skills: {env.get_hierarchical_skills(state)}\")\n    \n    # Calculate total improvement\n    total_improvement = sum(h['skill_improvement'] for h in episode_history)\n    print(f\"Total skill improvement: {total_improvement:.3f}\")\n    \n    return env, agent, episode_history\n\n\n# =============================================================================\n# MAIN EXECUTION\n# =============================================================================\n\ndef main():\n    \"\"\"Main entry point for the employee training optimization system.\"\"\"\n    parser = argparse.ArgumentParser(description='Employee Training Optimization with RL')\n    parser.add_argument('--mode', choices=['train', 'evaluate', 'visualize', 'example'], \n                       default='train', help='Run mode')\n    parser.add_argument('--model', type=str, help='Path to saved model for evaluation')\n    parser.add_argument('--episodes', type=int, default=1000, help='Number of training episodes')\n    parser.add_argument('--no-baseline', action='store_true', help='Disable baseline (Actor-Critic)')\n    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n    \n    args = parser.parse_args()\n    \n    # Set random seeds for reproducibility\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    \n    # Create directories\n    create_directories()\n    \n    # Load configuration\n    config = get_default_config()\n    \n    # Override config with command line arguments\n    if args.episodes:\n        config['num_episodes'] = args.episodes\n    if args.no_baseline:\n        config['use_baseline'] = False\n    \n    print(\"Employee Training Optimization System\")\n    print(\"=\" * 50)\n    print(f\"Mode: {args.mode}\")\n    print(f\"Seed: {args.seed}\")\n    print(\"=\" * 50)\n    \n    if args.mode == 'example':\n        # Run examples\n        print(\"Running example demonstrations...\")\n        \n        # Basic training example\n        env1, agent1, loop1 = example_basic_training()\n        \n        # Skill analysis example\n        env2, agent2, history = example_skill_analysis()\n        \n        # Visualize results\n        viz = VisualizationUtils()\n        viz.plot_skill_progression(env1, agent1, num_episodes=3)\n        viz.plot_synergy_matrix(env1)\n        \n        print(\"\\n=== All Examples Completed Successfully! ===\")\n        \n    elif args.mode == 'train':\n        # Create environment\n        env = EmployeeTrainingEnv(\n            D=config['D'],\n            K=config['K'],\n            alpha=config['alpha'],\n            beta=config['beta'],\n            kappa=config['kappa'],\n            C_max=config['C_max'],\n            gamma=config['gamma']\n        )\n        \n        # Create agent\n        agent = REINFORCEAgent(\n            state_dim=env.D,\n            action_dim=env.K,\n            hidden_dim=config['hidden_dim'],\n            learning_rate=config['learning_rate'],\n            use_baseline=config['use_baseline']\n        )\n        \n        # Create training loop\n        training_loop = TrainingLoop(env, agent, config)\n        \n        print(f\"Environment: {env.D} skills, {env.K} training modules\")\n        print(f\"Agent: {'Actor-Critic' if config['use_baseline'] else 'REINFORCE'}\")\n        \n        # Training mode\n        print(\"\\nStarting training...\")\n        training_loop.train()\n        \n        # Plot training curves\n        training_loop.plot_training_curves(\n            save_path=os.path.join(config['plot_save_path'], 'training_curves.png')\n        )\n        \n        # Evaluate trained model\n        print(\"\\nEvaluating trained model...\")\n        eval_results = training_loop.evaluate(\n            num_episodes=config['eval_episodes'],\n            render=config['eval_render']\n        )\n        \n        # Visualize skill progression\n        viz = VisualizationUtils()\n        viz.plot_skill_progression(\n            env, agent, num_episodes=3,\n            save_path=os.path.join(config['plot_save_path'], 'skill_progression.png')\n        )\n        \n    elif args.mode == 'evaluate':\n        # Create environment and agent\n        env = EmployeeTrainingEnv(\n            D=config['D'],\n            K=config['K'],\n            alpha=config['alpha'],\n            beta=config['beta'],\n            kappa=config['kappa'],\n            C_max=config['C_max'],\n            gamma=config['gamma']\n        )\n        \n        agent = REINFORCEAgent(\n            state_dim=env.D,\n            action_dim=env.K,\n            hidden_dim=config['hidden_dim'],\n            learning_rate=config['learning_rate'],\n            use_baseline=config['use_baseline']\n        )\n        \n        # Load model if specified\n        if args.model:\n            agent.load_model(args.model)\n            print(f\"Loaded model from {args.model}\")\n        else:\n            print(\"No model specified for evaluation. Using randomly initialized policy.\")\n        \n        # Create training loop for evaluation\n        training_loop = TrainingLoop(env, agent, config)\n        \n        eval_results = training_loop.evaluate(\n            num_episodes=config['eval_episodes'],\n            render=True\n        )\n        \n        # Visualize skill progression\n        viz = VisualizationUtils()\n        viz.plot_skill_progression(env, agent, num_episodes=5)\n        \n    elif args.mode == 'visualize':\n        # Create environment and agent\n        env = EmployeeTrainingEnv(\n            D=config['D'],\n            K=config['K'],\n            alpha=config['alpha'],\n            beta=config['beta'],\n            kappa=config['kappa'],\n            C_max=config['C_max'],\n            gamma=config['gamma']\n        )\n        \n        agent = REINFORCEAgent(\n            state_dim=env.D,\n            action_dim=env.K,\n            hidden_dim=config['hidden_dim'],\n            learning_rate=config['learning_rate'],\n            use_baseline=config['use_baseline']\n        )\n        \n        # Load model if specified\n        if args.model:\n            agent.load_model(args.model)\n            print(f\"Loaded model from {args.model}\")\n        \n        print(\"\\nGenerating visualizations...\")\n        \n        # Create visualization utils\n        viz = VisualizationUtils()\n        \n        # Plot synergy matrix\n        viz.plot_synergy_matrix(\n            env, \n            save_path=os.path.join(config['plot_save_path'], 'synergy_matrix.png')\n        )\n        \n        # Plot skill progression\n        viz.plot_skill_progression(\n            env, agent, num_episodes=5,\n            save_path=os.path.join(config['plot_save_path'], 'skill_progression.png')\n        )\n        \n        print(\"Visualizations completed!\")\n    \n    print(\"\\nSystem execution completed.\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:44:06.796992Z","iopub.execute_input":"2025-07-09T14:44:06.797328Z","iopub.status.idle":"2025-07-09T14:44:06.819920Z","shell.execute_reply.started":"2025-07-09T14:44:06.797303Z","shell.execute_reply":"2025-07-09T14:44:06.819188Z"}},"outputs":[{"name":"stdout","text":"Overwriting single_file_rl_training.py\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!python single_file_rl_training.py --mode train --episodes 10000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:44:06.975744Z","iopub.execute_input":"2025-07-09T14:44:06.976422Z","iopub.status.idle":"2025-07-09T14:47:34.410496Z","shell.execute_reply.started":"2025-07-09T14:44:06.976397Z","shell.execute_reply":"2025-07-09T14:47:34.409738Z"}},"outputs":[{"name":"stdout","text":"Employee Training Optimization System\n==================================================\nMode: train\nSeed: 42\n==================================================\nEnvironment: 8 skills, 4 training modules\nAgent: Actor-Critic\n\nStarting training...\nStarting training...\nEpisodes: 10000\nEnvironment: 8 skills, 4 training modules\nAgent: with baseline\n--------------------------------------------------\nEpisode   50 | Reward:  -8.49 | Avg Reward:  -9.14 | Length:  9 | Cost:   0.0 | Skill Δ:  2.486 | Policy Loss: -1.425\nEpisode  100 | Reward:  -9.45 | Avg Reward:  -8.59 | Length:  9 | Cost:   0.0 | Skill Δ:  1.474 | Policy Loss: -4.817\nEpisode  150 | Reward:  -9.44 | Avg Reward:  -7.92 | Length: 10 | Cost:   0.0 | Skill Δ:  1.495 | Policy Loss:  0.645\nEpisode  200 | Reward:  -9.07 | Avg Reward:  -8.00 | Length: 10 | Cost:   0.0 | Skill Δ:  1.892 | Policy Loss:  0.642\nModel saved at episode 200\nEpisode  250 | Reward:   1.44 | Avg Reward:  -7.45 | Length:  9 | Cost: 100.0 | Skill Δ:  2.445 | Policy Loss: -1.820\nEpisode  300 | Reward:  -9.21 | Avg Reward:  -7.08 | Length: 10 | Cost:   0.0 | Skill Δ:  1.768 | Policy Loss:  3.520\nEpisode  350 | Reward:  -9.00 | Avg Reward:  -7.59 | Length: 10 | Cost:   0.0 | Skill Δ:  1.989 | Policy Loss: -2.918\nEpisode  400 | Reward:  -9.69 | Avg Reward:  -7.93 | Length: 10 | Cost:   0.0 | Skill Δ:  1.289 | Policy Loss:  5.943\nModel saved at episode 400\nEpisode  450 | Reward:  -9.65 | Avg Reward:  -7.97 | Length: 10 | Cost:   0.0 | Skill Δ:  1.329 | Policy Loss:  1.983\nEpisode  500 | Reward:  -9.21 | Avg Reward:  -7.14 | Length:  9 | Cost:   0.0 | Skill Δ:  1.713 | Policy Loss:  0.533\nEpisode  550 | Reward:  -9.13 | Avg Reward:  -7.30 | Length: 10 | Cost:   0.0 | Skill Δ:  1.790 | Policy Loss: -2.057\nEpisode  600 | Reward:   0.50 | Avg Reward:  -7.95 | Length:  9 | Cost: 100.0 | Skill Δ:  1.496 | Policy Loss:  1.159\nModel saved at episode 600\nEpisode  650 | Reward:  -8.77 | Avg Reward:  -7.18 | Length: 10 | Cost:   0.0 | Skill Δ:  2.214 | Policy Loss: -1.596\nEpisode  700 | Reward:   0.82 | Avg Reward:  -7.48 | Length:  9 | Cost: 100.0 | Skill Δ:  1.818 | Policy Loss: -1.162\nEpisode  750 | Reward:  -9.11 | Avg Reward:  -7.83 | Length: 10 | Cost:   0.0 | Skill Δ:  1.865 | Policy Loss: -0.494\nEpisode  800 | Reward:  -9.43 | Avg Reward:  -6.99 | Length: 10 | Cost:   0.0 | Skill Δ:  1.534 | Policy Loss:  3.537\nModel saved at episode 800\nEpisode  850 | Reward:   1.25 | Avg Reward:  -7.14 | Length:  9 | Cost: 100.0 | Skill Δ:  2.249 | Policy Loss: -1.424\nEpisode  900 | Reward:  -9.21 | Avg Reward:  -8.06 | Length: 10 | Cost:   0.0 | Skill Δ:  1.752 | Policy Loss:  2.104\nEpisode  950 | Reward:   1.08 | Avg Reward:  -7.42 | Length:  9 | Cost: 100.0 | Skill Δ:  2.080 | Policy Loss: -1.345\nEpisode 1000 | Reward:  -9.24 | Avg Reward:  -6.08 | Length: 10 | Cost:   0.0 | Skill Δ:  1.700 | Policy Loss: -0.370\nModel saved at episode 1000\nEpisode 1050 | Reward:  -8.85 | Avg Reward:  -5.71 | Length: 10 | Cost:   0.0 | Skill Δ:  2.125 | Policy Loss: -0.940\nEpisode 1100 | Reward:  -8.99 | Avg Reward:  -6.54 | Length: 10 | Cost:   0.0 | Skill Δ:  1.986 | Policy Loss: -1.407\nEpisode 1150 | Reward:  -8.97 | Avg Reward:  -8.12 | Length: 10 | Cost:   0.0 | Skill Δ:  2.008 | Policy Loss:  0.046\nEpisode 1200 | Reward:  -9.06 | Avg Reward:  -7.74 | Length:  9 | Cost:   0.0 | Skill Δ:  1.863 | Policy Loss: -0.676\nModel saved at episode 1200\nEpisode 1250 | Reward:   0.80 | Avg Reward:  -6.12 | Length:  9 | Cost: 100.0 | Skill Δ:  1.798 | Policy Loss: -0.258\nEpisode 1300 | Reward:   1.31 | Avg Reward:  -6.30 | Length:  9 | Cost: 100.0 | Skill Δ:  2.314 | Policy Loss: -0.997\nEpisode 1350 | Reward:   1.52 | Avg Reward:  -6.31 | Length:  9 | Cost: 100.0 | Skill Δ:  2.515 | Policy Loss: -2.953\nEpisode 1400 | Reward:   0.31 | Avg Reward:  -5.03 | Length:  9 | Cost: 100.0 | Skill Δ:  1.313 | Policy Loss:  1.311\nModel saved at episode 1400\nEpisode 1450 | Reward:  -8.77 | Avg Reward:  -5.23 | Length: 10 | Cost:   0.0 | Skill Δ:  2.188 | Policy Loss: -0.160\nEpisode 1500 | Reward:  -9.62 | Avg Reward:  -7.43 | Length: 10 | Cost:   0.0 | Skill Δ:  1.317 | Policy Loss:  1.728\nEpisode 1550 | Reward:  -9.44 | Avg Reward:  -7.36 | Length:  9 | Cost:   0.0 | Skill Δ:  1.482 | Policy Loss:  1.155\nEpisode 1600 | Reward:  -9.24 | Avg Reward:  -5.88 | Length: 10 | Cost:   0.0 | Skill Δ:  1.743 | Policy Loss:  0.656\nModel saved at episode 1600\nEpisode 1650 | Reward:  -9.10 | Avg Reward:  -6.43 | Length: 10 | Cost:   0.0 | Skill Δ:  1.879 | Policy Loss: -1.924\nEpisode 1700 | Reward:  -8.94 | Avg Reward:  -7.61 | Length: 10 | Cost:   0.0 | Skill Δ:  2.021 | Policy Loss: -0.347\nEpisode 1750 | Reward:  -9.81 | Avg Reward:  -7.67 | Length: 10 | Cost:   0.0 | Skill Δ:  1.149 | Policy Loss:  3.538\nEpisode 1800 | Reward:  -9.27 | Avg Reward:  -7.67 | Length: 10 | Cost:   0.0 | Skill Δ:  1.708 | Policy Loss:  0.846\nModel saved at episode 1800\nEpisode 1850 | Reward:  -9.12 | Avg Reward:  -7.28 | Length:  9 | Cost:   0.0 | Skill Δ:  1.778 | Policy Loss:  0.289\nEpisode 1900 | Reward:  -8.75 | Avg Reward:  -5.28 | Length:  9 | Cost:   0.0 | Skill Δ:  2.155 | Policy Loss: -0.962\nEpisode 1950 | Reward:   0.92 | Avg Reward:  -3.96 | Length:  9 | Cost: 100.0 | Skill Δ:  1.924 | Policy Loss: -0.024\nEpisode 2000 | Reward:  -9.20 | Avg Reward:  -4.64 | Length: 10 | Cost:   0.0 | Skill Δ:  1.778 | Policy Loss:  0.948\nModel saved at episode 2000\nEpisode 2050 | Reward:  -8.72 | Avg Reward:  -5.28 | Length: 10 | Cost:   0.0 | Skill Δ:  2.262 | Policy Loss: -0.373\nEpisode 2100 | Reward:  -8.71 | Avg Reward:  -5.67 | Length: 10 | Cost:   0.0 | Skill Δ:  2.252 | Policy Loss: -0.614\nEpisode 2150 | Reward:  -8.96 | Avg Reward:  -5.97 | Length: 10 | Cost:   0.0 | Skill Δ:  2.016 | Policy Loss: -0.199\nEpisode 2200 | Reward:  -9.01 | Avg Reward:  -7.30 | Length: 10 | Cost:   0.0 | Skill Δ:  1.955 | Policy Loss: -0.063\nModel saved at episode 2200\nEpisode 2250 | Reward:   1.26 | Avg Reward:  -8.78 | Length:  9 | Cost: 100.0 | Skill Δ:  2.263 | Policy Loss: -1.446\nEpisode 2300 | Reward:  -9.94 | Avg Reward:  -6.88 | Length:  9 | Cost:   0.0 | Skill Δ:  0.962 | Policy Loss:  5.873\nEpisode 2350 | Reward:  -8.83 | Avg Reward:  -4.99 | Length: 10 | Cost:   0.0 | Skill Δ:  2.146 | Policy Loss:  0.521\nEpisode 2400 | Reward:  -8.79 | Avg Reward:  -6.25 | Length: 10 | Cost:   0.0 | Skill Δ:  2.173 | Policy Loss:  0.357\nModel saved at episode 2400\nEpisode 2450 | Reward:  -8.63 | Avg Reward:  -8.39 | Length: 10 | Cost:   0.0 | Skill Δ:  2.328 | Policy Loss: -1.457\nEpisode 2500 | Reward:   0.95 | Avg Reward:  -6.93 | Length:  9 | Cost: 100.0 | Skill Δ:  1.952 | Policy Loss: -0.041\nEpisode 2550 | Reward:  -8.63 | Avg Reward:  -4.79 | Length: 10 | Cost:   0.0 | Skill Δ:  2.345 | Policy Loss: -0.814\nEpisode 2600 | Reward:  -9.24 | Avg Reward:  -6.42 | Length: 10 | Cost:   0.0 | Skill Δ:  1.722 | Policy Loss: -0.634\nModel saved at episode 2600\nEpisode 2650 | Reward:  -8.68 | Avg Reward:  -8.47 | Length: 10 | Cost:   0.0 | Skill Δ:  2.301 | Policy Loss: -1.308\nEpisode 2700 | Reward:  -9.45 | Avg Reward:  -6.95 | Length: 10 | Cost:   0.0 | Skill Δ:  1.534 | Policy Loss:  3.134\nEpisode 2750 | Reward:  -8.97 | Avg Reward:  -5.12 | Length: 10 | Cost:   0.0 | Skill Δ:  2.005 | Policy Loss:  0.308\nEpisode 2800 | Reward:  -9.38 | Avg Reward:  -4.91 | Length: 10 | Cost:   0.0 | Skill Δ:  1.598 | Policy Loss:  0.911\nModel saved at episode 2800\nEpisode 2850 | Reward:  -8.80 | Avg Reward:  -4.99 | Length: 10 | Cost:   0.0 | Skill Δ:  2.183 | Policy Loss: -0.055\nEpisode 2900 | Reward:  -9.13 | Avg Reward:  -6.36 | Length: 10 | Cost:   0.0 | Skill Δ:  1.833 | Policy Loss: -0.537\nEpisode 2950 | Reward:  -9.17 | Avg Reward:  -7.76 | Length: 10 | Cost:   0.0 | Skill Δ:  1.810 | Policy Loss:  1.792\nEpisode 3000 | Reward:   1.38 | Avg Reward:  -6.26 | Length:  9 | Cost: 100.0 | Skill Δ:  2.384 | Policy Loss: -0.648\nModel saved at episode 3000\nEpisode 3050 | Reward:   0.96 | Avg Reward:  -3.64 | Length:  9 | Cost: 100.0 | Skill Δ:  1.963 | Policy Loss: -0.598\nEpisode 3100 | Reward:  -9.02 | Avg Reward:  -3.15 | Length:  9 | Cost:   0.0 | Skill Δ:  1.883 | Policy Loss:  0.629\nEpisode 3150 | Reward:   0.96 | Avg Reward:  -4.61 | Length:  9 | Cost: 100.0 | Skill Δ:  1.959 | Policy Loss: -0.209\nEpisode 3200 | Reward:  -9.35 | Avg Reward:  -5.23 | Length: 10 | Cost:   0.0 | Skill Δ:  1.609 | Policy Loss:  0.510\nModel saved at episode 3200\nEpisode 3250 | Reward:  -8.91 | Avg Reward:  -6.92 | Length: 10 | Cost:   0.0 | Skill Δ:  2.030 | Policy Loss: -0.333\nEpisode 3300 | Reward:  -8.81 | Avg Reward:  -7.50 | Length: 10 | Cost:   0.0 | Skill Δ:  2.169 | Policy Loss: -0.969\nEpisode 3350 | Reward:   1.13 | Avg Reward:  -5.26 | Length:  9 | Cost: 100.0 | Skill Δ:  2.127 | Policy Loss: -0.447\nEpisode 3400 | Reward:  -9.11 | Avg Reward:  -5.22 | Length: 10 | Cost:   0.0 | Skill Δ:  1.872 | Policy Loss:  0.161\nModel saved at episode 3400\nEpisode 3450 | Reward:  -9.20 | Avg Reward:  -6.59 | Length:  9 | Cost:   0.0 | Skill Δ:  1.740 | Policy Loss:  0.254\nEpisode 3500 | Reward:  -9.51 | Avg Reward:  -7.65 | Length:  9 | Cost:   0.0 | Skill Δ:  1.426 | Policy Loss:  0.583\nEpisode 3550 | Reward:  -9.20 | Avg Reward:  -7.39 | Length: 10 | Cost:   0.0 | Skill Δ:  1.782 | Policy Loss:  1.131\nEpisode 3600 | Reward:  -9.31 | Avg Reward:  -7.09 | Length: 10 | Cost:   0.0 | Skill Δ:  1.652 | Policy Loss:  0.228\nModel saved at episode 3600\nEpisode 3650 | Reward:  -9.07 | Avg Reward:  -7.87 | Length: 10 | Cost:   0.0 | Skill Δ:  1.894 | Policy Loss:  0.962\nEpisode 3700 | Reward:  -9.07 | Avg Reward:  -8.36 | Length: 10 | Cost:   0.0 | Skill Δ:  1.892 | Policy Loss: -0.155\nEpisode 3750 | Reward:  -9.03 | Avg Reward:  -7.99 | Length: 10 | Cost:   0.0 | Skill Δ:  1.933 | Policy Loss:  0.461\nEpisode 3800 | Reward:  -9.05 | Avg Reward:  -7.49 | Length:  9 | Cost:   0.0 | Skill Δ:  1.867 | Policy Loss: -0.358\nModel saved at episode 3800\nEpisode 3850 | Reward:  -8.71 | Avg Reward:  -8.24 | Length:  9 | Cost:   0.0 | Skill Δ:  2.193 | Policy Loss: -0.314\nEpisode 3900 | Reward:   1.12 | Avg Reward:  -6.44 | Length:  9 | Cost: 100.0 | Skill Δ:  2.115 | Policy Loss: -0.331\nEpisode 3950 | Reward:  -9.14 | Avg Reward:  -5.17 | Length: 10 | Cost:   0.0 | Skill Δ:  1.838 | Policy Loss: -0.154\nEpisode 4000 | Reward:  -8.76 | Avg Reward:  -7.75 | Length: 10 | Cost:   0.0 | Skill Δ:  2.222 | Policy Loss: -0.798\nModel saved at episode 4000\nEpisode 4050 | Reward:  -9.38 | Avg Reward:  -8.06 | Length: 10 | Cost:   0.0 | Skill Δ:  1.580 | Policy Loss:  1.430\nEpisode 4100 | Reward:  -8.90 | Avg Reward:  -5.81 | Length: 10 | Cost:   0.0 | Skill Δ:  2.080 | Policy Loss:  0.224\nEpisode 4150 | Reward:  -8.88 | Avg Reward:  -5.11 | Length: 10 | Cost:   0.0 | Skill Δ:  2.104 | Policy Loss: -0.630\nEpisode 4200 | Reward:   1.46 | Avg Reward:  -6.00 | Length:  9 | Cost: 100.0 | Skill Δ:  2.457 | Policy Loss: -1.219\nModel saved at episode 4200\nEpisode 4250 | Reward:   1.09 | Avg Reward:  -4.59 | Length:  9 | Cost: 100.0 | Skill Δ:  2.091 | Policy Loss: -0.557\nEpisode 4300 | Reward:   0.94 | Avg Reward:  -3.86 | Length:  9 | Cost: 100.0 | Skill Δ:  1.940 | Policy Loss: -0.843\nEpisode 4350 | Reward:   0.79 | Avg Reward:  -5.10 | Length:  9 | Cost: 100.0 | Skill Δ:  1.789 | Policy Loss: -0.436\nEpisode 4400 | Reward:   1.04 | Avg Reward:  -4.39 | Length:  9 | Cost: 100.0 | Skill Δ:  2.044 | Policy Loss: -0.645\nModel saved at episode 4400\nEpisode 4450 | Reward:  -9.43 | Avg Reward:  -5.08 | Length: 10 | Cost:   0.0 | Skill Δ:  1.554 | Policy Loss:  0.919\nEpisode 4500 | Reward:   0.76 | Avg Reward:  -6.42 | Length:  9 | Cost: 100.0 | Skill Δ:  1.764 | Policy Loss: -0.023\nEpisode 4550 | Reward:   0.39 | Avg Reward:  -4.60 | Length:  9 | Cost: 100.0 | Skill Δ:  1.392 | Policy Loss:  1.204\nEpisode 4600 | Reward:  -9.43 | Avg Reward:  -3.59 | Length: 10 | Cost:   0.0 | Skill Δ:  1.547 | Policy Loss:  2.171\nModel saved at episode 4600\nEpisode 4650 | Reward:   0.90 | Avg Reward:  -3.92 | Length:  9 | Cost: 100.0 | Skill Δ:  1.904 | Policy Loss: -0.334\nEpisode 4700 | Reward:  -9.13 | Avg Reward:  -3.59 | Length: 10 | Cost:   0.0 | Skill Δ:  1.846 | Policy Loss:  0.851\nEpisode 4750 | Reward:  -9.25 | Avg Reward:  -5.98 | Length: 10 | Cost:   0.0 | Skill Δ:  1.710 | Policy Loss:  1.036\nEpisode 4800 | Reward:  -8.82 | Avg Reward:  -8.18 | Length: 10 | Cost:   0.0 | Skill Δ:  2.163 | Policy Loss: -0.562\nModel saved at episode 4800\nEpisode 4850 | Reward:  -8.84 | Avg Reward:  -6.30 | Length:  9 | Cost:   0.0 | Skill Δ:  2.064 | Policy Loss: -0.241\nEpisode 4900 | Reward:  -8.76 | Avg Reward:  -5.74 | Length: 10 | Cost:   0.0 | Skill Δ:  2.218 | Policy Loss: -0.184\nEpisode 4950 | Reward:   0.87 | Avg Reward:  -6.43 | Length:  9 | Cost: 100.0 | Skill Δ:  1.868 | Policy Loss: -2.253\nEpisode 5000 | Reward:   0.98 | Avg Reward:  -5.01 | Length:  9 | Cost: 100.0 | Skill Δ:  1.981 | Policy Loss:  0.003\nModel saved at episode 5000\nEpisode 5050 | Reward:  -9.36 | Avg Reward:  -5.29 | Length: 10 | Cost:   0.0 | Skill Δ:  1.582 | Policy Loss:  0.467\nEpisode 5100 | Reward:  -9.86 | Avg Reward:  -7.21 | Length: 10 | Cost:   0.0 | Skill Δ:  1.115 | Policy Loss:  1.324\nEpisode 5150 | Reward:  -8.59 | Avg Reward:  -5.91 | Length: 10 | Cost:   0.0 | Skill Δ:  2.393 | Policy Loss: -0.474\nEpisode 5200 | Reward:  -9.33 | Avg Reward:  -5.58 | Length: 10 | Cost:   0.0 | Skill Δ:  1.652 | Policy Loss:  0.513\nModel saved at episode 5200\nEpisode 5250 | Reward:  -8.64 | Avg Reward:  -7.47 | Length: 10 | Cost:   0.0 | Skill Δ:  2.325 | Policy Loss: -0.837\nEpisode 5300 | Reward:  -9.17 | Avg Reward:  -8.26 | Length: 10 | Cost:   0.0 | Skill Δ:  1.807 | Policy Loss:  1.470\nEpisode 5350 | Reward:   1.12 | Avg Reward:  -6.38 | Length:  9 | Cost: 100.0 | Skill Δ:  2.117 | Policy Loss: -0.666\nEpisode 5400 | Reward:  -8.84 | Avg Reward:  -3.48 | Length: 10 | Cost:   0.0 | Skill Δ:  2.137 | Policy Loss: -0.420\nModel saved at episode 5400\nEpisode 5450 | Reward:  -8.89 | Avg Reward:  -3.08 | Length: 10 | Cost:   0.0 | Skill Δ:  2.089 | Policy Loss:  0.060\nEpisode 5500 | Reward:   0.58 | Avg Reward:  -5.58 | Length:  9 | Cost: 100.0 | Skill Δ:  1.580 | Policy Loss:  0.814\nEpisode 5550 | Reward:  -9.40 | Avg Reward:  -7.72 | Length: 10 | Cost:   0.0 | Skill Δ:  1.559 | Policy Loss:  0.741\nEpisode 5600 | Reward:   0.68 | Avg Reward:  -6.74 | Length:  9 | Cost: 100.0 | Skill Δ:  1.681 | Policy Loss:  0.616\nModel saved at episode 5600\nEpisode 5650 | Reward:   0.37 | Avg Reward:  -4.12 | Length:  9 | Cost: 100.0 | Skill Δ:  1.373 | Policy Loss:  3.010\nEpisode 5700 | Reward:   0.56 | Avg Reward:  -3.52 | Length:  9 | Cost: 100.0 | Skill Δ:  1.559 | Policy Loss:  0.094\nEpisode 5750 | Reward:  -9.67 | Avg Reward:  -4.44 | Length:  9 | Cost:   0.0 | Skill Δ:  1.231 | Policy Loss:  0.152\nEpisode 5800 | Reward:  -8.75 | Avg Reward:  -3.43 | Length:  9 | Cost:   0.0 | Skill Δ:  2.152 | Policy Loss: -0.173\nModel saved at episode 5800\nEpisode 5850 | Reward:  -8.85 | Avg Reward:  -4.27 | Length: 10 | Cost:   0.0 | Skill Δ:  2.132 | Policy Loss: -0.864\nEpisode 5900 | Reward:   0.78 | Avg Reward:  -5.37 | Length:  9 | Cost: 100.0 | Skill Δ:  1.780 | Policy Loss:  0.094\nEpisode 5950 | Reward:  -8.96 | Avg Reward:  -5.27 | Length: 10 | Cost:   0.0 | Skill Δ:  2.024 | Policy Loss: -0.254\nEpisode 6000 | Reward:   1.07 | Avg Reward:  -5.99 | Length:  9 | Cost: 100.0 | Skill Δ:  2.066 | Policy Loss: -0.051\nModel saved at episode 6000\nEpisode 6050 | Reward:   1.17 | Avg Reward:  -4.38 | Length:  9 | Cost: 100.0 | Skill Δ:  2.169 | Policy Loss: -0.032\nEpisode 6100 | Reward:  -9.07 | Avg Reward:  -3.57 | Length: 10 | Cost:   0.0 | Skill Δ:  1.906 | Policy Loss:  0.391\nEpisode 6150 | Reward:  -9.75 | Avg Reward:  -6.22 | Length: 10 | Cost:   0.0 | Skill Δ:  1.211 | Policy Loss:  0.119\nEpisode 6200 | Reward:  -8.73 | Avg Reward:  -8.03 | Length: 10 | Cost:   0.0 | Skill Δ:  2.229 | Policy Loss: -0.069\nModel saved at episode 6200\nEpisode 6250 | Reward:  -8.93 | Avg Reward:  -7.60 | Length: 10 | Cost:   0.0 | Skill Δ:  2.052 | Policy Loss:  0.289\nEpisode 6300 | Reward:   0.71 | Avg Reward:  -7.28 | Length:  9 | Cost: 100.0 | Skill Δ:  1.711 | Policy Loss:  0.711\nEpisode 6350 | Reward:   1.20 | Avg Reward:  -5.19 | Length:  9 | Cost: 100.0 | Skill Δ:  2.203 | Policy Loss: -0.169\nEpisode 6400 | Reward:   0.88 | Avg Reward:  -3.40 | Length:  9 | Cost: 100.0 | Skill Δ:  1.880 | Policy Loss: -0.115\nModel saved at episode 6400\nEpisode 6450 | Reward:  -8.62 | Avg Reward:  -3.89 | Length: 10 | Cost:   0.0 | Skill Δ:  2.361 | Policy Loss: -0.507\nEpisode 6500 | Reward:   1.08 | Avg Reward:  -3.99 | Length:  9 | Cost: 100.0 | Skill Δ:  2.081 | Policy Loss:  0.220\nEpisode 6550 | Reward:  -9.24 | Avg Reward:  -3.99 | Length:  9 | Cost:   0.0 | Skill Δ:  1.678 | Policy Loss:  0.043\nEpisode 6600 | Reward:  -9.52 | Avg Reward:  -4.14 | Length: 10 | Cost:   0.0 | Skill Δ:  1.460 | Policy Loss:  0.656\nModel saved at episode 6600\nEpisode 6650 | Reward:   1.03 | Avg Reward:  -4.03 | Length:  9 | Cost: 100.0 | Skill Δ:  2.030 | Policy Loss: -0.605\nEpisode 6700 | Reward:  -8.81 | Avg Reward:  -4.38 | Length: 10 | Cost:   0.0 | Skill Δ:  2.165 | Policy Loss: -0.156\nEpisode 6750 | Reward:   0.94 | Avg Reward:  -4.33 | Length:  9 | Cost: 100.0 | Skill Δ:  1.937 | Policy Loss:  0.325\nEpisode 6800 | Reward:  -8.84 | Avg Reward:  -4.61 | Length: 10 | Cost:   0.0 | Skill Δ:  2.140 | Policy Loss: -0.306\nModel saved at episode 6800\nEpisode 6850 | Reward:  -9.12 | Avg Reward:  -5.61 | Length:  9 | Cost:   0.0 | Skill Δ:  1.775 | Policy Loss: -0.025\nEpisode 6900 | Reward:  -9.29 | Avg Reward:  -4.11 | Length: 10 | Cost:   0.0 | Skill Δ:  1.689 | Policy Loss: -0.086\nEpisode 6950 | Reward:   1.47 | Avg Reward:  -4.05 | Length:  9 | Cost: 100.0 | Skill Δ:  2.468 | Policy Loss:  0.048\nEpisode 7000 | Reward:  -8.85 | Avg Reward:  -4.39 | Length:  9 | Cost:   0.0 | Skill Δ:  2.051 | Policy Loss: -1.844\nModel saved at episode 7000\nEpisode 7050 | Reward:   1.59 | Avg Reward:  -3.11 | Length:  9 | Cost: 100.0 | Skill Δ:  2.586 | Policy Loss: -0.783\nEpisode 7100 | Reward:   1.37 | Avg Reward:  -4.59 | Length:  9 | Cost: 100.0 | Skill Δ:  2.374 | Policy Loss: -0.240\nEpisode 7150 | Reward:  -9.34 | Avg Reward:  -7.36 | Length: 10 | Cost:   0.0 | Skill Δ:  1.642 | Policy Loss:  0.675\nEpisode 7200 | Reward:   0.27 | Avg Reward:  -6.48 | Length:  9 | Cost: 100.0 | Skill Δ:  1.266 | Policy Loss:  0.537\nModel saved at episode 7200\nEpisode 7250 | Reward:  -9.14 | Avg Reward:  -5.68 | Length: 10 | Cost:   0.0 | Skill Δ:  1.824 | Policy Loss:  0.274\nEpisode 7300 | Reward:  -9.48 | Avg Reward:  -6.88 | Length: 10 | Cost:   0.0 | Skill Δ:  1.500 | Policy Loss:  0.498\nEpisode 7350 | Reward:  -9.36 | Avg Reward:  -7.62 | Length: 10 | Cost:   0.0 | Skill Δ:  1.603 | Policy Loss:  0.626\nEpisode 7400 | Reward:  -8.91 | Avg Reward:  -7.11 | Length: 10 | Cost:   0.0 | Skill Δ:  2.051 | Policy Loss: -0.155\nModel saved at episode 7400\nEpisode 7450 | Reward:  -8.74 | Avg Reward:  -7.51 | Length: 10 | Cost:   0.0 | Skill Δ:  2.217 | Policy Loss: -0.977\nEpisode 7500 | Reward:  -9.43 | Avg Reward:  -8.92 | Length: 10 | Cost:   0.0 | Skill Δ:  1.553 | Policy Loss:  1.522\nEpisode 7550 | Reward:   0.64 | Avg Reward:  -6.89 | Length:  9 | Cost: 100.0 | Skill Δ:  1.644 | Policy Loss:  0.252\nEpisode 7600 | Reward:  -9.24 | Avg Reward:  -4.36 | Length: 10 | Cost:   0.0 | Skill Δ:  1.744 | Policy Loss:  0.196\nModel saved at episode 7600\nEpisode 7650 | Reward:  -9.15 | Avg Reward:  -3.40 | Length: 10 | Cost:   0.0 | Skill Δ:  1.835 | Policy Loss: -0.370\nEpisode 7700 | Reward:  -8.67 | Avg Reward:  -3.69 | Length: 10 | Cost:   0.0 | Skill Δ:  2.314 | Policy Loss: -0.196\nEpisode 7750 | Reward:  -9.16 | Avg Reward:  -5.39 | Length: 10 | Cost:   0.0 | Skill Δ:  1.819 | Policy Loss:  0.427\nEpisode 7800 | Reward:  -9.61 | Avg Reward:  -6.95 | Length: 10 | Cost:   0.0 | Skill Δ:  1.353 | Policy Loss:  1.146\nModel saved at episode 7800\nEpisode 7850 | Reward:  -9.22 | Avg Reward:  -7.60 | Length: 10 | Cost:   0.0 | Skill Δ:  1.755 | Policy Loss:  0.043\nEpisode 7900 | Reward:  -9.04 | Avg Reward:  -7.01 | Length: 10 | Cost:   0.0 | Skill Δ:  1.940 | Policy Loss:  0.738\nEpisode 7950 | Reward:   1.18 | Avg Reward:  -4.42 | Length:  9 | Cost: 100.0 | Skill Δ:  2.180 | Policy Loss: -0.112\nEpisode 8000 | Reward:  -9.37 | Avg Reward:  -4.39 | Length: 10 | Cost:   0.0 | Skill Δ:  1.566 | Policy Loss: -0.340\nModel saved at episode 8000\nEpisode 8050 | Reward:  -8.70 | Avg Reward:  -7.65 | Length: 10 | Cost:   0.0 | Skill Δ:  2.261 | Policy Loss: -0.873\nEpisode 8100 | Reward:  -9.20 | Avg Reward:  -8.81 | Length: 10 | Cost:   0.0 | Skill Δ:  1.777 | Policy Loss:  0.948\nEpisode 8150 | Reward:  -9.27 | Avg Reward:  -7.98 | Length: 10 | Cost:   0.0 | Skill Δ:  1.708 | Policy Loss:  0.406\nEpisode 8200 | Reward:  -9.20 | Avg Reward:  -8.25 | Length: 10 | Cost:   0.0 | Skill Δ:  1.763 | Policy Loss: -0.159\nModel saved at episode 8200\nEpisode 8250 | Reward:  -9.24 | Avg Reward:  -9.07 | Length: 10 | Cost:   0.0 | Skill Δ:  1.741 | Policy Loss:  0.074\nEpisode 8300 | Reward:  -9.31 | Avg Reward:  -8.05 | Length: 10 | Cost:   0.0 | Skill Δ:  1.669 | Policy Loss:  0.115\nEpisode 8350 | Reward:   0.69 | Avg Reward:  -6.01 | Length:  9 | Cost: 100.0 | Skill Δ:  1.688 | Policy Loss:  0.069\nEpisode 8400 | Reward:   0.95 | Avg Reward:  -3.96 | Length:  9 | Cost: 100.0 | Skill Δ:  1.948 | Policy Loss:  0.007\nModel saved at episode 8400\nEpisode 8450 | Reward:   0.55 | Avg Reward:  -4.59 | Length:  9 | Cost: 100.0 | Skill Δ:  1.551 | Policy Loss:  2.405\nEpisode 8500 | Reward:   0.98 | Avg Reward:  -4.33 | Length:  9 | Cost: 100.0 | Skill Δ:  1.976 | Policy Loss: -0.415\nEpisode 8550 | Reward:  -9.03 | Avg Reward:  -3.69 | Length: 10 | Cost:   0.0 | Skill Δ:  1.952 | Policy Loss: -0.155\nEpisode 8600 | Reward:  -9.40 | Avg Reward:  -5.10 | Length: 10 | Cost:   0.0 | Skill Δ:  1.576 | Policy Loss:  0.363\nModel saved at episode 8600\nEpisode 8650 | Reward:  -9.08 | Avg Reward:  -5.22 | Length: 10 | Cost:   0.0 | Skill Δ:  1.899 | Policy Loss: -0.080\nEpisode 8700 | Reward:  -9.11 | Avg Reward:  -6.26 | Length: 10 | Cost:   0.0 | Skill Δ:  1.868 | Policy Loss:  0.006\nEpisode 8750 | Reward:   0.70 | Avg Reward:  -6.46 | Length:  9 | Cost: 100.0 | Skill Δ:  1.701 | Policy Loss:  0.348\nEpisode 8800 | Reward:  -9.10 | Avg Reward:  -5.22 | Length: 10 | Cost:   0.0 | Skill Δ:  1.878 | Policy Loss:  0.087\nModel saved at episode 8800\nEpisode 8850 | Reward:  -8.94 | Avg Reward:  -6.23 | Length: 10 | Cost:   0.0 | Skill Δ:  2.023 | Policy Loss: -0.916\nEpisode 8900 | Reward:  -9.15 | Avg Reward:  -8.32 | Length: 10 | Cost:   0.0 | Skill Δ:  1.810 | Policy Loss:  0.102\nEpisode 8950 | Reward:  -8.96 | Avg Reward:  -9.04 | Length: 10 | Cost:   0.0 | Skill Δ:  2.002 | Policy Loss: -0.718\nEpisode 9000 | Reward:   1.30 | Avg Reward:  -7.30 | Length:  9 | Cost: 100.0 | Skill Δ:  2.303 | Policy Loss: -0.629\nModel saved at episode 9000\nEpisode 9050 | Reward:   0.72 | Avg Reward:  -3.86 | Length:  9 | Cost: 100.0 | Skill Δ:  1.721 | Policy Loss:  0.151\nEpisode 9100 | Reward:  -9.60 | Avg Reward:  -3.29 | Length: 10 | Cost:   0.0 | Skill Δ:  1.381 | Policy Loss:  0.109\nEpisode 9150 | Reward:   1.46 | Avg Reward:  -4.69 | Length:  9 | Cost: 100.0 | Skill Δ:  2.460 | Policy Loss: -0.110\nEpisode 9200 | Reward:   0.44 | Avg Reward:  -5.60 | Length:  9 | Cost: 100.0 | Skill Δ:  1.438 | Policy Loss:  1.598\nModel saved at episode 9200\nEpisode 9250 | Reward:  -9.17 | Avg Reward:  -5.50 | Length:  9 | Cost:   0.0 | Skill Δ:  1.730 | Policy Loss:  0.038\nEpisode 9300 | Reward:  -9.20 | Avg Reward:  -4.22 | Length: 10 | Cost:   0.0 | Skill Δ:  1.784 | Policy Loss:  0.831\nEpisode 9350 | Reward:   0.42 | Avg Reward:  -3.91 | Length:  9 | Cost: 100.0 | Skill Δ:  1.419 | Policy Loss:  1.049\nEpisode 9400 | Reward:  -9.23 | Avg Reward:  -3.39 | Length: 10 | Cost:   0.0 | Skill Δ:  1.746 | Policy Loss: -0.232\nModel saved at episode 9400\nEpisode 9450 | Reward:   0.48 | Avg Reward:  -2.96 | Length:  9 | Cost: 100.0 | Skill Δ:  1.480 | Policy Loss:  0.686\nEpisode 9500 | Reward:  -9.12 | Avg Reward:  -5.07 | Length:  9 | Cost:   0.0 | Skill Δ:  1.780 | Policy Loss:  0.298\nEpisode 9550 | Reward:  -8.31 | Avg Reward:  -5.82 | Length: 10 | Cost:   0.0 | Skill Δ:  2.668 | Policy Loss: -0.624\nEpisode 9600 | Reward:  -8.81 | Avg Reward:  -4.79 | Length: 10 | Cost:   0.0 | Skill Δ:  2.169 | Policy Loss: -0.053\nModel saved at episode 9600\nEpisode 9650 | Reward:  -9.23 | Avg Reward:  -4.47 | Length: 10 | Cost:   0.0 | Skill Δ:  1.748 | Policy Loss:  0.128\nEpisode 9700 | Reward:  -9.16 | Avg Reward:  -4.90 | Length:  9 | Cost:   0.0 | Skill Δ:  1.745 | Policy Loss:  0.318\nEpisode 9750 | Reward:  -9.01 | Avg Reward:  -5.51 | Length: 10 | Cost:   0.0 | Skill Δ:  1.975 | Policy Loss:  0.041\nEpisode 9800 | Reward:   0.78 | Avg Reward:  -5.60 | Length:  9 | Cost: 100.0 | Skill Δ:  1.781 | Policy Loss:  0.138\nModel saved at episode 9800\nEpisode 9850 | Reward:   1.08 | Avg Reward:  -4.76 | Length:  9 | Cost: 100.0 | Skill Δ:  2.080 | Policy Loss: -0.202\nEpisode 9900 | Reward:   1.19 | Avg Reward:  -3.15 | Length:  9 | Cost: 100.0 | Skill Δ:  2.187 | Policy Loss: -0.584\nEpisode 9950 | Reward:  -9.49 | Avg Reward:  -3.56 | Length: 10 | Cost:   0.0 | Skill Δ:  1.488 | Policy Loss:  0.328\nEpisode 10000 | Reward:  -9.49 | Avg Reward:  -5.89 | Length: 10 | Cost:   0.0 | Skill Δ:  1.485 | Policy Loss:  0.278\nModel saved at episode 10000\n\nTraining completed in 192.51 seconds\nAverage reward (last 100 episodes): -5.885\nAverage length (last 100 episodes): 9.56\nTraining curves saved to plots/training_curves.png\nFigure(1500x1000)\n\nEvaluating trained model...\n\nEvaluating policy over 100 episodes...\nEvaluation Results:\n  Mean Reward: -7.786 ± 3.337\n  Mean Length: 9.86\n  Mean Cost: 13.00\n  Mean Skill Improvement: 1.893\n  Success Rate: 13.00%\nSkill progression plot saved to plots/skill_progression.png\nFigure(1500x1200)\n\nSystem execution completed.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}