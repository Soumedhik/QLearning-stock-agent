{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c2a943",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Stock Trading in Data-Scarce Environments\n",
    "\n",
    "This notebook implements a reinforcement learning (RL) solution for stock trading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define the stock ticker and the date range\n",
    "STOCK_TICKER = 'AAPL'\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2023-01-01'\n",
    "\n",
    "# Fetch historical data\n",
    "df = yf.download(STOCK_TICKER, start=START_DATE, end=END_DATE)\n",
    "\n",
    "# Calculate daily returns and price changes\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in df.columns else \"Close\"\n",
    "df['Daily_Return'] = df[price_col].pct_change()\n",
    "df['Price_Change'] = df[price_col].diff()\n",
    "\n",
    "# Drop any NaN values created by the diff/pct_change operations\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(f\"Data for {STOCK_TICKER} from {START_DATE} to {END_DATE}:\")\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "\n",
    "class StockTradingEnv:\n",
    "    def __init__(self, df, initial_balance=10000, stock_quantity=10):\n",
    "        self.df = df.reset_index()\n",
    "        self.initial_balance = initial_balance\n",
    "        self.stock_quantity = stock_quantity\n",
    "        self.current_step = 0\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = initial_balance\n",
    "        self.action_space = [0, 1, 2] # 0: Hold, 1: Buy, 2: Sell\n",
    "        self.state_space_size = 4 # Current Price, Price Change, Balance, Shares Held\n",
    "\n",
    "    def _get_state(self):\n",
    "        price_col = \"Adj Close\" if \"Adj Close\" in self.df.columns else \"Close\"\n",
    "        current_price = self.df.loc[self.current_step, price_col]\n",
    "        if isinstance(current_price, pd.Series):\n",
    "            current_price = current_price.item()\n",
    "        price_change = self.df.loc[self.current_step, \"Price_Change\"]\n",
    "        if isinstance(price_change, pd.Series):\n",
    "            price_change = price_change.item()\n",
    "        return np.array([current_price, price_change, self.balance, self.shares_held])\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        price_col = \"Adj Close\" if \"Adj Close\" in self.df.columns else \"Close\"\n",
    "        current_price = self.df.loc[self.current_step, price_col]\n",
    "        if isinstance(current_price, pd.Series):\n",
    "            current_price = current_price.item()\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if action == 1: # Buy\n",
    "            if self.balance >= self.stock_quantity * current_price:\n",
    "                self.balance -= self.stock_quantity * current_price\n",
    "                self.shares_held += self.stock_quantity\n",
    "        elif action == 2: # Sell\n",
    "            if self.shares_held >= self.stock_quantity:\n",
    "                self.balance += self.stock_quantity * current_price\n",
    "                self.shares_held -= self.stock_quantity\n",
    "\n",
    "        new_portfolio_value = self.balance + self.shares_held * current_price\n",
    "        reward = new_portfolio_value - self.portfolio_value\n",
    "        self.portfolio_value = new_portfolio_value\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            done = True\n",
    "\n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay_rate=0.995, min_epsilon=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def _discretize_state(self, state):\n",
    "        current_price_bucket = int(state[0] / 10)\n",
    "        price_change_bucket = int(state[1] * 10)\n",
    "        balance_bucket = int(state[2] / 1000)\n",
    "        shares_held_bucket = int(state[3] / 10)\n",
    "        return (current_price_bucket, price_change_bucket, balance_bucket, shares_held_bucket)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        discretized_state = str(self._discretize_state(state))\n",
    "        if discretized_state not in self.q_table:\n",
    "            self.q_table[discretized_state] = np.zeros(self.action_size)\n",
    "\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[discretized_state])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        discretized_state = str(self._discretize_state(state))\n",
    "        discretized_next_state = str(self._discretize_state(next_state))\n",
    "\n",
    "        if discretized_state not in self.q_table:\n",
    "            self.q_table[discretized_state] = np.zeros(self.action_size)\n",
    "        if discretized_next_state not in self.q_table:\n",
    "            self.q_table[discretized_next_state] = np.zeros(self.action_size)\n",
    "\n",
    "        current_q_value = self.q_table[discretized_state][action]\n",
    "        max_future_q_value = np.max(self.q_table[discretized_next_state])\n",
    "\n",
    "        new_q_value = current_q_value + self.learning_rate * (reward + self.discount_factor * max_future_q_value - current_q_value)\n",
    "        self.q_table[discretized_state][action] = new_q_value\n",
    "\n",
    "        if done:\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay_rate)\n",
    "\n",
    "# Training parameters\n",
    "EPISODES = 100\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "\n",
    "env = StockTradingEnv(df)\n",
    "agent = QLearningAgent(state_size=env.state_space_size, action_size=len(env.action_space))\n",
    "\n",
    "episode_rewards = []\n",
    "portfolio_values = []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    portfolio_values.append(env.portfolio_value)\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{EPISODES} - Total Reward: {total_reward:.2f} - Final Portfolio Value: {env.portfolio_value:.2f} - Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "    if (episode + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "        print(f\"Checkpoint: Q-table saved at episode {episode + 1}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# Evaluation\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "env_eval = StockTradingEnv(df)\n",
    "state_eval = env_eval.reset()\n",
    "\n",
    "eval_portfolio_values = []\n",
    "\n",
    "done_eval = False\n",
    "while not done_eval:\n",
    "    action_eval = agent.get_action(state_eval)\n",
    "    next_state_eval, reward_eval, done_eval, _ = env_eval.step(action_eval)\n",
    "    state_eval = next_state_eval\n",
    "    eval_portfolio_values.append(env_eval.portfolio_value)\n",
    "\n",
    "print(f\"\\nEvaluation complete! Final Portfolio Value: {env_eval.portfolio_value:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Episode Rewards during Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio_values)\n",
    "plt.title(\"Portfolio Value during Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(eval_portfolio_values)\n",
    "plt.title(\"Portfolio Value during Evaluation\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
